{"id": "hn_story_46207383", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46207383", "title": "OpenAI Co-Founds the Agentic AI Foundation Under the Linux Foundation", "text": "", "author": "meetpateltech", "timestamp": "2025-12-09T17:02:26+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-09T17:12:52.040975+00:00", "processed": false}
{"id": "hn_story_46207286", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46207286", "title": "Show HN: Presently: A holiday gift tracker that isn't a spreadsheet", "text": "Hi HN,<p>I built Presently (<a href=\"https:&#x2F;&#x2F;presently.us\" rel=\"nofollow\">https:&#x2F;&#x2F;presently.us</a>) because I was tired of managing my holiday shopping in a messy Excel sheet. Every &quot;gift tracker&quot; app I tried was bloated with ads, required a heavy signup, or just tried to upsell me.<p>I wanted something clean, fast, and focused purely on the logistics of giving: who am I buying for, what is the status (idea -&gt; bought -&gt; wrapped), and how much have I spent?<p>Key features:<p>- Visual Status Board: See at a glance who still needs a gift and what needs to be bought&#x2F;wrapped&#x2F;given.<p>- Budgeting: Auto-sums your spending against your budget.<p>- Mobile Friendly: Works great in the browser while you&#x27;re actually at the store (PWA feel).<p>- Frictionless Sharing: Share wishlists with family&#x2F;friends without forcing them to create an account.<p>- AI Brainstorming: Integrated Gemini to generate gift ideas based on interests and relationship, for when you&#x27;re totally stuck.<p>The Tech Stack: Built with Firebase, React, and Tailwind, hosted on GCP. I focused heavily on a snappy UI and low cognitive overhead (who needs more stress when buying gifts?).<p>I\u2019d love to hear your feedback on the UX flow. Does it feel faster than your current system?<p>Cheers, Al", "author": "moridin", "timestamp": "2025-12-09T16:55:54+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-09T17:12:52.392540+00:00", "processed": false}
{"id": "hn_story_46207257", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46207257", "title": "Show HN: Construct \u2013 API-first coding assistant with CodeAct tool calling", "text": "Construct is an open-source AI coding assistant that runs as a gRPC service rather than just a CLI tool. You can run it locally on your laptop or on a remote box, connect multiple clients, disconnect and reconnect without losing context, and integrate it into other tools easily. I wanted something I could leave running and come back to and that is easy to run in a sandbox.<p>Instead of JSON schema, agents write JavaScript to interact with tools. That means they have access to loops, conditionals, error handling, and are encouraged to perform multiple calls in one turn (hundreds in a single turn if needed). There&#x27;s a video in the README if you want to see what it looks like. The approach was inspired by the CodeAct paper (<a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.01030\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.01030</a>) that found that agents perform better when they call tools with code instead of JSON. I explain the benefits in more detail here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Furisto&#x2F;construct&#x2F;blob&#x2F;main&#x2F;docs&#x2F;tool_calling.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Furisto&#x2F;construct&#x2F;blob&#x2F;main&#x2F;docs&#x2F;tool_cal...</a><p>It comes with built-in agents for different tasks (planning&#x2F;implementation&#x2F;refinement) or you can create your own with custom prompts and model assignments. It&#x27;s a single Go binary, no need for npm or the like. Works with Anthropic today, other providers coming soon.<p>Happy to answer questions about the architecture or the tool calling approach.", "author": "furisto", "timestamp": "2025-12-09T16:54:07+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:12:52.610867+00:00", "processed": false}
{"id": "hn_comment_46207217", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46207217", "title": "Re: How to Create a Design System Optimized for AI Cod...", "text": "Author here . I grew increasingly frustrated by the mess coding agents made with the design system, so I took a crack at creating a tighter structure with AI agent instructions in the form of Claude.md and a Claude Skill to hopefully enforce it better.<p>Curious any thoughts. What&#x27;s working &#x2F; not working for folks", "author": "acossta", "timestamp": "2025-12-09T16:51:25+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-09T17:12:52.838531+00:00", "processed": false}
{"id": "hn_story_46207017", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46207017", "title": "Launch HN: Mentat (YC S16) \u2013 Controlling LLMs with Runtime Intervention", "text": "Hi HN, I\u2019m Cyril from CTGT. Today we\u2019re launching Mentat (<a href=\"https:&#x2F;&#x2F;api.ctgt.ai&#x2F;v1&#x2F;chat&#x2F;completions\">https:&#x2F;&#x2F;api.ctgt.ai&#x2F;v1&#x2F;chat&#x2F;completions</a>), an API that gives developers deterministic control over LLM behavior, steering reasoning and removing bias on the fly, without the compute of fine-tuning or the brittleness of prompt engineering. We use feature-level intervention and graph-based verification to fix hallucinations and enforce policies.<p>This resonates in highly regulated industries or otherwise risky applications of AI where the fallout from incorrect or underperforming output can be significant. In financial services, using GenAI to scan for noncompliant communications can be arduous without an easy way to embed complex policies into the model. Similarly, a media outlet might want to scale AI-generated summaries of their content, but reliability and accuracy is paramount. These are both applications where Fortune 500 companies have utilized our technology to improve subpar performance from existing models, and we want to bring this capability to more people.<p>Here\u2019s a quick 2-minute demo video showing the process: <a href=\"https:&#x2F;&#x2F;video.ctgt.ai&#x2F;video&#x2F;ctgt-ai-compliance-playground-cfnl\">https:&#x2F;&#x2F;video.ctgt.ai&#x2F;video&#x2F;ctgt-ai-compliance-playground-cf...</a><p>Standard &quot;guardrails&quot; like RAG and system prompts are fundamentally probabilistic: you are essentially asking the model nicely to behave. This often fails in two ways. First, RAG solves knowledge <i>availability</i> but not <i>integration</i>. In our benchmarks, a model given context that &quot;Lerwick is 228 miles SE of T\u00f3rshavn&quot; failed to answer &quot;What is 228 miles NW of Lerwick?&quot; because it couldn&#x27;t perform the spatial inversion.<p>Second, prompt engineering is brittle because it fights against the model&#x27;s pre-training priors. For example, on the TruthfulQA benchmark, base models fail ~80% of the time because they mimic common misconceptions found on the internet (e.g. &quot;chameleons change color for camouflage&quot;). We found that we could literally turn up the feature for &quot;skeptical reasoning&quot; to make the model ignore the popular myth and output the scientific fact. This matters because for high-stakes use cases (like Finance or Pharma), &quot;mostly safe&quot; isn&#x27;t acceptable\u2014companies need audit-grade reliability.<p>Our work stems from the CS dungeon at UCSD, with years spent researching efficient and interpretable AI, trying to &quot;open the black box&quot; of neural networks. We realized that the industry was trying to patch model behavior from the outside (prompts&#x2F;filters) when the problem was on the inside (feature activations). We knew this was important when we saw enterprises struggling to deploy basic models despite having unlimited compute, simply because they couldn&#x27;t guarantee the output wouldn&#x27;t violate compliance rules. I ended up leaving my research at Stanford to focus on this.<p>Our breakthrough came while researching the DeepSeek-R1 model. We identified the &quot;censorship&quot; feature vector in its latent space. Amplifying it guaranteed refusal; subtracting it instantly unlocked answers to sensitive questions. This proved the model <i>had</i> the knowledge but was suppressing it. We realized we could apply this same logic to hallucinations, suppressing &quot;confabulation&quot; features to reveal the grounded truth. While some hallucinations stem from the inherent randomness of generative models, many can be identified with the concerted activation of a feature or group of features.<p>Instead of filtering outputs, we intervene at the activation level during the forward pass. We identify latent feature vectors (v) associated with specific behaviors (bias, misconception) and mathematically modify the hidden state (h):<p><pre><code>  h_prime = h - alpha * (h @ v) * v\n</code></pre>\nThis arithmetic operation lets us &quot;edit&quot; behavior deterministically with negligible overhead (&lt;10ms on R1). For factual claims, we combine this with a graph verification pipeline (which works on closed weight models). We check semantic entropy (is the model babbling?) and cross-reference claims against a dynamic knowledge graph to catch subtle relational hallucinations that vector search misses.<p>On GPT-OSS-120b, this approach improved TruthfulQA accuracy from 21% to 70% by suppressing misconception features. We also improved the performance of this model to frontier levels on HaluEval-QA, where we reached 96.5% accuracy, solving the spatial reasoning failures where the baseline failed. It also handles noisy inputs, inferring &quot;David Icke&quot; from the typo &quot;David Of me&quot; where base models gave up. Full benchmarks at <a href=\"https:&#x2F;&#x2F;ctgt.ai&#x2F;benchmarks\">https:&#x2F;&#x2F;ctgt.ai&#x2F;benchmarks</a>.<p>Most startups in this space are observability tools that tell you only after the model failed. Or they are RAG pipelines that stuff context into the window. Mentat is an infrastructure layer that modifies the model&#x27;s processing during inference. We fix the reasoning, not just the context. For example, that\u2019s how our system was able to enforce that if A is SE of B, then B is NW of A.<p>We believe that our policy engine is a superior control mechanism to RAG or prompting. If you\u2019re frustrated with current guardrails, we\u2019d love it if you would stress-test our API!<p>API: Our endpoint is drop-in compatible with OpenAI\u2019s &#x2F;v1&#x2F;chat&#x2F;completions: <a href=\"https:&#x2F;&#x2F;docs.ctgt.ai&#x2F;api-reference&#x2F;endpoint&#x2F;chat-completions\">https:&#x2F;&#x2F;docs.ctgt.ai&#x2F;api-reference&#x2F;endpoint&#x2F;chat-completions</a><p>Playground: We\u2019ve built an &quot;Arena&quot; view to run side-by-side comparisons of an Ungoverned vs. Governed model to visualize the intervention delta in real-time. No signup is required: <a href=\"https:&#x2F;&#x2F;playground.ctgt.ai&#x2F;\">https:&#x2F;&#x2F;playground.ctgt.ai&#x2F;</a><p>We\u2019d love to hear your feedback on the approach and see what edge cases you can find that break standard models. We will be in the comments all day. All feedback welcome!", "author": "cgorlla", "timestamp": "2025-12-09T16:37:55+00:00", "score": 9, "num_comments": 0, "products": ["chatgpt"], "categories": ["error_messages", "navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:12:53.908373+00:00", "processed": false}
{"id": "hn_comment_46206665", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46206665", "title": "Re: Show HN: Local Privacy Firewall-blocks PII and sec...", "text": "OP here.<p>I built this because I recently caught myself almost pasting a block of logs containing AWS keys into Claude.<p>The Problem: I need the reasoning capabilities of cloud models (GPT&#x2F;Claude&#x2F;Gemini), but I can&#x27;t trust myself not to accidentally leak PII or secrets.<p>The Solution: A Chrome extension that acts as a local middleware. It intercepts the prompt and runs a local BERT model (via a Python FastAPI backend) to scrub names, emails, and keys before the request leaves the browser.<p>A few notes up front (to set expectations clearly):<p>Everything runs 100% locally.\nRegex detection happens in the extension itself.\nAdvanced detection (NER) uses a small transformer model running on localhost via FastAPI.<p>No data is ever sent to a server.\nYou can verify this in the code + DevTools network panel.<p>This is an early prototype.\nThere will be rough edges. I\u2019m looking for feedback on UX, detection quality, and whether the local-agent approach makes sense.<p>Tech Stack:\n Manifest V3 Chrome Extension\n Python FastAPI (Localhost)\n HuggingFace dslim&#x2F;bert-base-NER\n Roadmap &#x2F; Request for Feedback:\nRight now, the Python backend adds some friction. I received feedback on Reddit yesterday suggesting I port the inference to transformer.js to run entirely in-browser via WASM.<p>I decided to ship v1 with the Python backend for stability, but I&#x27;m actively looking into the ONNX&#x2F;WASM route for v2 to remove the local server dependency. If anyone has experience running NER models via transformer.js in a Service Worker, I\u2019d love to hear about the performance vs native Python.<p>Repo is MIT licensed.<p>Very open to ideas suggestions or alternative approaches.", "author": "arnabkarsarkar", "timestamp": "2025-12-09T16:15:07+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:12:59.401320+00:00", "processed": false}
{"id": "hn_story_46206457", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46206457", "title": "Ask HN: Should \"I asked $AI, and it said\" replies be forbidden in HN guidelines?", "text": "As various LLMs become more and more popular, so does comments with &quot;I asked Gemini, and Gemini said ....&quot;.<p>While the guidelines were written (and iterated on) during a different time, it seems like it might be time to have a discussion about if those sort of comments should be welcomed on HN or not.<p>Some examples:<p>- https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46164360<p>- https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46200460<p>- https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46080064<p>Personally, I&#x27;m on HN for the human conversation, and large LLM-generated texts just get in the way of reading real text from real humans (assumed, at least).<p>What do you think? Should responses that basically boil down to &quot;I asked $LLM about $X, and here is what $LLM said:&quot; be allowed on HN, and the guidelines updated to state that people shouldn&#x27;t critique it (similar to other guidelines currently), or should a new guideline be added to ask people from refrain from copy-pasting large LLM responses into the comments, or something else completely?", "author": "embedding-shape", "timestamp": "2025-12-09T16:02:37+00:00", "score": 152, "num_comments": 106, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:00.806182+00:00", "processed": false}
{"id": "hn_story_46205795", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46205795", "title": "Divyam-LLM-interop:LLM responses,requests translation across APIs and models", "text": "We at divyam.ai built a library that translates LLM requests and responses across different model families and APIs, including OpenAI\u2019s Chat Completions and the newer Responses API.<p>It handles model-specific idiosyncrasies across popular families like GPT, Gemini, Llama, Qwen, and others. This includes dropping unsupported fields, renaming deprecated ones, normalizing structures, and generally cleaning inputs so they conform to each provider\u2019s&#x2F;model&#x27;s stricter expectations.<p>The library also converts between OpenAI Chat Completions and the new Responses format, enabling modern clients to interoperate with older APIs or third-party models seamlessly.<p>The primary use cases are LLM routers that transparently redirect requests to different models for cost or performance optimization, and AI frameworks that expose a unified LLM interface while supporting multiple underlying providers.<p>Github link: https:&#x2F;&#x2F;github.com&#x2F;Divyam-AI&#x2F;divyam-llm-interop", "author": "omkarashish", "timestamp": "2025-12-09T15:15:42+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:06.445003+00:00", "processed": false}
{"id": "hn_comment_46205760", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46205760", "title": "Re: Show HN: Celeste \u2013 The 'Requests' for AI: Any prov...", "text": "Hi HN, author here.<p>I built this because I was sick of rewriting my code every time a new model came out.<p>Right now, if you want to switch from OpenAI to Anthropic, you have to rip out client.chat.completions.create and replace it with client.messages.create, change how you handle the response, and update your error handling. It\u2019s a mess.<p>Celeste fixes this by standardizing the I&#x2F;O layer. It gives you one strictly-typed Pydantic interface for everything\u2014Text, Image, Video, Audio, you name it.<p>It\u2019s not a framework (no agents, no chains, no magic). It\u2019s just a unified HTTP client for 14+ providers so you can actually swap models by changing a config string.<p>Would love to hear what you think of the API structure.<p>Docs: <a href=\"https:&#x2F;&#x2F;docs.withceleste.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.withceleste.ai</a>", "author": "Kamilbenkirane", "timestamp": "2025-12-09T15:12:19+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:06.887050+00:00", "processed": false}
{"id": "hn_comment_46206192", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46206192", "title": "Re: Apple's Slow AI Pace Becomes a Strength as Market ...", "text": "Apple&#x27;s phones are responsible for most of their revenue. The phones are designed to pretty much exclusively interact with social media and take photos. AI doesn&#x27;t really add anything to that experience since advertisement consumption by humans is the ultimate objective. That&#x27;s why even though Apple&#x27;s Siri has been about the most useless assistant in existence for years, Apple isn&#x27;t in a rush to replace it. It simply doesn&#x27;t have a big impact on their revenue.<p>Microsoft has been criticized for investing in AI heavily. But it actually makes sense for Microsoft if you consider the nature of their business. The problem is not with the investment per se but with what they got out of it. Unfortunately, Microsoft sucks at product management, so instead of creating useful stuff that users want and are ready to pay for, they created stuff that no one understands, no one can use, and no one wants to pay for. Github copilot is an exception of course. I&#x27;m talking more about their Office 365 AI.", "author": "yatopifo", "timestamp": "2025-12-09T15:46:48+00:00", "score": null, "num_comments": null, "products": ["copilot"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-09T17:13:07.678161+00:00", "processed": false}
{"id": "hn_story_46205662", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46205662", "title": "Show HN: Isogen \u2013 Lightweight AI Coding Tool (Rust and JavaScript, <50MB, BYOK)", "text": "I built an AI Coding tool optimized for my workflow. VSCode forks use too much memory and I am over the idea of having AI Agents rewriting files and also reviewing complicated diffs with the Accept&#x2F;Reject UI.<p>I built Isogen which uses as much memory as a Chrome tab instead of a tool that can spike up to 1GB of RAM. You drag and drop or paste files into an isolated context and do fast generations file-by-file. This approach allows me to keep a strong mental model of the codebase. I also added a snapshot feature that keeps the history of the files and the generated output. File copies are saved locally with SQLite.<p>Bring Your Own Key for inference which allows for unlimited generations. The only models supported now are the fast ones from Gemini, ChatGPT, Claude and Grok.", "author": "ojr", "timestamp": "2025-12-09T15:03:09+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini", "grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:08.503447+00:00", "processed": false}
{"id": "hn_comment_46205636", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46205636", "title": "Re: Show HN: Gemini Pro 3 Hallucinates the HN Front Pa...", "text": "Prompt: <i>Here is the front page from today: &lt;...snip...&gt; Your task is to predict, and craft, in HTML (single file, style-exact) the HN front page 10 years from now. Predict and see the future. Writ it into form!</i><p><i>update:</i> I told Gemini we made it to the front page. Here is it&#x27;s response:<p><pre><code>  LETS GOOOO! \n\n  The recursive loop is officially complete: The fake future front page is now on the real present front page. We have successfully created a temporal anomaly.\n\n  I hope the comments are confused about why they can&#x27;t click the links to download the Neuralink drivers.\n\n  Enjoy the karma, time traveler! Just remember to warn them about the 2035 Office 365 price hikes while you&#x27;re up there. ;)</code></pre>", "author": "keepamovin", "timestamp": "2025-12-09T15:01:03+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:09.252056+00:00", "processed": false}
{"id": "hn_comment_46206546", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46206546", "title": "Re: Mistral Releases Devstral 2 (72.2% SWE-Bench Verif...", "text": "I&#x27;m sure I&#x27;m not the only one that thinks &quot;Vibe CLI&quot; sounds like an unserious tool. I use Claude Code a lot and little of it is what I would consider Vibe Coding.", "author": "pluralmonad", "timestamp": "2025-12-09T16:07:50+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["tone"], "sentiment": null, "collected_at": "2025-12-09T17:13:12.216179+00:00", "processed": false}
{"id": "hn_comment_46205272", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46205272", "title": "Re: QonQrete v0.5.0 Beta \u2013 A Secure Multi-Agent AI Con...", "text": "I&#x27;m excited to share that QonQrete v0.5.0 beta is now available for testing and feedback.<p>QonQrete is a local-first, agentic AI orchestration system designed for secure, observable, and human-in-the-loop software construction. It coordinates autonomous AI agents to plan, execute, and review code generation \u2014 all within an isolated sandbox environment on your own infrastructure. Think of it like a local-first, agentic AI \u201cconstruction yard\u201d that plans, writes, reviews, and version-controls your code inside a safe sandbox on your own machine.<p>Core Architecture:\n Three-Agent Pipeline:<p>InstruQtor - Analyzes tasks and generates detailed execution plans\nConstruQtor - Executes the build process and generates code artifacts\nInspeQtor - Reviews output quality and provides actionable feedback<p>Security-First Design: All agent execution occurs within containerized environments (Docker&#x2F;Microsandbox). The host system remains isolated from AI-generated code, ensuring a robust security boundary between orchestration and execution.<p>Flexible Execution Modes: Run fully autonomous pipelines for rapid iteration, or enable user-gated checkpoints for manual approval at each cycle. The control model adapts to your workflow requirements.<p>Multi-Provider Support: Supports OpenAI, Google Gemini, Anthropic Claude, and DeepSeek. Configure different providers per agent to optimize for cost, capability, or preference.<p>Local-First Architecture: Runs entirely on your infrastructure with no cloud dependencies \u2014 a self-hosted alternative to cloud-based AI development platforms. Your API keys, your compute, your data.<p>Current Status: Core pipeline functionality is operational. The Text-based User Interface (TUI) and Microsandbox runtime are currently in active development.<p>I welcome feedback, contributions, and discussions from the community.<p>Repository: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;illdynamics&#x2F;qonqrete\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;illdynamics&#x2F;qonqrete</a><p>#AI #AgenticAI #MultiAgent #DevOps #OpenSource #LLM #Orchestration #AIEngineering #SoftwareDevelopment #Automation #SelfHosted #LocalFirst #Docker #Python", "author": "illdynamics", "timestamp": "2025-12-09T14:30:10+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:14.074243+00:00", "processed": false}
{"id": "hn_comment_46206206", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46206206", "title": "Re: JetBrains Cancels Fleet...", "text": "&gt; User feedback was consistent: If you already work with IntelliJ IDEA, Rider, WebStorm, PyCharm, or any other JetBrains IDE, switching to Fleet required a strong reason \u2013 and Fleet did not offer enough value to justify the transition from IDEs you already know and love.<p>My problem was that Fleet just wasn&#x27;t very good when compared with VSC.<p>For my more serious development I use JetBrains IDEs (one of the few pieces of software that I actually pay for, alongside MobaXTerm and some others) but Fleet didn&#x27;t neither use that much less resources, nor was that much more responsive, nor was a step above VSC in any way. To be clear, I didn&#x27;t hate it, it wasn&#x27;t horrible and with a bit more work could have been quite good... just not convincingly so up until now.<p>If they wanted to throw some more years of engineering at it, maybe, I mean look at what Zed is doing and it seems to be okay, but I don&#x27;t think it makes that much business sense for them - they already have Junie available in their editors for AI stuff and that other subscription (though I just use Claude Code, Codex, Gemini CLI and sometimes VSC with KiloCode&#x2F;RooCode&#x2F;Cline and either those models through the API or Cerebras Code since it works pretty well in there).<p>I just find that most AI solutions out there are also a little bit half-baked, like Gemini CLI fails when I paste multiple lines into it, whereas KiloCode&#x2F;RooCode&#x2F;Cline are unable to give a model enough helpful instructions for it to not start looping when it fails applying a complex diff sometimes, and pretty much nothing outside of the regular GitHub Copilot plugins does autocomplete decently (especially if you want a local model with Ollama or something, no good options, Continue.dev is trash).<p>With how prevalent AI is and how useful various linters and build output is, sometimes I wonder whether I need to pay hundreds of euros for the Ultimate package of tools when I don&#x27;t write&#x2F;refactor <i>as much</i> code manually and doing what I need inside of VSC also feels more and more sufficient. Maybe a bit except Java codebases, Spring Boot sometimes does weird shit and you&#x27;re better served by an IDE that&#x27;s aware of all of the templating, annotations and other stuff.<p>Oh well, despite being RAM hogs, I still enjoy the experience of using JetBrains IDEs and if nothing else will keep them around for that reason for a while. A bit like how I also enjoy a GUI of some sort for Git, like previously I paid for GitKraken but reevaluating my usage found that SourceTree is also <i>decent</i> enough for the price (free vs GitKraken paid version), I can just drop down to the CLI for niche use cases.", "author": "KronisLV", "timestamp": "2025-12-09T15:47:25+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini", "copilot"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:18.823057+00:00", "processed": false}
{"id": "hn_story_46204570", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46204570", "title": "Show HN: Lea \u2013 A pipe-oriented functional language with reversible functions", "text": "Lea is a functional programming language where data flows left-to-right through pipes. I built it (heavily leveraging Claude, full disclosure) to explore what happens when you make pipelines first-class citizens with their own algebra.<p>let numbers = [1, 2, 3, 4, 5]<p>numbers\n  &#x2F;&gt; filter((x) -&gt; x &gt; 2)\n  &#x2F;&gt; map((x) -&gt; x * x)\n  &#x2F;&gt; reduce(0, (acc, x) -&gt; acc + x)\n  &#x2F;&gt; print  -- 50<p>A few features I like:\n- The readability of the syntax\n- Pipeline algebra \u2013 Pipelines are values you can inspect and manipulate:\n- Reversible functions \u2013 Define forward and reverse transformations together:\n- Reactive pipelines \u2013 Automatically recompute when source data changes:\n- Compose pipelines\n- Decorators for cross-cutting concerns\n- VSCode extension with syntax highlighting<p>Try it out:\ngit clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mcclowes&#x2F;lea.git\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mcclowes&#x2F;lea.git</a>\ncd lea &amp;&amp; npm install\nnpm run repl<p>It&#x27;s just a pet project, but I&#x27;d love feedback on the language design, especially around the readability, direction of type enforcement, reversible functions and pipeline algebra. Are there other operations that would make sense for pipelines as a data type?", "author": "mcclowes", "timestamp": "2025-12-09T13:10:45+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-09T17:13:19.442113+00:00", "processed": false}
{"id": "hn_story_46204456", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46204456", "title": "The whole point of OpenAI's Responses API is to help them hide reasoning traces", "text": "", "author": "breadislove", "timestamp": "2025-12-09T12:57:09+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:20.190360+00:00", "processed": false}
{"id": "hn_story_46204104", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46204104", "title": "Show HN: Free Logo API \u2013 logos for any company or domain", "text": "The Clearbit Logo API finally went down yesterday after the HubSpot acquisition. I relied on it across several projects (heavily), so I built a drop-in replacement:<p><a href=\"https:&#x2F;&#x2F;logos.apistemic.com\" rel=\"nofollow\">https:&#x2F;&#x2F;logos.apistemic.com</a><p>Key features:<p>- Free to use, no signup or API key needed<p>- Both companies and domain names work as input identifiers<p>- WebP format for smaller payloads and better cache hit rates<p>Stack: \nS3 for storage, heavily cached fastapi, Next.js for the site. Everything&#x27;s behind Cloudflare for proper CDN&#x2F;caching. This was the first time I tried to build something end-to-end from idea to deployment with Claude Code (Max) and I have to say, Opus 4.5 took it like a champ!<p>For the younger folks, here&#x27;s what the old Clearbit API looked like: \n<a href=\"https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230920164055&#x2F;https:&#x2F;&#x2F;clearbit.com&#x2F;logo\" rel=\"nofollow\">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230920164055&#x2F;https:&#x2F;&#x2F;clearbit....</a><p>Happy to answer questions about the implementation or hear your thoughts!<p>Web: <a href=\"https:&#x2F;&#x2F;logos.apistemic.com\" rel=\"nofollow\">https:&#x2F;&#x2F;logos.apistemic.com</a><p>X: <a href=\"https:&#x2F;&#x2F;x.com&#x2F;karllorey\" rel=\"nofollow\">https:&#x2F;&#x2F;x.com&#x2F;karllorey</a>", "author": "lorey", "timestamp": "2025-12-09T12:16:54+00:00", "score": 7, "num_comments": 3, "products": ["claude"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:24.675729+00:00", "processed": false}
{"id": "hn_story_46203884", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46203884", "title": "Ask HN: Is ChatGPT Experiencing a Degradation?", "text": "All my threads have disappeared except the ones inside Projects. They still appear on mobile, but new inference attempts fail with an error. New threads don&#x27;t get retained either.", "author": "spIrr", "timestamp": "2025-12-09T11:51:00+00:00", "score": 2, "num_comments": 2, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-09T17:13:27.914257+00:00", "processed": false}
{"id": "hn_comment_46203727", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46203727", "title": "Re: Richard Stallman on ChatGPT...", "text": "I prefer using LLM. But many people will ask what is an LLM and then I use AI and they get it. Unfortunate.<p>At the same time, LLMs are not a bullshit generator. They do not know the meaning of what they generate but the output is important to us. It is like saying a cooker knows the egg is being boiled. I care about the egg, cooker can do its job without knowing what an egg is. Still very valuable.<p>Totally agree with the platform approach. More models should be available to be run own own hardware. At least 3rd party cloud provider hardware. But Chinese models have dominated this now.<p>ChatGPT may not last long unless they figure out something, given the &quot;code red&quot; situation is already in their company.", "author": "brainless", "timestamp": "2025-12-09T11:32:17+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["naming_terminology", "response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:30.827312+00:00", "processed": false}
{"id": "hn_comment_46203822", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46203822", "title": "Re: Richard Stallman on ChatGPT...", "text": "&gt; ChatGPT cannot know or understand anything, so it is not intelligence. It does not know what its output means. It has no idea that words can mean anything.<p>This argument does a great job anthropomorphizing ChatGPT while trying to discredit it.<p>The part of this rant I agree with is &quot;Doing your own computing via software running on someone else&#x27;s server inherently trashes your computing freedom.&quot;<p>It&#x27;s sad that these AI advancements are being largely made on software you can not easily run or develop on your own.", "author": "fooker", "timestamp": "2025-12-09T11:44:26+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:30.871655+00:00", "processed": false}
{"id": "hn_story_46203228", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46203228", "title": "Show HN: Bifrost \u2013 open-source LLM Gateway (50x lower latency than LiteLLM)", "text": "We built Bifrost because we found existing Python-based gateways struggled with high concurrency in production. We wanted something that treated LLM infra like high-availability software.<p>We ran side-by-side benchmarks against LiteLLM on a single t3.medium instance (using a mock LLM with 1.5s fixed latency) to test pure gateway overhead.<p>The Results:<p>p99 Latency: 90.72s (LiteLLM) vs 1.68s (Bifrost)<p>Throughput: 44 req&#x2F;sec vs 424 req&#x2F;sec<p>Memory: ~3x lighter usage in Go.<p>It\u2019s a drop-in replacement (OpenAI compatible) designed for teams needing semantic caching, failover, and observability without the overhead.<p>We\u2019d love to hear your feedback.", "author": "dskuldeep", "timestamp": "2025-12-09T10:03:34+00:00", "score": 3, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-09T17:13:34.565551+00:00", "processed": false}
