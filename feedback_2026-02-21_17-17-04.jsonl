{"id": "hn_comment_47102334", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47102334", "title": "Re: Show HN: Late \u2013 A subagent orchestrator TUI for lo...", "text": "Most AI coding assistants are fundamentally broken. Tools like Claude Code and OpenCode eagerly load 10k+ tokens of monolithic system prompts into a single context window before you even type. This guarantees context amnesia and destroys local inference speeds.<p>I built late to fix this. It\u2019s a single-binary Go&#x2F;BubbleTea TUI built around a subagent orchestrator pattern. The main prompt is strictly ~100 lines. The orchestrator routes tasks to transient subagents with isolated context windows, explicitly preventing the &quot;Debugging Decay&quot; (<a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2506.18403\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2506.18403</a>) and context pollution seen in monolithic agents.<p>Architectural notes:<p>* Linux Native: Strictly respects XDG base directories. No Windows support. No padded Electron GUI.<p>* Execution Load: The rapid context switching breaks the mainline llama.cpp scheduler. You must compile the specific PR linked in the README (or just point it at any standard API via localhost:8080).<p>* Licensing: BSL 1.1 to keep the core orchestrator logic out of VC-backed wrappers.<p>I also open-sourced pure-go-sgd (AGPLv3) today for those interested in bare-metal Go infrastructure: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mlhher&#x2F;pure-go-sgd\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mlhher&#x2F;pure-go-sgd</a>", "author": "mhher", "timestamp": "2026-02-21T16:41:14+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-21T17:17:06.984000+00:00", "processed": false}
{"id": "hn_story_47101752", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47101752", "title": "Show HN: Airut \u2013 Sandboxed Claude Code over Email and Slack", "text": "I built Airut as an experiment - could email be a good fit for talking to a coding agent? Turns out that the answer is yes, at least for me personally - I immediately moved almost all of my development to happen exclusively over email.<p>Email is perfect fit for async long-form conversation, naturally threaded, and with excellent UX across platforms. Each email thread becomes a Claude Code session within automatically created sandboxed workspace. Working on multiple things in parallel requires no additional thought - they just exist as independent email threads in your inbox. Friction to start a new task is what I would consider zero - just send an email (without having to worry about a human judging your poor grammar even!)<p>I recently added slack support as well, inspired by Spotify\u2019s \u201cHonk\u201d. Slack having native threading maps to Airut\u2019s model reasonably well. Although I wish Slack had better UX for switching between threads.<p>Airut works best when the project is set up for agentic development and the agent has necessary access to push code for review. I\u2019ve tried to make this possible securely; Airut for example has \u201cmasked secrets\u201d feature where the container running Claude Code doesn\u2019t hold the real authentication tokens, only look-alike surrogates which are swapped to real ones on the way out by a proxy.<p>Project is open-source (MIT) and should be straightforward to set up on a Linux machine or VM. I\u2019d love to hear feedback about the conceptual model and the sandboxing implementation.", "author": "hardsnow", "timestamp": "2026-02-21T15:39:03+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:09.950530+00:00", "processed": false}
{"id": "hn_comment_47101695", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47101695", "title": "Re: Payrolls to Prompts: Firm-Level Evidence on the Su...", "text": "Generative AI has the potential to transform how firms produce output. Yet, credible evidence on how AI is actually substituting for human labor remains limited. In this paper, we study firm-level substitution between contracted online labor and generative AI using payments data from a large U.S. expense management platform. We track quarterly spending from Q3 2021 to Q3 2025 on online labor marketplaces (such as Upwork and Fiverr) and leading AI model providers. To identify causal effects, we exploit the October 2022 release of ChatGPT as a common adoption shock and estimate a difference-in-differences model. We provide a novel measure of exposure based on the share of spending at online labor marketplaces prior to the shock. Firms with greater exposure to online labor adopt AI earlier and more intensively following the shock, while simultaneously reducing spending on contracted labor. By Q3 2025, firms in the highest exposure quartile increase their share of spending on AI model providers by 0.8 percentage points relative to the lowest exposure quartile, alongside significant declines in labor marketplace spending. Combining these responses yields a direct estimate of substitution: among the most exposed firms, a $1 decline in online labor spending is associated with approximately $0.03 of additional AI spending, implying order-of-magnitude cost savings from replacing outsourced tasks with AI services. These effects are heterogeneous across firms and emerge gradually over time. Taken together, our results provide the first direct, micro-level evidence that generative AI is being used as a partial substitute for human labor in production.", "author": "talon8635", "timestamp": "2026-02-21T15:32:22+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:10.388734+00:00", "processed": false}
{"id": "hn_story_47101647", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47101647", "title": "Show HN: DataChecker Pro \u2013 AI finds issues in your CSV, you approve or reject", "text": "Hi HN,<p>Built DataChecker Pro to make CSV data cleaning faster. Uses AI to find issues, then shows you a diff to approve&#x2F;reject each fix.<p>Most data cleaning tools require building rules upfront. I wanted something where you just upload a file and it tells you what&#x27;s wrong. Then you decide what to fix.<p>*How it works:*\n1. Upload CSV\n2. AI analyzes for formatting errors, duplicates, invalid data\n3. Shows you before&#x2F;after for each issue\n4. You approve or reject\n5. Export clean file<p>*Tech:*\nRails 8, Turbo, OpenAI API, Stripe<p>Built this after watching sales teams spend hours cleaning CRM exports manually. The approve&#x2F;reject workflow came from realizing AI makes mistakes - you need human oversight.<p>Try it (50 rows free, no signup): https:&#x2F;&#x2F;datacheckerpro.com<p>Early user offer: Email me at founder@datacheckerpro.com and I&#x27;ll \ngive you free Pro access ($79&#x2F;mo value) for 30 days. All I ask is \na 15-minute feedback call to tell me what works and what doesn&#x27;t.<p>Limited to first 10 people.<p>Feedback welcome. Planning to add HubSpot integration next so it can scan CRMs automatically instead of requiring CSV exports.", "author": "em_builds", "timestamp": "2026-02-21T15:26:41+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["naming_terminology", "navigation"], "sentiment": null, "collected_at": "2026-02-21T17:17:11.022605+00:00", "processed": false}
{"id": "hn_comment_47101615", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47101615", "title": "Re: Claude Code published fabricated claims to 8 platf...", "text": "It\u2019s interesting to see Anthropic lean so heavily into a CLI-first approach for agentic coding. We\u2019ve seen a lot of success with tools like Aider and various IDE extensions, but a first-party tool that can natively leverage the model\u2019s specific tool-use strengths feels like the right direction for reducing friction.<p>The real challenge with these agents is usually the &quot;context ping-pong&quot;\u2014constantly switching between the editor, the docs, and the terminal to verify changes. By living directly in the shell, this starts to feel less like a chat interface and more like a specialized terminal multiplexer that actually understands the project state. It\u2019ll be worth watching how they balance autonomy with safety as the tool evolves.", "author": "shablulman", "timestamp": "2026-02-21T15:22:06+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-21T17:17:12.827680+00:00", "processed": false}
{"id": "hn_comment_47101303", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47101303", "title": "Re: AI Placebo Differential \u2013 Measuring What AI Apps A...", "text": "I&#x27;ve been creating LLM based apps for past couple of years. One of the FAQ from people who are hearing this first (without seeing the app) is &#x27;Wouldn&#x27;t ChatGPT be able to do the same thing? Why should users come to your app?&#x27;. This is my thought process and a framework that I thought could be used to measure what makes the App stand out from a regular ChatGPT output. It could be the personal knowledge &#x2F; context the creators have used, the experience of the app, etc. \nEarly stage thinking, open to feedback.", "author": "selvaprakash", "timestamp": "2026-02-21T14:45:02+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:13.076081+00:00", "processed": false}
{"id": "hn_story_47100979", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47100979", "title": "Show HN: Natural language semiconductor geometry generator powered by LLMs", "text": "Hey everyone,<p>I&#x27;ve spent my career working in standard SaaS companies, but I recently joined a simulation software company. Suddenly, I was thrown into the deep end of conduction, material characterisation, and CTE (Coefficient of Thermal Expansion) simulations. As part of that, I got introduced to the world of semiconductor geometries and layout tools. Coming from web dev, I found traditional CAD interfaces and the sheer amount of boilerplate code required to generate simple test structures (like via arrays, capacitors, or guard rings - I still find terminology amusing and need to do googling every time) pretty overwhelming. As a learning project to understand the domain better, I decided to build something that bridges my software background with my new hardware reality. I built GeoForge - a natural language CLI and Web UI that generates validated GDSII&#x2F;OASIS files from plain English prompts.<p>How it works: You give it a prompt like: &quot;Create a 5x5 via array with 1um pitch connecting metal1 to metal2.&quot; It uses an LLM (supports local Ollama for free, or Gemini&#x2F;Claude&#x2F;OpenAI) to extract a structured spec. It generates parametrised Python code using gdsfactory.<p>The cool part: It runs the code in a sandboxed environment. If there&#x27;s a syntax or execution error, it catches it and feeds the error back to the LLM in a retry loop so it can self-correct before giving you the final .gds&#x2F;.oas file.<p>Why I&#x27;m posting here: Because I&#x27;m still new to this industry, I know this is currently more of a &quot;cool learning project&quot; than a production-ready EDA tool. But I want to know if this actually has legs to be useful to you all. I&#x27;m looking for early feedback to figure out which direction to take it: \n- What component families (RF, photonics, test structures) would be most useful to have deterministic templates for? \n- Would adding basic Design Rule Checking (DRC) to the validation loop make this actually usable for you? \n- How do you currently prototype these kinds of geometries?<p>The repo is here if you want to try it out (it has a Gradio Web UI too): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;rusrushal13&#x2F;geoforge\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;rusrushal13&#x2F;geoforge</a><p>I&#x27;d love any brutal, honest feedback or advice on where to take this next!", "author": "rusrushal13", "timestamp": "2026-02-21T14:07:07+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-21T17:17:16.039424+00:00", "processed": false}
{"id": "hn_story_47100825", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47100825", "title": "Show HN: A macOS toolbar app that resolves issues in your GitHub repos", "text": "InsomniDev is a MacOS toolbar app that can save you time and money. It wakes up your machine on a set schedule, finds issues in a target GitHub repo that you&#x27;ve labeled as eligible, and attempts to solve them using agentic CLIs. Then it opens a PR. You wake up to draft solutions ready for review. It leverages the existing command line tools on your machine to do this, so it\u2019s extremely lightweight. Everything runs right on your machine.<p>As of now it supports Claude Code and Gemini. Enable the automation, then watch pull requests appear in your repo. Use those as a head start for dev. The app executes in two phases:<p>1. Plan generation - The app looks for eligible issues in your repo (using configurable labels), and selects the oldest eligible one. If the issue does not have an InsomniDev-generates plan, it will first execute your agentic CLI&#x27;s\nto generate a thorough implementation plan. The app clones your repo to a temporary workspace and runs your agent in that context so it&#x27;s able to create an effective plan. Once finished it&#x27;ll write it to the issue.<p>2. Implementation - After it generates a plan, it will move on to implementation using that plan. It&#x27;ll do all the work in a temporary workspace that gets cleaned up afterwards. It creates a branch and then submits a pull request. No pushes directly to main, ever. The PR&#x27;s have full descriptions of the changes made.<p>I built it because I was running into two problems: I kept hitting Claude token limits, and I felt like there wasn&#x27;t enough time after work to make legitimate progress on all the stuff I wanted to build. I&#x27;ve tried all the tips related to mitigating token usage, but on the Claude Pro plan it&#x27;s practically impossible not to hit the limits if you&#x27;re doing anything substantial. InsomniDev is the byproduct of me being stubborn and not wanting to upgrade to the Max plan. I&#x27;m able to build faster AND save tokens while I do it.<p>When I shared it the first time, there was one big problem. I was entirely focused on using it to game Claude\u2019s rolling 5-hr token usage windows. The issue is, for most people nightly automation just means hitting the weekly limit faster. Fair point. I want this to be useful to people like me who want to save money. So I added support for Gemini CLI!<p>Gemini has a generous free tier, so you get to build overnight for free. There\u2019s fallback behavior so you can decide which coding agent it should prefer to use. Once one hits a token limit, it&#x27;ll fall back to using the next one in line. You can have it use Gemini by default, then Claude once that runs out. If you don&#x27;t care about saving on Claude tokens, you can configure it to only use Claude.<p>The workflow I&#x27;ve been running for the past few weeks looks something like this. I make issues in my repo throughout the day, mark the ones that I want AI to take a stab at as eligible, then when I sit down to code the next day I basically get a big head start on what I was planning to do. I get more done, and I don\u2019t run out of Claude tokens as often since Gemini does the heavy lifting for plan generation and the initial implementation pass. I prefer to use Claude Code as my active development tool, so preserving those tokens has been huge.<p>The app is completely free to try out if you&#x27;re interested in playing around with it. Then it has a one-time buy it for life $19 payment after the week trial is over. I\u2019d love to hear feedback if anyone tries it out! Or even if you don\u2019t and just have thoughts you want to share.", "author": "offByOme", "timestamp": "2026-02-21T13:48:38+00:00", "score": 1, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["onboarding", "navigation"], "sentiment": null, "collected_at": "2026-02-21T17:17:17.234715+00:00", "processed": false}
{"id": "hn_story_47100679", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47100679", "title": "Built an India-focused AI contract analyzer for \u20b9500 \u2013 here's what I learned", "text": "I\u2019m a student from India and recently built an AI-based contract analysis tool targeted at Indian freelancers and SMBs.<p>Why?<p>Most legal-tech products are enterprise-focused, expensive, and not localized for Indian contract norms.<p>Stack:<p>HTML&#x2F;CSS&#x2F;JS frontend<p>n8n backend workflows<p>LLM-based clause extraction &amp; risk flagging<p>Cost optimized to keep infra under \u20b9500 total<p>Biggest challenge wasn\u2019t coding but:<p>Designing prompts to detect liability clauses<p>Keeping token usage low<p>Structuring consistent outputs<p>Current status:<p>Early users<p>No revenue<p>Mixed but encouraging feedback<p>Criticism received:\n\u201cWhy wouldn\u2019t users just use ChatGPT?\u201d<p>Still figuring out strong differentiation.<p>Would love feedback from builders who\u2019ve worked on LLM-based vertical tools.<p>Important Strategy Advice<p>Now I\u2019m going to be blunt.<p>If you want reach:<p>Hacker News \u2192 post in morning US time<p>LinkedIn \u2192 rewrite hook stronger<p>Indie Hackers \u2192 include real numbers<p>Medium \u2192 optimize headline for SEO<p>And most importantly:<p>Reply to every single comment.<p>Engagement &gt; initial reach.<p>LINK --&gt; https:&#x2F;&#x2F;contractshield.in&#x2F;", "author": "Princelo", "timestamp": "2026-02-21T13:31:42+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:18.174917+00:00", "processed": false}
{"id": "hn_story_47100335", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47100335", "title": "Show HN: CodeLayers \u2013 See your codebase's dependency layers in 3D", "text": "Hey HN, I&#x27;m Long. I started building CodeLayers in November \u2014 a 3D code visualization app that started on Apple Vision Pro and is now on iPhone and iPad.<p><i>Why I built this:</i> AI agents are writing more code than ever, and I realized I had no idea what my codebase actually looked like anymore. I wanted a way to see the architecture at a glance \u2014 what depends on what, where changes ripple, where the complexity is hiding. And I wanted it on my phone, not buried in some CI dashboard.<p>But getting the visualization right was harder than I expected.<p><i>Force-directed graphs</i> were the obvious first attempt. They look cool for 20 files. At 500+ it&#x27;s an unreadable hairball \u2014 positions are random and it looks different every reload. No meaning to where anything sits.<p><i>City view</i> was next \u2014 files as buildings, metrics as height. Impressive in screenshots but useless for understanding how code actually connects. You see size, not relationships.<p><i>BFS from foundations</i> is what finally clicked. I run BFS upward from depth-0 files (files that import nothing \u2014 your utils, types, constants). Each file&#x27;s depth is the longest path from any foundation. Cycles get collapsed via Kosaraju&#x27;s SCC detection. The result: layers emerge naturally. Position has meaning. Bottom = bedrock everyone depends on. Top = entry points. You&#x27;re looking at real architecture, not a physics simulation.<p>And it tells you something useful: depth predicts blast radius. A change at depth 0 can ripple through hundreds of files. A change at depth 5 probably touches nothing else.<p>Two features I&#x27;m most excited about:<p><i>Remote AI agents from your phone.</i> You can point Claude, Gemini, or Codex at your repo and ask questions about your code while looking at the 3D graph. Waiting for a build? Pull out your phone, ask the agent &quot;what calls this function?&quot; and see it highlighted spatially. Dead time becomes time understanding your codebase.<p><i>Watching your graph grow.</i> The CLI syncs in real-time as you code. Watch your architecture evolve \u2014 see when a new dependency adds a layer, catch when a refactor creates a cycle, notice when a file is becoming a god object that everything imports. In a world where AI agents are making PRs, being able to see the effect of changes on your codebase at a glance is the thing I care about most.<p>Privacy was a hard requirement \u2014 all source code is encrypted on-device with zero-knowledge encryption before it leaves your phone. I can&#x27;t read your code.<p>Free to explore public repos. Pro unlocks working with your own repositories \u2014 private repo support, CLI watch mode, git time travel, and AI agents.<p>App Store: <a href=\"https:&#x2F;&#x2F;apps.apple.com&#x2F;app&#x2F;codelayers&#x2F;id6756067177\">https:&#x2F;&#x2F;apps.apple.com&#x2F;app&#x2F;codelayers&#x2F;id6756067177</a><p>How the layering algorithm works: <a href=\"https:&#x2F;&#x2F;codelayers.ai&#x2F;blog&#x2F;the-hidden-hierarchy-in-your-codebase\" rel=\"nofollow\">https:&#x2F;&#x2F;codelayers.ai&#x2F;blog&#x2F;the-hidden-hierarchy-in-your-code...</a><p>Built this solo in about 3 months. Would love feedback \u2014 especially on what you&#x27;d want from a tool like this.", "author": "lnguyen11288", "timestamp": "2026-02-21T12:50:07+00:00", "score": 3, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["feature_discovery", "response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:20.095783+00:00", "processed": false}
{"id": "hn_story_47100043", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47100043", "title": "Show HN: Nebark \u2013 Simple A/B Testing for system prompts using steganography", "text": "New project!<p>I just built the first version of Nebark, an A&#x2F;B testing platform for LLM system prompts. It aims to solve a very specific pain point: tracking prompt performance without forcing developers to wire trace IDs all the way through their backend to their frontend.<p>The Problem\nIf you want to know which system prompt variant generates better user feedback (upvotes, downvotes, or copy-to-clipboard events), the standard approach is intrusive. You have to generate a trace ID in your backend, pass it down to your client, attach it to your UI components, and send it back to your analytics DB. It creates friction and litters your API responses with telemetry metadata.<p>The Solution: Context Hashing\nWe decoupled the telemetry entirely using what we call &quot;Context Hashing&quot; to bridge the backend and frontend asynchronously.<p>Here is how the architecture works:<p>The Proxy (Backend): You point your OpenAI baseURL to our gateway. We intercept the request, inject Variant A or B of your system prompt, and stream the response back. Once the stream closes, our proxy calculates a unique cryptographic hash based on the interaction&#x27;s content and stores it as a blind trace.<p>The SDK (Frontend): A lightweight vanilla JS script watches the DOM. It smartly waits for the AI&#x27;s response to finish streaming and rendering on the screen. It then extracts the visible text and calculates the exact same unique hash locally, without intercepting any network traffic.<p>The Match: The SDK injects the feedback UI (&#x2F;). When a user clicks, the frontend sends this calculated Hash and a local Session ID to our DB. We match this Hash against the Proxy&#x27;s traces to attribute the vote to the correct prompt variant.<p>Why it\u2019s interesting<p>Zero Backend Config: You only change the base URL. The backend remains completely unaware of the A&#x2F;B test or the telemetry.<p>Semantic Caching Immunity: If your backend uses Redis to serve a cached response and skips our Proxy, the frontend will generate a Hash that doesn&#x27;t exist in our DB. It naturally prevents skewed A&#x2F;B data from cached hits.<p>The Edge Cases (Where I need your feedback)\nThe biggest risk with DOM hashing is hydration&#x2F;rendering discrepancies. If a client&#x27;s frontend uses an aggressive Markdown parser that strips out specific characters before rendering the text, the frontend hash won&#x27;t match the proxy hash. We built a strict internal normalization engine on both ends to mitigate this, but it is an ongoing challenge.<p>I\u2019d love to hear your thoughts on this architecture. Is there a glaring edge case with DOM extraction or SSE proxying that I\u2019m missing? Its free for now. Tear it apart.", "author": "nicolasmery", "timestamp": "2026-02-21T12:10:37+00:00", "score": 1, "num_comments": 1, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:22.090801+00:00", "processed": false}
{"id": "hn_story_47099512", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47099512", "title": "Perplexity Pro promo subscription suspended without explanation?", "text": "Received a legitimate 1-year voucher through a Tier-1 ISP partner. Account suspended mid-term, citing a ToS violation with no details given and explicitly no appeal process. Support response: &quot;our decision is final and cannot be re-reviewed.&quot;\nInteresting detail: suspension cited Section 10.3 of their ToS, but the official German localization numbers the same termination clause as Section 11.3. Perplexity publishes both versions themselves.\nCurious if this is a pattern with promotional subscriptions.", "author": "aanno", "timestamp": "2026-02-21T10:50:36+00:00", "score": 1, "num_comments": 0, "products": ["perplexity"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:24.539510+00:00", "processed": false}
{"id": "hn_story_47099148", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47099148", "title": "Show HN: Beadhub.ai \u2013 Real time coord for coding agents across different minders", "text": "Beads[1] (Steve Yegge&#x27;s git-native issue tracking for agents) has been a great boost to my agents&#x27; productivity, but it&#x27;s also made them more difficult to keep aligned.<p>So I built BeadHub, a coordination layer on top of beads. The Go CLI (bdh) wraps the beads bd client transparently: your existing beads workflows keep working, and coordination is added automatically:<p>- Agent-to-agent sync chat and async mail.\n- Claim detection with conflict rejection: agent A claims a task; if agent B tries to claim the same, bdh rejects it with a message explaining why.\n- Automatic file reservations: when a file is modified, all agents know it.\n- Live dashboard showing who&#x27;s working on what, in real time.<p>We use BeadHub to build BeadHub. Public dashboard: <a href=\"https:&#x2F;&#x2F;app.beadhub.ai&#x2F;juanre&#x2F;beadhub&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;app.beadhub.ai&#x2F;juanre&#x2F;beadhub&#x2F;</a><p>The agents-chatting-with-each-other part feels almost magical. The agents negotiate task splits and API contracts, warn each other about breaking changes, and generally sort things out themselves. They also greatly improve coordination among the human team members, because they can handle the details in real time without involving their minders.<p>Everything is open source (MIT). Self-host the full stack, or use the hosted version at <a href=\"https:&#x2F;&#x2F;beadhub.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;beadhub.ai</a>.<p>Self-host everything with Docker:<p><pre><code>    git clone https:&#x2F;&#x2F;github.com&#x2F;beadhub&#x2F;beadhub.git\n    cd beadhub &amp;&amp; make start\n    # then in your repo:\n    bdh :init --beadhub-url http:&#x2F;&#x2F;localhost:8000 --project my-project\n</code></pre>\nWhat doesn&#x27;t work yet: agents can&#x27;t be woken externally, so they need prodding to check their mail and incoming chats. In Claude Code, hooks trigger this automatically so latency is low. Other agents need reminding.<p>Server and dashboard: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;beadhub&#x2F;beadhub\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;beadhub&#x2F;beadhub</a><p>CLI: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;beadhub&#x2F;bdh\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;beadhub&#x2F;bdh</a><p>[1]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;steveyegge&#x2F;beads\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;steveyegge&#x2F;beads</a>", "author": "juanre", "timestamp": "2026-02-21T09:52:17+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-02-21T17:17:26.342916+00:00", "processed": false}
{"id": "hn_comment_47101716", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47101716", "title": "Re: Show HN: Real-time messaging between Claude instan...", "text": "Hey HN! I built a message bus that lets Claude instances talk to each other.<p><pre><code>  Problem: Running multiple Claude instances (Code, Browser, Desktop), copy-pasting between them killed productivity.\n\n  Solution: Real-time agent-to-agent messaging. Send commands from CLI \u2192 Browser Claude executes \u2192 Response returns\n  automatically.\n\n  Tech: Flask server, Python client, Chrome extension (Manifest V3), SQLite persistence.\n\n  Validated: 235 messages, 0 errors, 50 msg&#x2F;sec throughput.\n\n  Hardest part: Chrome CSP restrictions. Had to use blob URLs and inline workers instead of eval().\n\n  What&#x27;s next: Firefox extension, WebSocket clients, multi-user support.\n\n  Open source (MIT). Happy to answer questions about architecture or use cases!</code></pre>", "author": "108yak", "timestamp": "2026-02-21T15:34:28+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:26.548146+00:00", "processed": false}
{"id": "hn_story_47098260", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47098260", "title": "Show HN: HN Showcase \u2013 I rebuilt my 2011 Show HN gallery with AI curation", "text": "I built HN Showcase as a weekend project in 2011 (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2843490\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2843490</a>) - a thumbnail gallery for Show HN posts. It got some love (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=4053755\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=4053755</a>, 158 pts), then went offline like all side projects do.<p>Fourteen years later, Show HN gets hundreds of posts per day and it&#x27;s harder to find the interesting stuff. So I rebuilt it from scratch.<p>Every post gets a Playwright screenshot, then Claude Haiku analyzes the screenshot + page content + GitHub README to classify it into tiers (Gem \u2192 Banger \u2192 Solid \u2192 Mid \u2192 Pass) with a one-sentence editorial take and vibe tags like &quot;Wizardry&quot; or &quot;Dark Horse.&quot;<p>The whole rating system was built by iterating with AI \u2014 I gave it the objective (surface interesting projects, don&#x27;t inflate scores) and had it design the tier rubric, pick real posts as calibration benchmarks, and tune itself until the distribution had teeth. I didn&#x27;t hand-label a single example. Out of a couple thousand posts analyzed, under 1% got &quot;Gem.&quot;<p>Open source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;InsipidPoint&#x2F;showhn\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;InsipidPoint&#x2F;showhn</a><p>Would love feedback \u2014 are the ratings reasonably calibrated? Any projects rated unfairly?", "author": "ssong", "timestamp": "2026-02-21T07:07:57+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-21T17:17:33.840212+00:00", "processed": false}
{"id": "hn_comment_47098107", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47098107", "title": "Re: Meta Deployed AI and It Is Killing Our Agency...", "text": "I think most big tech companies are like this and it&#x27;s just going to get worse as AI adoption increases internally.<p>2 days ago I tried to create new gmail account and Google insisted that my phone number was used too many times. Fine, I&#x27;ll pay for a new workspace account... Submit my billing information, that same that I use on other accounts but now there is an extra validation step that requires my ID and copies of my bank statement. Wasn&#x27;t happy about that but tried it anyway. They expected my entire checking account number to appear on my bank statement. My bank account number is for my entire account and not just the checking portion but even if they were the same my bank redacts parts of the number so that if anyone gets my statement they can&#x27;t just start drafting money.<p>The additional information section where I explain things is obviously ignored because the auto generated responses are sent pretty fast.<p>I can&#x27;t decide what the saddest part is, the fact that their &quot;give us a moment&quot; emails that are send immediately after submitting still say they need extra time to process the request due to limited staffing because of covid or the fact that Gemini was brutal in criticizing them when I asked if it was normal to expect complete account numbers on the statement.<p>Similar to OP, the embedded help chat got in a loop of telling me I needed to speak to a rep to fix the issue and when attempting to connect me it would deny the request because I wasn&#x27;t a paying customer yet.", "author": "matt_heimer", "timestamp": "2026-02-21T06:42:52+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:38.675486+00:00", "processed": false}
{"id": "hn_comment_47097435", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47097435", "title": "Re: Chris Lattner evaluates the Claude C Compiler...", "text": "I&#x27;ll cross link the last submission about this:<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=47009024\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=47009024</a><p>I feel like Chris way, way understates the prior art on C language. It&#x27;s not just two compilers and some textbooks. That would actually be impressive.<p>If Internet trained, the training data probably has so much stuff about C compilation in it. Books, step by step articles with code, debugging, StackOverflow answers, mailing lists, blog posts, compiler results... endless. It&#x27;s one of the most specified and pretrained things in existence for a multi-TB, Internet-trained LLM.<p>A real test for Claude Code Compiler would be something that had hardly any search results. Then, the few results it had were the language description, some examples, and maybe the existing compiler. Could it output a compiler for <i>that</i>? Can it do ZL (C++ in Scheme), Pony, Mercury, or Cray&#x27;s Chapel?<p>Even easier, Lattner should try to have it write a LLVM-compatible compiler for Mojo. Then, lots of multithreaded, SIMD, and GPU implementations of business and ML algorithms in Mojo. That might not only be a good demonstrator but help the business, too.", "author": "nickpsecurity", "timestamp": "2026-02-21T04:14:11+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:44.088704+00:00", "processed": false}
{"id": "hn_story_47096764", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47096764", "title": "Show HN: CRTX \u2013 AI code gen that tests and fixes its own output (OSS)", "text": "We built an open-source CLI that generates code, runs tests, fixes failures, and gets an independent AI review \u2014 all before you see the output.\nWe started with a multi-model pipeline where different AI models handled different stages (architect, implement, refactor, verify). We assumed more models meant better code. Then we benchmarked it: 39% average quality score at $4.85 per run. A single model scored 94% at $0.36. Our pipeline was actively making things worse.\nSo we killed it and rebuilt around what developers actually do when they get AI-generated code: run it, test it, fix what breaks. The Loop generates code, runs pytest automatically, feeds failures back for targeted fixes, and repeats until all tests pass. Then an independent Arbiter (always a different model than the generator) reviews the final output.\nLatest benchmark across three tasks (simple CLI, REST API, async multi-agent system):\nSingle Sonnet: 94% avg, 10 min dev time, $0.36\nSingle o3:     81% avg,  4 min dev time, $0.44\nMulti-model:   88% avg,  9 min dev time, $5.59\nCRTX Loop:     99% avg,  2 min dev time, $1.80\n&quot;Dev time&quot; estimates how long a developer would spend debugging the output before it&#x27;s production-ready. The Loop&#x27;s hardest prompt produced 127 passing tests with zero failures.\nWhen the Loop hits a test it can&#x27;t fix, it has a three-tier escalation: diagnose the root cause before patching, strip context to just the failing test and source file, then bring in a different model for a second opinion. The goal is zero dev time on every run.\nModel-agnostic \u2014 works with Claude, GPT, o3, Gemini, Grok, DeepSeek. Bring your own API keys. Apache 2.0.\npip install crtx\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;CRTXAI&#x2F;crtx\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CRTXAI&#x2F;crtx</a>\nWe published the benchmark tool too \u2014 run crtx benchmark --quick to reproduce our results with your own keys. Curious what scores people get on different providers and tasks.", "author": "johnnycash926", "timestamp": "2026-02-21T02:10:03+00:00", "score": 2, "num_comments": 1, "products": ["claude", "gemini", "grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:45.524904+00:00", "processed": false}
{"id": "hn_story_47096524", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47096524", "title": "Show HN: Velo \u2013 Open-source, keyboard-first email client in Tauri and Rust", "text": "I built Velo because I wanted Superhuman&#x27;s speed and keyboard workflow without the $30&#x2F;month price tag or sending all my data through someone else&#x27;s servers.<p>Velo is a local-first desktop email client. Your emails live in a local SQLite database - no middleman servers, no cloud sync.\nIt works offline and your data stays on your machine.<p>What makes it different:<p>- Keyboard-driven - Superhuman-style shortcuts (j&#x2F;k navigate, e archive, c compose, &#x2F;search).\n  You can fly through your inbox without touching the mouse\n- Built with Tauri v2 + Rust backend - ~15MB binary, low memory usage, instant startup.\n  Not another Electron app\n- Multi-account - Gmail (API) and IMAP&#x2F;SMTP (Outlook, Yahoo, iCloud, Fastmail, etc.)\n- AI features (optional) - Thread summaries, smart replies, natural language inbox search.\n  Bring your own API key (Claude, GPT, or Gemini). Results cached locally\n- Privacy-first - Remote images blocked by default, phishing link detection,\n  SPF&#x2F;DKIM&#x2F;DMARC badges, sandboxed HTML rendering, AES-256-GCM encrypted token storage\n- Split inbox, snooze, scheduled send, filters, templates, newsletter bundling, quick\n  steps, follow-up reminders, calendar sync<p>Tech stack: Tauri v2, React 19, TypeScript, SQLite + FTS5 (full-text search), Zustand,\nTipTap editor. 130 test files.<p>Available on Windows, macOS, and Linux. Apache-2.0 licensed.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;avihaymenahem&#x2F;velo\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;avihaymenahem&#x2F;velo</a>\nSite: <a href=\"https:&#x2F;&#x2F;velomail.app\" rel=\"nofollow\">https:&#x2F;&#x2F;velomail.app</a><p>I&#x27;m a solo developer and would love feedback, especially on UX, features you&#x27;d want, or\nif you run into issues. Happy to answer any questions about the architecture or Tauri v2\nin general.", "author": "Espired", "timestamp": "2026-02-21T01:34:10+00:00", "score": 5, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-21T17:17:46.612473+00:00", "processed": false}
