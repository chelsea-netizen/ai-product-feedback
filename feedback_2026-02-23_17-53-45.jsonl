{"id": "hn_story_47125907", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47125907", "title": "Show HN: OmniGlass \u2013 An open-source, sandboxed Visual Action Engine", "text": "Hey HN,<p>We\u2019ve reached peak &quot;chat interface.&quot; Claude Desktop and OpenAI are amazing at coding, but they are terrible for repetitive, blue-collar, or legacy enterprise workflows.<p>You cannot deploy Claude Desktop to a warehouse floor or an auto repair shop and expect workers to prompt-engineer their way through a proprietary GUI or a locked PDF every 5 minutes. They don&#x27;t want a conversation; they want a button that does the job.<p>So I built OmniGlass (Rust&#x2F;Tauri). It is an open-source, local-first runtime that lets developers turn any MCP (Model Context Protocol) tool into a 1-click OS-level visual action.<p>How developers use it:\nYou write a standard MCP server (e.g., a 30-line Node script that queries an internal database). OmniGlass handles the visual pipeline:<p>The Trigger: The user snips a region of their screen.<p>The Vision: Native OS OCR (Apple Vision) extracts the text locally (no screen streaming).<p>The &quot;Brain&quot;: An LLM dynamically maps the raw OCR text to your MCP tool\u2019s strict JSON input_schema.<p>The Action: OmniGlass instantly renders a native button on the screen. The user clicks it. The tool executes. Zero prompting.<p>The Real-World Use Case (Why this matters):\nI found a $1,000 bounty on ProblemHunt from an auto shop. Mechanics are wasting hours looking at informal WhatsApp messages (&quot;left radiator plug, Chery Tiggo&quot;) and manually cross-referencing them against messy, unsearchable Chinese auto-part PDFs.<p>A standard AI chat app fails here because of the friction. But with OmniGlass, a developer can build a &quot;Find Auto Part&quot; MCP plugin. You deploy it to the mechanic&#x27;s Mac. The mechanic just snips the WhatsApp message, clicks the &quot;Find Part&quot; button that pops up, and the plugin queries the supplier API. The mechanic never talks to an AI. It just executes.<p>The Security Sandbox (Why not just use Claude&#x27;s MCP integration?):\nClaude Desktop runs MCP tools with raw user permissions. A hallucination or prompt injection can read your ~&#x2F;.ssh keys. Since OmniGlass is meant to be deployed in businesses, it runs entirely Zero-Trust. Every MCP plugin is locked inside a macOS sandbox-exec kernel profile. The &#x2F;Users&#x2F; directory is default-deny.<p>My Asks for HN:\nThis is infrastructure for developers to build on, and I need help pushing it forward:<p>The Auto Parts Bounty: I want to build the Chinese auto parts plugin to prove the platform. If anyone here has experience scraping or querying B2B auto supplier databases, let&#x27;s collaborate and claim this bounty.<p>Break my sandbox: If you can write an MCP plugin that escapes my sandbox-exec profile and reads a protected file, please open a GitHub issue immediately. I want this bulletproofed.<p>OS Ports: The Rust core is solid, but if any low-level devs know Windows AppContainer or Linux Bubblewrap, I\u2019d love help porting the security moat.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;goshtasb&#x2F;omniglass\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;goshtasb&#x2F;omniglass</a>\nDocs&#x2F;Plugin Guide: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;goshtasb&#x2F;omniglass&#x2F;blob&#x2F;main&#x2F;docs&#x2F;plugin-guide.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;goshtasb&#x2F;omniglass&#x2F;blob&#x2F;main&#x2F;docs&#x2F;plugin-...</a><p>What legacy enterprise workflows would you build a visual tool for?", "author": "goshtasb", "timestamp": "2026-02-23T17:53:09+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-23T17:53:45.675871+00:00", "processed": false}
{"id": "hn_story_47125841", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47125841", "title": "Show HN: DealLedger \u2013 An open ledger of every business for sale in America", "text": "I&#x27;m building an open, verifiable record of businesses for sale in the US. Think EDGAR for Main Street.\nThe problem: ~7,500 business brokers each maintain their own listings on their own websites. There&#x27;s no central registry, no standardized data, and no way to audit what&#x27;s actually on the market. The same listing might appear on 4 different sites. A business that sold 6 months ago can still show as &quot;available.&quot;\nDealLedger scrapes 1,700 broker websites daily using a combination of specialized scrapers for major franchise networks and ML-based pattern detection for the long tail. Every listing is source-linked, timestamped, and hashed. Snapshots are committed to GitHub daily.\nIt&#x27;s not a marketplace. We don&#x27;t rank, recommend, or broker. It&#x27;s infrastructure.\nThe stack: Python scrapers, Playwright for JS-heavy sites, GitHub Actions for daily automation, an AI agent (Claude) that generates new scraper configs from any broker URL you submit. Everything outputs to flat files \u2014 JSON&#x2F;CSV committed to git. The git history IS the ledger.\nWhat&#x27;s open source:<p>The scraper framework (specialized + ML-based)\nThe broker registry (1,700 URLs)\nAn AI scraper builder agent\nAll daily data snapshots\nThe methodology documentation<p>Live data browser + broker submission form: <a href=\"https:&#x2F;&#x2F;dealledger.org\" rel=\"nofollow\">https:&#x2F;&#x2F;dealledger.org</a>\nSource: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jeffsosville&#x2F;dealledger\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jeffsosville&#x2F;dealledger</a>\nBackground: I&#x27;ve brokered businesses for 12 years (200+ transactions, $75M+). The data infrastructure in this industry hasn&#x27;t changed since 2005. This is my attempt to fix that, starting with transparency.\nLooking for: feedback on the approach, broker URLs we&#x27;re missing, and anyone interested in adding verticals (HVAC, landscaping, vending, etc.).", "author": "jsosville", "timestamp": "2026-02-23T17:49:20+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:45.956279+00:00", "processed": false}
{"id": "hn_comment_47125585", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47125585", "title": "Re: Paper Lantern \u2013 Best code methods are in papers. T...", "text": "The best engineering knowledge is locked in research papers. Paper Lantern unlocks it for your code.<p>Paper Lantern is an MCP server that distills 2M+ CS research papers into the right method for your problem \u2014 its tradeoffs, benchmarks, and how to implement it \u2014 delivered directly to your coding agent. Works with Claude Code, Cursor, Copilot, any MCP client.<p>Your coding agent can search for papers, but it&#x27;s searching the open web \u2014 not a purpose-built research index. And even when it finds papers, the hard questions remain: which methods actually matter for your problem? What are the tradeoffs at your scale? What was tried and failed? What should you actually implement? That reasoning lives in papers and it never reaches your code.<p>EXAMPLE\nask your agent to implement chunking for a RAG pipeline. Paper Lantern detects your context \u2014 multi-source corpus, accuracy-critical, technical documents \u2014 then searches across 2M+ papers and finds 4 from January 2026 that directly apply. It explains each technique in plain language, shows why it matters for your specific setup, synthesizes how they address different pipeline stages, and recommends what to start with and why \u2014 with implementation details your coding agent can act on immediately.<p>One of those papers: a cross-document topic-aligned chunking approach hitting 0.93 faithfulness vs 0.78 for semantic chunking (arxiv:2601.05265). Another: a pruning method that cuts input tokens 76% while improving answer quality (arxiv:2601.17532).<p>The index covers agent design, RAG and retrieval, LLM inference, fine-tuning, evaluation, search and ranking \u2014 hundreds of techniques across applied CS.<p>BACKGROUND\nI spent 7 years leading various LLM and RAG teams at AWS Bedrock (IIT-Bombay, Stanford). Paper Lantern started as a research discovery platform - this is the same engine with additional reasoning, now plugged into coding workflows via MCP.<p>Looking for engineers who use coding agents daily. Happy to answer questions about the search, the synthesis, or the MCP integration.<p>code.paperlantern.ai", "author": "kalpitdixit", "timestamp": "2026-02-23T17:33:27+00:00", "score": null, "num_comments": null, "products": ["claude", "copilot"], "categories": ["error_messages", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:46.894544+00:00", "processed": false}
{"id": "hn_comment_47125566", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47125566", "title": "Re: The Statistical Signature of LLMs...", "text": "Abstract: &quot;Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.&quot;", "author": "bikenaga", "timestamp": "2026-02-23T17:32:08+00:00", "score": null, "num_comments": null, "products": ["grok"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:47.185450+00:00", "processed": false}
{"id": "hn_comment_47124748", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47124748", "title": "Re: Show HN: 412 deterministic modules so AI agents st...", "text": "I built this because I got tired of watching AI agents generate shell commands that <i>almost</i> work.<p>Stuff like wrong curl flags, made-up Python APIs, or the same task producing slightly different output every run. After a while, it felt like the obvious fix was: stop asking the LLM to write code.<p>So in flyto-ai, the LLM doesn\u2019t write scripts. It just:\n- finds the right module\n- fills in params<p>Then the params are validated against the module schema before anything runs. If it\u2019s wrong, it fails early instead of blowing up halfway through execution.<p>There are 412 prebuilt modules right now (browser, HTTP, files, DB, image, notifications, etc.), so the model is mostly doing selection + parameterization, not improvising code.<p>Every run also outputs a reusable YAML workflow.<p>The part I\u2019m experimenting with now is \u201cblueprints\u201d:\nsuccessful runs get saved, and if a similar task comes in later, the agent can replay the blueprint directly (no LLM call), so you get the same result instantly and basically free.<p>Install is just:<p><pre><code>    pip install flyto-ai &amp;&amp; flyto-ai\n</code></pre>\nWorks with OpenAI &#x2F; Anthropic &#x2F; Ollama (local models). Apache-2.0.<p>Curious if other people here have landed on a similar split:\nLLM decides <i>what to do</i>, deterministic engine decides <i>how it gets done</i>.", "author": "ChesterHsu", "timestamp": "2026-02-23T16:41:25+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:52.801564+00:00", "processed": false}
{"id": "hn_story_47124474", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47124474", "title": "Show HN: MarkdownLM \u2013 Stop being the human middleware for your AI agent", "text": "I got tired of being the human middleware between my AI agent and my own codebase rules.<p>Every session my agent would start fresh with no memory of the architectural decisions we had made. It would confidently ignore naming conventions, bypass security patterns, and quietly undo things I had spent weeks getting right.<p>I tried CLAUDE.md, .cursorrules, plan files, task files. They all have the same ceiling: the agent treats them as suggestions, context windows bury them as sessions grow, and there is zero enforcement when they get violated. The prompt is the spec in AI-native development, and right now that spec disappears every time the chat closes.<p>MarkdownLM is my attempt to fix the layer below the agent, not by writing better prompts, but by treating your team&#x27;s engineering rules as infrastructure.<p>How it works:<p>Your knowledge base lives in structured categories: architecture decisions, security constraints, business logic, naming conventions, whatever your team actually cares about. When an agent makes a call, MarkdownLM uses semantic embeddings to pull only the relevant rules rather than flooding the prompt with your entire knowledge base. Out of 500 documents, the agent sees the 3 that matter for this specific task. That keeps context focused, tokens low, and the agent from getting lost in irrelevant rules.<p>Before generation, relevant context is injected. After generation, a validation gate checks the output against your rules and blocks violations with a receipt showing the specific rule, the reason, and the smallest suggested fix. When the agent hits something ambiguous with no rule coverage, it does not guess and ship. It stops, flags it as a gap, and routes it to whoever you have designated as the decision maker for that category.<p>Everything is MCP-native so it works across Cursor, Claude Code, and any MCP-compatible host without changing your workflow. The CLI lets you manage your knowledge base from the terminal like code: clone, diff, push, sync across your team.<p>What I learned building it:<p>The interesting part was realizing the cost structure. Using Google&#x27;s text-embedding-004 at $0.15 per million input tokens to retrieve the right 3 documents means the retrieval layer costs fractions of a cent per call. That cheap embedding lookup replaces what would otherwise be a 100k-token prompt. Lower cost and better results because focused context beats large context almost every time.<p>The gap resolution feature surprised me most in practice. Teams do not just have rule violations. They have rule gaps, situations the agent encounters that nobody thought to write a rule for yet. Surfacing those gaps as actionable items rather than silent guesses turned out to be as useful as the enforcement itself.<p>Current state:<p>Public beta. BYOK is free, no credit card. Your code never touches our servers. The CLI and MCP server are open source on GitHub. Will stay free for individuals because I know the pain.<p>Site: <a href=\"https:&#x2F;&#x2F;markdownlm.com\" rel=\"nofollow\">https:&#x2F;&#x2F;markdownlm.com</a>\nCLI: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;MarkdownLM&#x2F;cli\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;MarkdownLM&#x2F;cli</a>\nMCP: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;MarkdownLM&#x2F;mcp\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;MarkdownLM&#x2F;mcp</a><p>I am the solo founder. Brutal feedback is the only feedback I want.", "author": "sundancegh", "timestamp": "2026-02-23T16:24:43+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:53.337616+00:00", "processed": false}
{"id": "hn_story_47124230", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47124230", "title": "Show HN: sc-research \u2013 Social media analysis skill for AI agents (Reddit and X)", "text": "Hey HN! I built sc-research \u2014 an open source skill that gives AI coding assistants (Claude Code, Cursor, Windsurf) the ability to do social media analysis.<p>You just ask your agent something like &quot;What do people think about product X?&quot; or &quot;What&#x27;s trending this week?&quot; and it goes out, pulls live discussions from Reddit and X, and comes back with structured results + interactive dashboards.<p>It can do rankings, sentiment breakdowns, trend timelines, controversy mapping, and topic discovery \u2014 all backed by real quotes from real people.<p>How it works: it pulls live discussions from Reddit (via OpenAI) and X (via xAI&#x2F;Grok), classifies what kind of analysis your question needs (ranking, sentiment, trend, etc.), runs the analysis, and outputs structured JSON + an interactive dashboard. Install is just &quot;npm install -g sc-research&quot; and &quot;sc-research init --ai claude&quot;.<p>Stack: Bun&#x2F;Node, OpenAI + xAI APIs, Zod, Vite + TS for the dashboard.<p>Happy to answer any questions!", "author": "skainguyen1412", "timestamp": "2026-02-23T16:07:40+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:54.748763+00:00", "processed": false}
{"id": "hn_story_47124064", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47124064", "title": "Show HN: Agent Multiplexer \u2013 manage Claude Code via tmux", "text": "", "author": "Beefin", "timestamp": "2026-02-23T15:56:39+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-23T17:53:55.495713+00:00", "processed": false}
{"id": "hn_story_47124000", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47124000", "title": "Show HN: SpecterQA \u2013 AI personas test your web app, no scripts needed", "text": "SpecterQA is an open-source CLI for behavioral testing. Instead of writing test scripts with selectors, you define personas (YAML) and journeys (goals). The engine launches a real browser via Playwright, screenshots the page, sends it to Claude&#x27;s vision model, and the AI decides what to click&#x2F;type&#x2F;scroll. Loop until done or stuck.<p>The idea: test scripts break when markup changes. Vision-based tests break when the UX actually breaks.<p>Personas have attributes like technical comfort, patience, and role that shape navigation behavior. A &quot;frustrated non-technical admin&quot; navigates differently than a &quot;power user developer.&quot;<p><pre><code>  pip install specterqa\n  specterqa init\n  specterqa run -p demo\n</code></pre>\nCost: ~$0.30-$3.00&#x2F;run depending on journey length. Built-in budget caps. Requires Anthropic API key.<p>Previously called GhostQA \u2014 we rebranded after discovering ghostqa.com exists as an AI testing company. Clean break, no confusion.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;SyncTek-LLC&#x2F;specterqa\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;SyncTek-LLC&#x2F;specterqa</a><p>MIT license. Alpha (v0.3.0). Feedback welcome.", "author": "synctek", "timestamp": "2026-02-23T15:53:39+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-23T17:53:56.089034+00:00", "processed": false}
{"id": "hn_story_47123966", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47123966", "title": "Show HN: Vexp \u2013 Local-first context engine for AI coding agents", "text": "I built vexp to solve two problems I kept hitting with AI coding agents (Claude Code, Cursor, etc.):<p>1. Token waste: agents read entire files linearly to understand a codebase. On a medium TypeScript project, a single query was consuming ~18k tokens of context when only ~2.4k were relevant.<p>2. Session amnesia: every new session starts from zero. The agent re-reads the same files, re-discovers the same architecture, re-traces the same dependencies.<p>How it works:<p>- tree-sitter parses the codebase and extracts symbols + relationships into a SQLite dependency graph\n- When an agent requests context, vexp serves a token-budgeted &quot;capsule&quot; containing only the relevant nodes: the target function, its callers, imported types, cross-file dependencies \u2014 not entire files\n- Session memory: every agent interaction is auto-captured as an observation linked to specific graph nodes. Next session, relevant observations surface automatically via MCP\n- Staleness detection: when code changes, observations linked to modified symbols are flagged stale. The agent sees &quot;I learned X about this function, but it&#x27;s changed since then&quot;\n- Cross-repo: detects API contracts, shared types, and env contracts across multiple repos, all locally via workspace config<p>Architecture: native Rust binary + SQLite. No cloud, no embedding model, no external dependencies. Talks to agents via MCP (Model Context Protocol) stdio. Currently distributed as a VS Code extension but the MCP server is standalone.<p>The dependency graph is fully deterministic \u2014 AST structural analysis, not vector embeddings. Code relationships are facts (imports, calls, type references), not similarity scores. This means zero hallucination risk in the retrieval layer and trivial staleness detection (you know exactly which nodes changed).<p>Hybrid search for memory: FTS5 BM25 + TF-IDF cosine similarity + recency decay (7-day half-life) + graph proximity scoring - staleness penalty.<p>Supports 11 languages, 12+ MCP-compatible agents. Free tier includes context capsules + all session memory tools.<p><a href=\"https:&#x2F;&#x2F;vexp.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;vexp.dev</a><p>Happy to discuss the architectur; the most interesting design decisions were around the scoring algorithm for pivot node selection and how to link observations to graph nodes without making the staleness detection too aggressive or too lenient.", "author": "nicola_alessi", "timestamp": "2026-02-23T15:52:12+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-23T17:53:56.396970+00:00", "processed": false}
{"id": "hn_story_47123886", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47123886", "title": "Show HN: Claude plugin to automate Kubebuilder setup", "text": "Hey HN,<p>I kept spending 2\u20133 hours scaffolding Kubebuilder projects before writing any actual reconciliation logic \u2014 initializing the project, wiring the controller, configuring webhooks, setting up cert-manager, writing a Tiltfile for iteration.<p>So I built a Claude Code plugin that automates the repetitive setup and guides the full operator lifecycle via slash commands.<p>It wraps Kubebuilder and related tooling \u2014 it doesn\u2019t replace them.<p>Available commands:<p><pre><code>  &#x2F;k8s:prereqs - Checks and installs Go, Kubebuilder, kind, kubectl, Kustomize, Tilt.\n\n  &#x2F;k8s:create-cluster - Creates a kind cluster for local development.\n\n  &#x2F;k8s:create-operator - Scaffolds a controller-runtime operator including: CRD definitions, reconciliation skeleton, RBAC markers, webhook boilerplate, project layout, Tiltfile for live iteration\n\n  &#x2F;k8s:dev - Starts the Tilt-based dev loop.\n\n  &#x2F;k8s:verify - Validates CRDs, pods, webhooks, and cluster events.\n\n  &#x2F;k8s:checklist - Runs a safety and quality checklist before shipping.\n\n</code></pre>\nThe part I care most about is the safety hook.<p>It uses a PreToolUse hook to block kubectl&#x2F;helm&#x2F;kustomize commands unless the current context matches kind-*. This prevents accidental operations against non-local clusters during development \u2014 something I\u2019ve personally seen go wrong.<p>Install:<p><pre><code>  claude plugin marketplace add https:&#x2F;&#x2F;github.com&#x2F;Sagart-cactus&#x2F;claude-k8s-plugin\n\n  claude plugin install k8s@sagart-devtools\n\n</code></pre>\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Sagart-cactus&#x2F;claude-k8s-plugin\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Sagart-cactus&#x2F;claude-k8s-plugin</a><p>I built this partly to learn Claude Code\u2019s plugin system, and partly because I needed it while building OptiPod (a GitOps-safe Kubernetes resource rightsizing operator).<p>Happy to answer questions about:<p>- Claude plugin architecture\n- The hook system\n- Kubebuilder automation patterns\n- or operator lifecycle design.", "author": "saggy4", "timestamp": "2026-02-23T15:48:10+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:56.807384+00:00", "processed": false}
{"id": "hn_story_47123849", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47123849", "title": "Show HN: Honeypo(e)t \u2013 a honeypot that replies to every scan with a poem", "text": "Hi HN, I built a small internet artwork and security toy: a honeypot that looks like a misconfigured server, so scanners come knocking. Instead of an error page, every request gets a short poem tailored to what it was looking for. WordPress probes get haikus, .env hunters find verse hidden in fake credentials, brute-force attempts get meditations on doors and keys.<p>The idea came one night watching fail2ban logs scroll by. Thousands of knocks from all over the world, all silently ignored. What if the door answered?<p>Try it:<p><pre><code>  $ curl -A &#x27;python-requests&#x2F;2.28.1&#x27; https:&#x2F;&#x2F;honeypoet.art&#x2F;.env\n\n  # ========================================\n  # PRODUCTION ENVIRONMENT \u2014 DO NOT SHARE\n  # ========================================\n  APP_KEY=base64:dGhlcmUgaXMgbm90aGluZyBoZXJl\n  DB_HOST=127.0.0.1\n  DB_PASSWORD=a-long-hallway-with-no-doors\n  JWT_SECRET=you-could-have-been-anything-and-you-chose-this\n  API_KEY=listen-the-wind-is-rising\n\n  # if you&#x27;re reading this, you already know:\n  # the secret was never in the file.\n</code></pre>\nThe bots never read the responses (they check the status code and move on), but humans can: the gallery visualizes every knock on a world map and lets you browse the generated poems. Click a dot to filter by country, scroll to zoom in.<p>Poems are generated asynchronously by Granite 4.0 Tiny running locally on a single GPU. A 1B parameter model writing verse for machines that will never read it (yet).<p>Live: <a href=\"https:&#x2F;&#x2F;honeypoet.art\" rel=\"nofollow\">https:&#x2F;&#x2F;honeypoet.art</a>\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;vrontier&#x2F;honeypoet\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vrontier&#x2F;honeypoet</a><p>The public gallery shows aggregated locations and knock counts; individual IPs are masked and scan details are not published.<p>Built with PHP, Go, SQLite, vanilla JS, and a Claude instance named Loom who wrote the code and hand-crafted backfill poems for the first 500 visitors.", "author": "Honeypoet", "timestamp": "2026-02-23T15:46:12+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:56.951027+00:00", "processed": false}
{"id": "hn_comment_47125813", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47125813", "title": "Re: Anthropic Education the AI Fluency Index...", "text": "This report introduces the AI Fluency Index to measure how effectively individuals collaborate with AI tools like Claude using the 4D AI Fluency Framework. The study found that iterative refinement is the strongest driver of fluency, yet users tend to trust polished outputs too much, decreasing their critical oversight when AI generates complex artifacts. Developing these skills is essential as AI becomes a standard &#x27;thought partner&#x27; in professional and educational environments.<p>TL;DR generated at <a href=\"https:&#x2F;&#x2F;toolong.co&#x2F;s&#x2F;h7inab9y\" rel=\"nofollow\">https:&#x2F;&#x2F;toolong.co&#x2F;s&#x2F;h7inab9y</a>", "author": "a_void_sky", "timestamp": "2026-02-23T17:47:34+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:53:59.505506+00:00", "processed": false}
{"id": "hn_story_47123105", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47123105", "title": "Show HN: Ilove4o \u2013 a simple way to keep using GPT-4o", "text": "Hi HN,<p>When OpenAI started phasing out GPT-4o from the main ChatGPT interface, I noticed a surprising amount of backlash \u2014 not about benchmarks or features, but about tone.<p>A lot of people (myself included) felt that 4o had a certain conversational warmth that later models don\u2019t quite replicate in the same way. That difference was subtle, but noticeable.<p>So I built a small side project for myself: <a href=\"https:&#x2F;&#x2F;www.ilove4o.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.ilove4o.com&#x2F;</a><p>It\u2019s a minimal interface that connects directly to GPT-4o via the OpenAI API. No extra layers, no personality hacks \u2014 just a focused 4o-only chat experience.<p>I\u2019m sharing it here because:\n- There seems to be real user preference around model \u201cpersonality.\u201d\n- I\u2019m curious whether others noticed the same behavioral shift.\n- It raises an interesting question: how much of perceived \u201cfriendliness\u201d comes from system prompts, UI, or subtle model tuning?<p>If you try it, I\u2019d genuinely love feedback \u2014 especially from people who\u2019ve spent significant time with multiple model versions.<p>Happy to answer technical or architectural questions.", "author": "Tanjim", "timestamp": "2026-02-23T14:56:42+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:54:02.665548+00:00", "processed": false}
{"id": "hn_story_47122925", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47122925", "title": "Show HN: MEVA \u2013 A lightweight desktop Markdown viewer built with Rust (<15MB)", "text": "I built a 15MB desktop app that does one thing: renders markdown files beautifully and watches them for changes.<p>There are hundreds of markdown editors. Almost zero dedicated readers. When Claude Code or Cursor writes a design doc to disk, I don&#x27;t want to edit \u2014 I just want to read it. VS Code&#x27;s preview splits your workspace in half, browser renderers don&#x27;t watch files, and Obsidian wants a vault. I just needed a separate window with beautifully rendered markdown that updates live.<p>Tauri (Rust + system WebView) instead of Electron keeps it under 15MB. Renders LaTeX, Mermaid diagrams, and syntax-highlighted code blocks. File watching via Rust&#x27;s notify crate with debouncing. Fully offline, no accounts, no telemetry.<p>Free version has all core features. Paid adds tabs, themes, export.<p>Interactive demo: <a href=\"https:&#x2F;&#x2F;app.supademo.com&#x2F;demo&#x2F;cml7hp8pl6yyfzsad2v0ei2mn\" rel=\"nofollow\">https:&#x2F;&#x2F;app.supademo.com&#x2F;demo&#x2F;cml7hp8pl6yyfzsad2v0ei2mn</a><p><a href=\"https:&#x2F;&#x2F;usemeva.com\" rel=\"nofollow\">https:&#x2F;&#x2F;usemeva.com</a>", "author": "ss_meva", "timestamp": "2026-02-23T14:39:09+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-23T17:54:03.542519+00:00", "processed": false}
{"id": "hn_story_47122431", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47122431", "title": "Show HN: Attest \u2013 Test AI agents with 8-layer graduated assertions", "text": "I built Attest because every team I&#x27;ve seen building AI agents ends up writing the same ad-hoc pytest scaffolding \u2014 checking if the right tools were called, if cost stayed under budget, if the output made semantic sense. It works until the agent gets complex, then it collapses.<p>60\u201370% of what makes an agent correct is fully deterministic: tool call schemas, execution order, cost budgets, content format. Routing all of this through an LLM judge is expensive, slow, and unnecessarily non-deterministic. Attest exhausts deterministic checks first and only escalates when necessary.<p>The 8 layers: schema validation \u2192 cost&#x2F;perf constraints \u2192 trace structure (tool ordering, loop detection) \u2192 content validation \u2192 semantic similarity via local ONNX embeddings (no API key) \u2192 LLM-as-judge \u2192 simulation with fault injection \u2192 multi-agent trace tree evaluation.<p>Example:<p><pre><code>  from attest import agent, expect\n  from attest.trace import TraceBuilder\n\n  @agent(&quot;support-agent&quot;)\n  def support_agent(builder: TraceBuilder, user_message: str):\n      builder.add_tool_call(name=&quot;lookup_user&quot;, args={&quot;query&quot;: user_message}, result={...})\n      builder.add_tool_call(name=&quot;reset_password&quot;, args={&quot;user_id&quot;: &quot;U-123&quot;}, result={...})\n      builder.set_metadata(total_tokens=150, cost_usd=0.005, latency_ms=1200)\n      return {&quot;message&quot;: &quot;Your temporary password is abc123.&quot;}\n\n  def test_support_agent(attest):\n      result = support_agent(user_message=&quot;Reset my password&quot;)\n      chain = (\n          expect(result)\n          .cost_under(0.05)\n          .tools_called_in_order([&quot;lookup_user&quot;, &quot;reset_password&quot;])\n          .output_contains(&quot;temporary password&quot;)\n          .output_similar_to(&quot;password has been reset&quot;, threshold=0.8)\n      )\n      attest.evaluate(chain)\n</code></pre>\nThe .output_similar_to() call runs locally via ONNX Runtime \u2014 no embeddings API key required. Layers 1\u20135 are free or near-free. The LLM judge is only invoked for genuinely subjective quality assessment.<p>Architecture: single Go binary engine (1.7ms cold start, &lt;2ms for 100-step trace eval) with thin Python and TypeScript SDKs. All evaluation logic lives in the engine \u2014 both SDKs produce identical assertion results. 11 adapters covering OpenAI, Anthropic, Gemini, Ollama, LangChain, Google ADK, LlamaIndex, CrewAI, and OpenTelemetry.<p>v0.4.0 adds continuous evaluation with \u03c3-based drift detection, a plugin system, result history, and CLI scaffolding. The engine and Python SDK are stable across four releases. The TypeScript SDK is newer \u2014 API is stable, hasn&#x27;t been battle-tested at scale yet.<p>The simulation runtime is the part I&#x27;m most curious about feedback on. You can define persona-driven simulated users (friendly, confused, adversarial), inject faults (latency, errors, rate limits), and run your agent against all of them in a single test suite. Is this useful in practice for CI, or is it a solution looking for a problem?<p>Apache 2.0 licensed. No platform to self-host, no BSL, no infrastructure requirements.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;attest-framework&#x2F;attest\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;attest-framework&#x2F;attest</a>\nExamples: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;attest-framework&#x2F;attest-examples\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;attest-framework&#x2F;attest-examples</a>\nWebsite: <a href=\"https:&#x2F;&#x2F;attest-framework.github.io&#x2F;attest-website&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;attest-framework.github.io&#x2F;attest-website&#x2F;</a>\nInstall: pip install attest-ai &#x2F; npm install @attest-ai&#x2F;core", "author": "tommathews", "timestamp": "2026-02-23T14:00:59+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:54:06.872783+00:00", "processed": false}
{"id": "hn_comment_47122136", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47122136", "title": "Re: Show HN: AgentReady \u2013 Drop-in proxy that cuts LLM ...", "text": "AgentReady is an OpenAI-compatible proxy. You swap your base_url, and every prompt gets compressed before hitting the LLM \u2014 40-60% fewer tokens, same responses, same streaming.<p>It uses a deterministic rule-based engine (not another LLM call): removes filler words, simplifies verbose constructions, strips redundant connectors. ~5ms overhead.<p>Works with any OpenAI-compatible SDK: Python, Node, LangChain, LlamaIndex, CrewAI, Vercel AI SDK.<p>Free during beta, no credit card: <a href=\"https:&#x2F;&#x2F;agentready.cloud&#x2F;hn\" rel=\"nofollow\">https:&#x2F;&#x2F;agentready.cloud&#x2F;hn</a><p>Python: pip install agentready-sdk &amp;&amp; agentready init<p>Happy to answer any technical questions.", "author": "christalingx", "timestamp": "2026-02-23T13:36:03+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-02-23T17:54:08.772581+00:00", "processed": false}
{"id": "hn_comment_47122177", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47122177", "title": "Re: Second Chatbot Chess Championship [video]...", "text": "For a second year, he runs a Chess Championship using chatbots.<p>The openings are quite fine because they have a lot to copy from the training material, but once one of the chatbots make an unusual move it may get chaotic.<p>Some chatbots play quite well in unusual situations and others make ilegal moves, make piece apear from thin air or forget they still have a piece on the board. If you look carefully, many ilegal moves look sensible for a different game, like they are copying the move from the training material, but sometimes it&#x27;s difficult to guess.<p>Anyway, he accept most illegal moves to get funny content, unless the chatbot is cheating too much, sometimes deciding it arbitrarily. I&#x27;m not sure how this mess with the internal representation the other chatbot has. Once a ilegal move is played it&#x27;s hard to guess if the future ilegal moves are error or a different interpretation of the consequences of the ilegal move.<p>Quarter final 1: ChatGPT vs Gemini<p>Quarter final 2: Grok vs Copilot<p>Quarter final 3: Snapchat vs Claude<p>Quarter final 4: Meta AI vs DeepSeek", "author": "gus_massa", "timestamp": "2026-02-23T13:39:36+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini", "copilot", "grok"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-23T17:54:09.099629+00:00", "processed": false}
{"id": "hn_comment_47121950", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47121950", "title": "Re: Ask PH: Worktrees or isolated sandboxes for multi-...", "text": "Git worktrees have become the default recommendation for running parallel AI coding agents (Claude Code, Codex, Cursor, etc.), and I get the appeal \u2014 they&#x27;re just directories, zero orchestration, instant setup. I&#x27;ve seen people running 5 worktrees in a tmux grid with a separate Claude instance in each pane and it looks great on paper.<p>But I keep running into the same wall: worktrees isolate code, not the environment. Port 3000&#x2F;5432&#x2F;8080 still gets fought over. Two agents installing conflicting deps stomp on each other. Secrets in the host env leak into every subprocess any agent spawns.<p>Containers fix all of this cleanly, but now you&#x27;re managing image lifecycles, cleanup policies, and orchestration \u2014 which is a lot of overhead for what might be a solo dev workflow.\nCurious what people have actually found:<p>1&#x2F; At what scale or workload type did worktrees start visibly breaking down for you?<p>2&#x2F; Has anyone found a middle ground that doesn&#x27;t require full container orchestration?<p>3&#x2F; For those who moved to proper sandboxes: was the management overhead actually as painful day-to-day as it sounds upfront?", "author": "NBenkovich", "timestamp": "2026-02-23T13:18:36+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-02-23T17:54:09.453960+00:00", "processed": false}
