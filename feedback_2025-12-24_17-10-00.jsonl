{"id": "hn_story_46377239", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46377239", "title": "Show HN: AI that edits your files directly, no approvals", "text": "Hey HN, I&#x27;m building Aye Chat (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;acrotron&#x2F;aye-chat\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;acrotron&#x2F;aye-chat</a>), an open source AI-powered terminal workspace that lets you edit files, run shell commands, and ask AI to modify your codebase directly, all in one REPL session.<p>I built this because I got tired of the &quot;suggest -&gt; review -&gt; approve&quot; loop in existing AI coding tools. As models improve and generate proper code more often than not, manual approval started to feel unnecessary as long as there is a strong safety net to allow easy rollback of the changes.<p>Aye Chat applies changes automatically, but every AI edit is snapshotted locally, so you can instantly undo any change with a single command. This automatic file update with a safety net is the core idea.<p>In the same session, you can run shell commands, open Vim, and ask the AI to modify your code.<p>It supports multiple models via OpenRouter, direct OpenAI API usage with your key, and also includes an offline local model (Qwen2.5 Coder 7B).<p>You can watch a ~1-minute demo here: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;i-vGI6-kP4c\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;i-vGI6-kP4c</a><p>Basically, the typical workflow goes like this (instead of a chat window, you stay in your terminal):<p><pre><code>  $ aye chat # starts the session\n  &gt; fix the bug in server.py\n   Fixed undefined variable on line 42\n\n  &gt; vim server.py\n  [opens real Vim, returns to chat after]\n\n  &gt; refactor: make it async\n   Updated server.py with async&#x2F;await\n\n  &gt; pytest\n   Tests fail\n\n  &gt; restore\n   Reverted last changes\n</code></pre>\nI use Aye Chat both in my work projects and to build Aye Chat itself. Recently, I used it to implement a local vector search engine in just a few days.<p>Lower-level technical details that went into the tool:<p>The snapshot engine is a Python-based implementation that serves as a lightweight version control layer.<p>For retrieval, we intentionally avoided PyTorch to keep installs lightweight. Instead, we use ChromaDB with ONNXMiniLM-L6_V2 running on onnxruntime.<p>File indexing runs in the background using a fast coarse pass followed by AST-based refinement.<p>What I learned:<p>The key realization was that the bottleneck in AI coding is often the interface, not the model.<p>I also learned that early users do not accept a custom snapshot engine, so to make it professional-grade we are now integrating it with git refs.<p>What I&#x27;d love feedback on:<p>- Does the snapshot safety net give you enough confidence to let the AI write files directly, or does it still feel too risky?<p>- Shell integration: does the ability to execute native commands and prompt the AI from a unified terminal interface solve the context-switching problem for you?<p>There is a 1-line quick install:<p><pre><code>  pip install ayechat\n  </code></pre>\nHomebrew and Windows installer are also available.<p>It&#x27;s early days, but Aye Chat is working well and is legitimately the tool I reach for first when I want to iterate faster. I would love to get your feedback. Feel free to hop into the Discord (<a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;ZexraQYH77\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;ZexraQYH77</a>) and let me know how it goes. If you find it interesting, a repo star would mean a lot!", "author": "acro-v", "timestamp": "2025-12-24T17:04:13+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["navigation"], "sentiment": null, "collected_at": "2025-12-24T17:10:00.522370+00:00", "processed": false}
{"id": "hn_story_46376987", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46376987", "title": "Show HN: I built the fastest AI app builder that I can find", "text": "A lot of times I use GenAI to quickly prototype something like an app idea or a UI&#x2F;UX mock for a site. I&#x27;d like this text-to-UI experience to be as fast as possible to quickly iterate.<p>I&#x27;ve tried classic LLMs like ChatGPT&#x2F;Claude&#x2F;Gemini and dedicated text-to-app builders like Lovable&#x2F;Blink&#x2F;Bolt&#x2F;Replit. For the former the experience is still a bit crude - a lot of times I have to manually spin up the pages they create to see what&#x27;s going on. The latter looks fancy but requires a sign up, and then by the time I enter the prompt, the spinner spins forever to bootstrap a production ready app with databases and log-in, when my intention is just to use it myself and see if it works.<p>So after I sign out from work yesterday for Christmas break, I decided to vibe one myself and hence created Vibe Builder. The idea is simple:\n- Single page HTML. TailwindCSS. HTML components and JS blocks. No need to create fancy frameworks or templates when you can just vibe on DOM elements.\n- Build the app where you enter your prompt. Zero deployment hassle.\n- Stream everything, never wait for AI to fully finish their thought.\n- Optimize for time-to-first-UI-change. You get to se the changes live.\n- And post on HN to see if it works.<p>This is just a V1, as you can see it only generates dead UI. but i already had fun asking it to generate wild app ideas or clones of existing apps and see how fast AI puts things together.<p>Next, I&#x27;m considering using HTMX to add interactivity to the components, as well as have a vibe API router that actually handles interaction.<p>Let me know if it builds the app you have in mind!", "author": "yuedongze", "timestamp": "2025-12-24T16:34:59+00:00", "score": 1, "num_comments": 3, "products": ["claude", "chatgpt", "gemini"], "categories": ["navigation"], "sentiment": null, "collected_at": "2025-12-24T17:10:00.937070+00:00", "processed": false}
{"id": "hn_story_46376342", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46376342", "title": "Show HN: I wrote a Christmas-themed Space Invaders clone in 8086 Assembly", "text": "I&#x27;ve had a goal for the past three years to learn Assembly language. This December, I finally put some real effort into learning the language. I followed the tutorials in Oscar Toledo G&#x27;s &quot;Programming Boot Sector Games&quot; and used Gemini as a tutor to help explain the concepts I was stuck on. It was tempting at points to vibe code some of the trickiest pieces, but I found resisting the temptation and using Gemini just to be an expert tutor to answer my ignorant questions brought back the joy of coding again. The result is a Christmas-themed Space Invaders clone written in 8086 Assembly.<p>The code isn&#x27;t perfect, and neither are my line-by-line comments, but it&#x27;s a good exercise if anyone else is trying to learn a language.<p>The game compiles to a .com file and runs in DOSBox. I&#x27;m not trying to fit it into a boot sector (512 bytes) like the original tutorial (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;nanochess&#x2F;Invaders&#x2F;blob&#x2F;master&#x2F;invaders.asm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;nanochess&#x2F;Invaders&#x2F;blob&#x2F;master&#x2F;invaders.a...</a>), so my code assembles to more verbose (around 700 bytes).", "author": "20wenty", "timestamp": "2025-12-24T15:21:50+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:02.368850+00:00", "processed": false}
{"id": "hn_story_46376212", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46376212", "title": "Show HN: MonumentAI \u2013 Shazam for buildings (History without the boring parts)", "text": "Hi HN,<p>I&#x27;m Ozan, the developer behind MonumentAI.<p>I built this app because I enjoy traveling but find traditional audio guides and plaques incredibly boring. They usually focus on dates and architectural styles, skipping the interesting human stories\u2014the scandals, exiles, and secrets.<p>I wanted a &quot;Shazam for Buildings&quot; that feels like a local friend whispering the gossip in your ear, rather than a history textbook.<p>How it works:\n1. You take a photo of a landmark.\n2. The app identifies the location using Google Gemini&#x27;s vision capabilities.\n3. It generates a short, engaging story focused on the &quot;gossip&quot; or hidden history (also via Gemini).<p>It&#x27;s currently an iOS app built with SwiftUI.<p>The app is free to download (with a paid tier for unlimited scans), but you can try the core functionality without paying.<p>App Store Link: \n<a href=\"https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;monumentai-scan-explore&#x2F;id6756325340\">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;monumentai-scan-explore&#x2F;id6756...</a><p>I&#x27;d love to hear your feedback on the UX and the quality of the stories.<p>Thanks!", "author": "OzanYldz", "timestamp": "2025-12-24T15:07:39+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:02.597700+00:00", "processed": false}
{"id": "hn_story_46375315", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46375315", "title": "Show HN: CRD Wizard \u2013 A GUI for Kubernetes Custom Resource Definitions", "text": "Hey HN,<p>I\u2019ve been working with Kubernetes for a while now, and one thing that has always been a friction point for me is dealing with Custom Resource Definitions (CRDs).<p>We use them for everything\u2014monitoring, cert-manager, custom controllers\u2014but the tooling around them always felt a bit raw. Dealing with them usually meant running `kubectl get crds`, piping output to grep, or staring at 5,000-line YAML files just to figure out what fields were available in the schema.<p>It got frustratingly worse when I started managing multiple clusters. I found myself constantly context-switching just to check if a CRD was installed or to diff versions between environments. It felt like I was spending more time memorizing `kubectl` flags than actually working.<p>So, I started building *CRD Wizard* to scratch my own itch.<p>It started as a simple TUI to view resources, but I realized I needed more visual context. Now it\u2019s a full desktop app (Go backend + Next.js frontend) that auto-discovers your local kubeconfigs and gives you a unified interface to explore CRDs across all your clusters.<p>Over the last few weekends, I\u2019ve added a few features that I really wanted:<p>1.  *Multi-Cluster Support:* It loads all your contexts automatically. You can switch between clusters instantly without touching your terminal.\n2.  *Documentation Generator:* I realized I often needed to share CRD specs with devs who don&#x27;t have cluster access. I added a generator that turns any CRD (from your cluster or a Git URL) into a clean, searchable static HTML or Markdown page.\n3.  *AI Integration:* I hooked it up to local LLMs (Ollama) and Google Gemini. You can click a button to have the AI explain a complex schema or generate a sample manifest for you.<p>I wrote it in Go because I wanted it to be snappy and easy to distribute as a single binary.<p>It\u2019s open source, and I\u2019d love to hear what you think or what other pain points you run into with CRDs.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pehlicd&#x2F;crd-wizard\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pehlicd&#x2F;crd-wizard</a>", "author": "pehli", "timestamp": "2025-12-24T13:18:27+00:00", "score": 2, "num_comments": 0, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:05.465313+00:00", "processed": false}
{"id": "hn_story_46374734", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46374734", "title": "A (humble) new proposal for the FE ecosystem", "text": "For many years, I focused quietly on my work, but now I feel compelled to point out a problem that is becoming increasingly apparent.<p>--<p>1. Correct Model \u2260 Adopted Model<p>Historical fact: In the frontend ecosystem, the winners aren&#x27;t those who create the most accurate abstraction; they&#x27;re those who provide the \u201cfeel of working\u201d with the least friction.<p>The result: correct thought model \u2192 low adoption and incorrect but easy model \u2192 explosion.<p>This is no coincidence.<p>2. Why Didn&#x27;t Intent-Based &#x2F; Deterministic Models Succeed?<p>There are several obvious reasons.<p>Cognitive Tax:<p>Intent + FSM + timeline requires:<p>\u201cI know what I&#x27;m doing,\u201d \u201cI&#x27;m designing the lifecycle,\u201d \u201cI&#x27;m consciously producing the state.\u201d<p>But for today&#x27;s FE crowd, this isn&#x27;t a feature, it&#x27;s a barrier. Because the ecosystem rewarded: quick demo, quick job, quick CV line<p>For someone with this profile:<p>FSM = fear, determinism = unnecessary, explicit lifecycle = \u201coverengineering\u201d<p>Failure Tolerance is Very High in UI:<p>In the backend: wrong abstraction \u2192 system crashes, money is lost, data is corrupted.<p>In the UI: the spinner spins a moment too long, the state glitches, the user refreshes.<p>In other words: Frontend errors can be tolerated for a long time. This has allowed bad abstractions to survive.<p>React&#x27;s Side Effect: The \u201cHide, Save\u201d Culture:<p>React did this: hid the lifecycle, hid concurrency, hid reconciliation.<p>Result: a \u201cit works even if you don&#x27;t understand it\u201d culture.<p>This culture: grew ritual memorization, not engineering.<p>Write a hook, if it works, fine. But why does it work, how does it work? No one asks.<p>The Vibe-Coder Explosion is Not a Cause, but a Consequence<p>Because this situation didn&#x27;t start with ChatGPT, Gemini, Cluade or Copilot. These accelerated the existing decay. The groundwork was already laid.<p>There was already a crowd that didn&#x27;t know abstraction, didn&#x27;t know what state was, didn&#x27;t know what concurrency was, but had memorized the framework.<p>AI just did this: it formalized the feeling of \u201cI don&#x27;t need to think.\u201d<p>4. Not a Framework, but an Infrastructure Layer<p>The biggest mistake was: \u201cLet&#x27;s build a new UI framework.\u201d This dies, and it did die.<p>-&gt; \u201chttps:&#x2F;&#x2F;dayssincelastjsframework.com&#x2F;\u201d<p>However, if a structure is created that positions itself as a runtime layer, intent engine, state coordinator, commit resolver, it becomes an invisible layer above giants like React, Vue, and Svelte.<p>Adoption comes like this: \u201cUse it if you want\u201d or \u201cDon&#x27;t see it at all if you don&#x27;t want to.\u201d<p>5. The Most Important Lesson (Perhaps All of Them)<p>I&#x27;m writing this sentence clearly: \u201cThe frontend world is not producing engineering right now; it&#x27;s producing conveyor belt-like behavior.\u201d<p>That&#x27;s why the right abstraction doesn&#x27;t win immediately, but it is inevitable.<p>Because: UIs are becoming more stateful, AI interaction is increasing, concurrency is inevitable.<p>At this point: the \u201cspinner + hook\u201d model will collapse, and people will have to ask \u201cwhy?\u201d again.<p>And yes: \u201cIt&#x27;s pointless to shout for standards in this mess.\u201d<p>But when approached from the right layer, with the right problem, and the right pain point, this model inevitably creates value.<p>This isn&#x27;t about hype; it&#x27;s about patience.<p>---<p>After all these words, I realized there&#x27;s complaint, but where&#x27;s the solution?<p>I opened a GitHub repo and added an \u201cAI-supported\u201d RFC-Proposal. Participation is open to anyone who wants to join.<p>Thank you.<p>https:&#x2F;&#x2F;github.com&#x2F;laphilosophia&#x2F;temporal-intent-resolution", "author": "laphilosophia", "timestamp": "2025-12-24T11:43:11+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt", "gemini", "copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:06.781188+00:00", "processed": false}
{"id": "hn_comment_46374561", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46374561", "title": "Re: ChatGPT's CSS may hide model info (clip-path, opac...", "text": "This is a reproducible technical report on how ChatGPT\u2019s UI may hide backend model details via CSS. The DOM includes model strings like GPT-5-2, but CSS properties like `clip-path`, `opacity:0`, and `user-select:none` prevent users from seeing or selecting them.\nThis may be unintentional UX design\u2014or a systematic obfuscation. Either way, I believe it deserves public discussion.", "author": "Ayanonymous", "timestamp": "2025-12-24T11:13:18+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-24T17:10:07.099000+00:00", "processed": false}
{"id": "hn_story_46374520", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46374520", "title": "Show HN: WatchLLM \u2013 Semantic caching to cut LLM API costs by 70%", "text": "Hey HN! I just shipped WatchLLM - a semantic caching layer for LLM APIs that sits between your app and providers like OpenAI&#x2F;Claude&#x2F;Groq.<p>The problem: LLM API costs add up fast, especially when users ask similar questions in different ways (&quot;how do I reset my password&quot; vs &quot;I forgot my password&quot;).<p>The solution: Semantic caching. WatchLLM vectorizes prompts, checks for similar queries (95%+ similarity), and returns cached responses instantly (50ms). If it&#x27;s a miss, we forward to the actual API and cache for next time.<p>Built in 3 days with Node.js, TypeScript, React, Cloudflare Workers (edge deployment), D1, and Redis. Just added prompt normalization today to boost cache hit rates even further.<p>It&#x27;s drop-in - literally just change your baseURL and keep using your existing OpenAI&#x2F;Claude SDKs. No code changes needed.<p>Currently in beta with a generous free tier (50K requests&#x2F;month). Would love feedback from anyone building LLM apps - especially on the semantic similarity threshold and normalization strategies.<p>Live demo on the site shows real-time cache hits and savings.", "author": "Kaadz", "timestamp": "2025-12-24T11:08:12+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:07.207752+00:00", "processed": false}
{"id": "hn_story_46374391", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46374391", "title": "Show HN: I built a tool that creates videos out of React code", "text": "The one-liner: You give it a script, it generates a portrait video in 10 minutes. No stock footage, no templates\u2014it writes code that renders as video.<p>## Why I built this\nWe were building game dev courses at Outscal and needed to produce a lot of video content fast. Traditional video production was slow. AI video generators looked weird. So we tried a different approach: what if video was just code?<p>Turns out it works. We can replicate specific art styles, and the output is consistent and editable.<p>## What it can do\n- Generate 30-60 second shorts from a script\n- Match specific visual styles\n- Let you fix individual scenes via chat if something&#x27;s off<p>## What it can&#x27;t do (yet)\n- Custom art styles (we have a few presets)\n- Run in a web UI (API costs are too high right now, so it runs through Claude Code in terminal)<p>Occasionally a shape might render slightly off, but you can tweak individual scenes through chat.<p>## How it actually works\nThe tool uses Claude Code to generate React components for each scene in your script. These components animate and render exactly like a video would. When you&#x27;re done, you get a real video file.<p>It&#x27;s not &quot;AI video generation&quot; in the usual sense. There&#x27;s no diffusion model hallucinating frames. It&#x27;s closer to programmatic motion graphics\u2014but you don&#x27;t write the code, Claude does.<p>## What I&#x27;m looking for\n10 people who want to try it. Not paying customers\u2014just people who make (or want to make) Short form  and are comfortable enough with a terminal to run Claude Code.<p>You&#x27;d get access to our setup, and I&#x27;d want your honest feedback on what works and what doesn&#x27;t.<p>If this sounds interesting, try the project on git, it is open-source.", "author": "mayankkgrover", "timestamp": "2025-12-24T10:45:38+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["naming_terminology", "tone", "response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:07.941656+00:00", "processed": false}
{"id": "hn_comment_46371023", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46371023", "title": "Re: Americans Have Mixed Views of AI \u2013 and an Appetite...", "text": "I want to point out this part:<p>&gt; A question that was interesting, but didn\u2019t lead to a larger conclusion, was asking what actually happens when you ask a tool like ChatGPT a question. 45% think it looks up an exact answer in a database, and 21% think it follows a script of prewritten responses.", "author": "in-silico", "timestamp": "2025-12-24T00:10:59+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-24T17:10:16.174192+00:00", "processed": false}
{"id": "hn_comment_46371012", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46371012", "title": "Re: SwiftZilla \u2013 RAG with Official Apple Docs for Swif...", "text": "I&#x27;ve been getting increasingly frustrated with how much current LLMs (Claude, GPT, Gemini, etc.) hallucinate on modern Swift, especially since Swift 6.0 and now with 6.1&#x2F;6.2 changes rolling out. They confidently suggest deprecated SwiftUI modifiers, wrong concurrency patterns (actors, Sendable violations, etc.), or APIs that changed in recent betas.<p>So I built a narrow, focused RAG just for Apple&#x2F;Swift development.<p>What it covers:\n- Full official Apple documentation (~100k+ pages)\n- Swift API Design Guidelines\n- All WWDC session transcripts\n- Swift Evolution proposals (SEPs)<p>It&#x27;s deliberately not a general-purpose RAG. Only Apple ecosystem stuff, kept fresh.<p>It speaks MCP (Model Context Protocol) natively \u2014 SSE or STDIO \u2014 so it plugs straight into Cursor, Windsurf, Claude Desktop, or any custom agent that supports MCP.<p>I&#x27;ve been using it myself and shared it with a few iOS&#x2F;macOS&#x2F;tvOS friends over this month. Pricing is $3&#x2F;month (basically covers hosting&#x2F;indexing costs; cancel anytime). There&#x27;s a free tier with limits, and right now a shared community pool of ~9\u201310M tokens is open (bypasses daily limits while it lasts).<p><a href=\"https:&#x2F;&#x2F;swiftzilla.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;swiftzilla.dev</a><p>Curious to hear from others who use AI agents for Swift work:  \nHow do you currently deal with hallucinations on Swift 6+ features (Observation, macros, strict concurrency, etc.)?  \nDo you just manually paste docs, use web search in agents, or something else?<p>Open to any feedback, especially if you&#x27;ve tried similar setups and run into the same issues.<p>Thanks!", "author": "vituu", "timestamp": "2025-12-24T00:09:25+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-24T17:10:16.266891+00:00", "processed": false}
