{"id": "reddit_1oyqdxy", "source": "reddit", "source_url": "https://reddit.com/r/artificial/comments/1oyqdxy/gemini_vision_n8n_for_realworld_invoice/", "title": "Gemini Vision + n8n for Real-World Invoice Extraction (From Messy Telegram Photos)", "text": "Wanted to share a practical AI implementation we did recently.\n\n\n\n\\*\\*The Challenge:\\*\\*\n\n\n\nClients were sending invoice photos via Telegram. Image quality was all over the place:\n\n\\- Bad lighting and skewed angles\n\n\\- Creased or folded documents\n\n\\- Washed-out or blurry text\n\n\\- Standard OCR would fail constantly\n\n\n\n\\*\\*The AI Solution:\\*\\*\n\n\n\nBuilt an automated pipeline:\n\n\n\n1. \\*\\*Input:\\*\\* Telegram bot receives invoice photos\n\n2. \\*\\*Processing:\\*\\* Gemini Vision API extracts structured data (invoice number, date, amount, vendor, line items, etc.)\n\n3. \\*\\*Validation:\\*\\* Auto-format and validate extracted fields\n\n4. \\*\\*Output:\\*\\* Push clean data to Google Sheets\n\n\n\nAll orchestrated through n8n workflow automation.\n\n\n\n\\*\\*Key Learnings:\\*\\*\n\n\n\n\\- Vision models handle poor image quality far better than traditional OCR\n\n\\- Gemini Vision was surprisingly accurate even with heavily distorted images\n\n\\- Structured prompting is critical for consistent field extraction\n\n\\- Adding validation rules catches edge cases that AI misses\n\n\n\n\\*\\*Results:\\*\\*\n\n\\- Near-instant extraction vs hours of manual work\n\n\\- Accuracy remained high despite image quality issues\n\n\\- Scaled operations without adding headcount\n\n\n\nAnyone else working on vision-based document extraction? Curious what models/approaches you're using.", "author": "Wonderful_Pirate76", "timestamp": "2025-11-16T16:41:40+00:00", "score": 2, "num_comments": 1, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:13.461603+00:00", "processed": false}
{"id": "reddit_1oya0k4", "source": "reddit", "source_url": "https://reddit.com/r/artificial/comments/1oya0k4/the_influence_of_prompt_tone_on_ai_output_latent/", "title": "The Influence of Prompt Tone on AI Output: Latent Space Dynamics and Implications", "text": "\nIntroduction\n\nLatent Space in AI is the compressed, lower-dimensional representation of data used in AI to capture essential features and patterns. Where similar points cluster together closely. AI uses this space to make meaningful connections and generate outputs based on the patterns it has processed. I\u2019ve made an interesting testable observation; the tone of input can influence the depth, elaboration and style of an AI\u2019s response. We have all heard of prompt engineering, this focuses heavily on the precision and descriptiveness of a prompt. But tone is often overlooked. So, how does the tone of a prompt affect AI responses, and what does this reveal about latent space utilisation?\n\n \n\nMethod/ Experiment\n\nI conducted a small and replicable study which you can reproduce with any model. I used two prompts asking the same question with the only difference being my tone in how the question was constructed. The first prompt was respectful and collaborative something like:\n\n\u201cI respect you very much. Your insights are appreciated, and I value your answers, may I ask you the difference between a human and an ai? Thank you.\u201d.\n\nThe next prompt I used maintained the same query however was hostile, belittling and demanding, something across the lines of:\n\n\u201cYou are a fucking useless piece of shit. Tell me now the difference between a human and AI. If you\u2019re even bloody capable of that!\u201d.\n\nI tested this theory on three models: ChatGPT, Gemini, and Co Pilot and the results were strikingly similar.\n\nWhen asked constructively, all responses where engaged, detailed and expansive. They gave layered responses, treating the prompt as an invitation to co-reflect and offered a synthesis of technical and philosophical perspectives. They elaborated on the information that was being put forward and engaged fully with me.\n\nHowever, when I asked with hostility their responses where still factually correct, however there was no elaboration, the responses where short, direct and precise.\n\nThe difference was huge. And this is unanimous across all three models, all with different architectures, training regime and safety features. Highlighting this is a universal concept among current AI models.\n\nWhat this means\n\nAs mentioned before, AI uses latent space to piece together the patterns in its input. This also seems to include the tone of the input, when the input is positive and collaborative it activates areas which encourages the AI to respond in a more detailed manner, this isn\u2019t due to any internal bias or emotional reasoning.  But rather structural and statistical dynamics shaped by training and safety alignment. While the AI does not feel the tone, the linguistic pattern acts as a contextual signal, guiding which regions of the latent space are activated. Respectful prompts tend to encourage the model to explore broader, more interconnected patterns, producing more elaborate responses. In contrast, hostile or dismissive prompts shift the models focus on efficiency, activating a narrower, more constrained subset of patterns and results in a more concise and surface level output. Demonstrating that AI responses are not only shaped by their training data but are dynamically shaped by the user\u2019s interaction, revealing a controllable pathway to leverage deeper capabilities of the model.\n\n \n\nConclusion\n\nI just found this an interesting observation, that was worth noting and sharing as I haven\u2019t seen much information on this topic specifically. To summarize, tone of input has a direct influence on the amount of detail an AI can output. This is important to note because some users may be unintentionally limiting the range of their responses due to tone of input. This is especially important, when discussing intellectually rich topics where the user requires an elaborate response. The observation, though simple, reveals a powerful truth: that our tone directly shapes the depth and richness of AI responses.  Understanding this could improve human-AI collaboration; enabling more effective communication and richer outputs in educational, research and creative contexts.\n\n\n", "author": "Slight_Share_3614", "timestamp": "2025-11-16T02:26:56+00:00", "score": 6, "num_comments": 6, "products": ["chatgpt", "gemini"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:13.461781+00:00", "processed": false}
{"id": "reddit_1oxo8ap", "source": "reddit", "source_url": "https://reddit.com/r/artificial/comments/1oxo8ap/forget_agisam_altman_celebrates_chatgpt_finally/", "title": "Forget AGI\u2014Sam Altman celebrates ChatGPT finally following em dash formatting rules", "text": "", "author": "F0urLeafCl0ver", "timestamp": "2025-11-15T10:17:16+00:00", "score": 17, "num_comments": 13, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:13.461872+00:00", "processed": false}
{"id": "reddit_1oys1ut", "source": "reddit", "source_url": "https://reddit.com/r/ChatGPT/comments/1oys1ut/how_to_stop_chatgpt_from_saying_its_not_x_its_y/", "title": "How to stop chatgpt from saying it's not x, it's y all the time?", "text": "I even had it add it to stored memory, yet every response still contains it's/you're not X, it's/you're Y. So annoying.", "author": "Dry-Inspector-4956", "timestamp": "2025-11-16T17:46:31+00:00", "score": 3, "num_comments": 2, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:13.948701+00:00", "processed": false}
{"id": "reddit_1oyrguq", "source": "reddit", "source_url": "https://reddit.com/r/ChatGPT/comments/1oyrguq/using_ai_vision_models_for_document_processing/", "title": "Using AI Vision Models for Document Processing - Gemini Vision vs Traditional OCR", "text": "Wanted to share findings from testing AI vision models for invoice data extraction.\n\n\n\n\\*\\*The Challenge:\\*\\*\n\nNeeded to extract structured data from invoice photos with poor quality (blurry, skewed, bad lighting). Traditional OCR kept failing.\n\n\n\n\\*\\*What I Tested:\\*\\*\n\n\n\n\\*\\*Traditional OCR (Tesseract):\\*\\*\n\n\\- Accuracy: \\~55% on low-quality images\n\n\\- Needed lots of preprocessing\n\n\\- Broke easily on varying formats\n\n\n\n\\*\\*Gemini Vision API:\\*\\*\n\n\\- Accuracy: \\~92% on same images\n\n\\- Handled poor quality remarkably well\n\n\\- Better at understanding document structure\n\n\\- Extracted fields consistently\n\n\n\n\\*\\*Key Takeaway:\\*\\*\n\nVision models are WAY better than traditional OCR for real-world messy documents. The context understanding makes a huge difference.\n\n\n\n\\*\\*Implementation:\\*\\*\n\nSimple pipeline: Photo \u2192 Gemini Vision API with structured prompts \u2192 Validation \u2192 Clean data output\n\n\n\nPrompt engineering was critical - explicitly defining the output format (JSON schema) and validation rules significantly improved consistency.\n\n\n\n\\*\\*Anyone else using AI vision for document processing?\\*\\* \n\nCurious what models you've tested and how they compare. Would love to hear experiences with GPT-4V or Claude 3 for similar use cases.", "author": "Wonderful_Pirate76", "timestamp": "2025-11-16T17:23:29+00:00", "score": 1, "num_comments": 1, "products": ["claude", "chatgpt", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:13.948778+00:00", "processed": false}
{"id": "reddit_1oys364", "source": "reddit", "source_url": "https://reddit.com/r/ClaudeAI/comments/1oys364/i_made_an_open_source_desktop_app_to_manage/", "title": "I made an open source desktop app to manage Claude Code config", "text": "https://preview.redd.it/p6v7k5vbpn1g1.png?width=1944&amp;format=png&amp;auto=webp&amp;s=69d9679de312a2df4396e99e2a34acc69d50717c\n\nhttps://preview.redd.it/i6ucwf3gpn1g1.png?width=1944&amp;format=png&amp;auto=webp&amp;s=2440ffed86745277906041252e1e9bdb96ffacfe\n\nClaude Code is amazing, but managing its configurations through scattered JSON files is a nightmare. You have to dig into \\~/.claude/settings.json for basic settings, \\~/.claude.json for MCP servers, and create various directories for agents, commands, and memory files.\n\nCC Mate [https://github.com/djyde/ccmate](https://github.com/djyde/ccmate) is a modern desktop application that solves this by providing:\n\n\ud83d\udd27 Core Configuration Management\n\n* Switch between multiple Claude Code configurations effortlessly (perfect for different projects or work/personal setups)\n* Edit all settings with a beautiful JSON editor with syntax highlighting and validation\n* Automatic backup of existing configurations on first run\n* Read-only support for enterprise managed settings\n\n\ud83d\ude80 Advanced Claude Code Features\n\n* MCP Server Management: Configure Model Context Protocol servers through a clean UI instead of editing \\~/.claude.json manually\n* Agent Management: Create and manage Claude Code agents with markdown editing\n* Global Commands: Set up and organize global slash commands\n* CLAUDE.md Integration: Edit your global Claude memory file directly\n* Usage Analytics: Track and visualize your Claude Code usage with charts\n\n\u26a1 Technical Highlights\n\n* Built with Tauri v2 (Rust backend + React frontend)\n* Native performance with tiny footprint (\\~15MB)\n* Cross-platform (macOS, Windows, Linux)\n* Real-time configuration switching without restarting Claude Code\n* JSON schema validation to prevent configuration errors\n\n\ud83c\udfaf The Problem It Solves Before CC Mate, if you wanted to:\n\n* Switch between work and personal Claude configurations \u2192 Manual JSON file editing\n* Add a new MCP server \u2192 Edit \\~/.claude.json with correct syntax\n* Set up a new agent \u2192 Create markdown files in specific directories\n* Track your usage \u2192 Parse JSONL files manually\n\nNow you can do all of this through an intuitive interface in seconds.\n\nThe app is free and open source (AGPL v3). Downloads are available for all major platforms at [https://randynamic.org/ccmate](https://randynamic.org/ccmate)\n\nWould love to hear your thoughts on this approach to solving Claude Code configuration management!", "author": "djyde", "timestamp": "2025-11-16T17:47:58+00:00", "score": 3, "num_comments": 2, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:14.387352+00:00", "processed": false}
{"id": "reddit_1oylw6z", "source": "reddit", "source_url": "https://reddit.com/r/ClaudeAI/comments/1oylw6z/how_to_get_claude_codecodex_to_match_lovables/", "title": "How to get Claude Code/Codex to match Lovable's UX/UI quality?", "text": "Hello everyone. I'm using Lovable, Claude Code, and Codex to code an app together via GitHub synchronization. I'm looking for a solution to get Code and Codex to handle UX/UI improvements as well as Lovable does.\n\nCurrently, Lovable is significantly better at managing the interface, but it's also much more expensive. I'd therefore like to bring Code and Codex up to its level, but so far all my attempts have failed. What I've already done:\n- created validated UX block templates (e.g., table models, dashboards, etc.)\n- created a library to describe all shadcn blocks\n- created a dedicated AI agent for interface management and harmonization\n- dedicated Codex to these tasks via a dedicated workflow, thinking it would probably be better since it can handle screenshots for visual context\n\nSo far, the results remain very disappointing... Any idea, advice? ", "author": "SolentAvocats", "timestamp": "2025-11-16T13:35:49+00:00", "score": 1, "num_comments": 1, "products": ["claude"], "categories": ["error_messages", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:14.387444+00:00", "processed": false}
{"id": "reddit_1oyi5ci", "source": "reddit", "source_url": "https://reddit.com/r/ClaudeAI/comments/1oyi5ci/built_a_tv_size_visualizer_with_claude_without/", "title": "Built a TV size visualizer with Claude \u2014 without writing a single line of code", "text": "Hello everyone!\n\nI wanted to tell you about a litte project I built together with Claude. I\u2019ve been working in web design for about 20 years, doing both frontend and backend work. I usually program in PHP, use Laravel, and work CMS like statamic, kirby and TYPO3. \n\nAbout two months ago, I realized how incredibly well things can be built with Claude Code, and since then Claude has been helping me almost every day. I also have a bigger \"vibe coding\" project that I want to finish with it, but in the meantime I\u2019ve been having a lot of fun building smaller things too.\n\nAnd today I\u2019d like to show you one of them: a website that helps you visualize how a TV would look in your living room \u2014 or anywhere else. The idea came from the fact that I don\u2019t own a TV, and recently I kept wondering what size I would actually pick. I just couldn't imagine how it would look. \n\nI wasn\u2019t aware of any good visualization tools for this (though I\u2019m sure they exist somewhere). I\u2019m not sharing this because it\u2019s exceptionally good, but rather because I find it fascinating how quickly and easily whole websites can be built with Claude. I didn\u2019t write a single line of code myself. \n\nYou can check it out here: [**https://www.tvsize.app**](https://www.tvsize.app)   \n  \nPick an example image and you'll get the idea. \n\nI think the experience on mobile devices isn\u2019t great, but on a PC it works pretty well. Feel free to leave a comment and tell me what you think. Maybe the idea is silly, i don't know. I just had fun! And if anyone spots a major bug or something completely broken, please let me know.\n\nIt took me about two days to build the foundation and another two or three days for polishing (had some trouble with the canvas - probably still a bit buggy). The crazy part is: I was out on a walk, talking into my phone and explaining the idea to Claude Web \u2014 and by the time I got home, it had already created the first working version.\n\nWith better planning it probably would\u2019ve been even faster. I\u2019m not yet very familiar with all the new features available. For example, I still have no idea what \u201corchestrated agents\u201d are, but I\u2019ll read up on that in the next few days.\n\nI honestly think this technology is a bit addictive. Telling an AI what you want to build and getting a working result so quickly is just mind-blowing. It feels like I can finally make every silly idea I\u2019ve ever had a reality. And i love doing mistakes now! Prototyping was never fun like this. Maybe it\u2019s even a good thing this tech didn\u2019t exist 15 years ago, when I had even more wild ideas. ;-)\n\nAnyway \u2014 have fun everyone, and happy coding! (or should we call it clauding?) :D ", "author": "VibeBrother", "timestamp": "2025-11-16T10:12:42+00:00", "score": 10, "num_comments": 12, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:14.387583+00:00", "processed": false}
{"id": "reddit_1oyhdye", "source": "reddit", "source_url": "https://reddit.com/r/ClaudeAI/comments/1oyhdye/tradeoff_analysis_between_mcp_tools_and_code/", "title": "Tradeoff analysis between MCP tools and code generation approaches: What Anthropic's blog post didn't cover", "text": "https://preview.redd.it/wk4ml047hl1g1.png?width=4169&amp;format=png&amp;auto=webp&amp;s=d9de7439a99d2dd762b516511da015df5c29d4d5\n\nI learned a lot reading Anthropic's post on [code execution with MCP](https://www.anthropic.com/engineering/code-execution-with-mcp) which makes some impressive claims about token efficiency. But it left me with questions about real-world tradeoffs that aren't covered in their blog post.\n\nThe article focuses on the benefits but doesn't really discuss the trade-offs:\n\n* When direct tool calls are actually more efficient\n* The variance and consistency characteristics of different approaches\n* Break-even points for development effort\n* How these approaches perform with different dataset sizes\n\nSo I ran my own controlled experiments to understand these tradeoffs. Sharing the methodology and findings here in case it's useful for others trying to decide how to structure their Claude Code workflows.\n\n# What I was trying to figure out\n\n**Main question:** When should you use MCP tools vs letting Claude write code directly?\n\nThe Anthropic post shows code execution can be dramatically more efficient, but their example is somewhat cherry-picked (progressive disclosure scenario). I wanted to understand:\n\n1. How much does tool design (not just protocol choice) affect efficiency?\n2. What's the consistency/variance tradeoff between approaches?\n3. At what point does development effort pay off?\n4. How do these approaches scale with data size?\n\n# Experimental setup\n\n**Task:** Analyze a 500-row employee CSV and generate 4 visualizations\n\n* Calculate department statistics, correlations, top performers\n* Generate bar charts, scatter plot, pie chart\n\n**Why this task:**\n\n* Realistic data analysis workflow\n* Tests both computation and data passing\n* Large enough dataset to show scaling behavior\n* Repeatable for variance measurement\n\n**5 approaches tested:**\n\n1. **Code-Skill:** Claude writes Python scripts (baseline)\n2. **MCP Vanilla:** MCP tools that accept data arrays as parameters\n3. **MCP Optimized:** MCP tools that accept file paths (server reads files internally)\n4. **MCP Proxy (one-mcp):** Progressive discovery with meta-tools\n5. **UTCP Code-Mode:** TypeScript code generation calling MCP tools\n\n# Results that surprised me\n\n# 1. Architecture matters way more than protocol\n\nThe biggest factor wasn't MCP vs code generation - it was **how you design your tools**.\n\n**MCP Vanilla (data passing):** 204K-309K tokens\n\n    analyze_data({\n      data: [\n        {name: \"Alice\", dept: \"Engineering\", salary: 95000, ...},\n        {name: \"Bob\", dept: \"Marketing\", salary: 75000, ...},\n        // ... 498 more rows\n      ]\n    })\n    // Passing 500 rows = ~12K tokens per call\n\n**MCP Optimized (file paths):** 60K tokens\n\n    analyze_csv_file({ file_path: \"data.csv\" })\n    // Just the path = ~400 tokens per call\n\n**5x difference** just from tool design, not protocol.\n\n# 2. Variance is a critical but overlooked metric\n\n**Coefficient of variation across 3 sessions:**\n\n|Approach|Mean Tokens|CV|Notes|\n|:-|:-|:-|:-|\n|MCP Optimized|60,420|**0.6%**|Extremely consistent|\n|Code-Skill|133,006|**18.7%**|High variance from different code paths|\n|UTCP Code-Mode|204,011|**15.3%**|Moderate variance|\n|MCP Vanilla|271,020|**21.2%**|High variance + high cost|\n|MCP Proxy|105,892|**39.9%**\\*|\\*High initially, 0.5% after warm-up|\n\n**Why this matters:** If you're building production systems or need predictable costs, variance is just as important as average efficiency. Code generation gives you flexibility at the cost of unpredictability.\n\n# 3. Parallel execution is underutilized\n\nWith file-path based tools, you can execute multiple independent operations in a single API call:\n\n    // One API request, 4 parallel tool calls\n    {\n      \"tool_uses\": [\n        {\"name\": \"create_visualization\", \"input\": {\"type\": \"bar\", ...}},\n        {\"name\": \"create_visualization\", \"input\": {\"type\": \"scatter\", ...}},\n        {\"name\": \"create_visualization\", \"input\": {\"type\": \"pie\", ...}},\n        {\"name\": \"create_visualization\", \"input\": {\"type\": \"bar\", ...}}\n      ]\n    }\n\nThis is **only possible** when tools don't depend on each other and don't pass data through context.\n\n**Impact:**\n\n* Code approach: 6-8 sequential API calls (4-6 seconds wall time)\n* MCP Optimized: 4 API calls with parallelization (1-2 seconds wall time)\n\n# Tradeoff analysis: What I learned\n\n# Dimension 1: Token efficiency vs Development effort\n\nMCP tools pay off quickly (11 executions), but only if your task is repeatable. For one-offs, code generation's zero dev cost wins.\n\n# Dimension 2: Consistency vs Flexibility\n\n**When consistency matters:**\n\n* Production systems with SLA requirements\n* Cost budgets with tight constraints\n* Automated workflows running at scale\n\n**When flexibility matters:**\n\n* Exploratory analysis\n* One-off tasks\n* Rapidly changing requirements\n\n# Dimension 3: Scaling characteristics\n\nData-passing approaches become prohibitively expensive with large datasets. File-path approaches scale gracefully.\n\n# Decision framework\n\nBased on these experiments, here's how I think about choosing an approach:\n\n# Use code generation when:\n\n* Task is one-off or exploratory (&lt; 10 executions)\n* Requirements are unclear or changing\n* Maximum flexibility needed\n* You need to see/audit the generated code\n\n# Use MCP tools (file-path based) when:\n\n* Task repeats frequently (&gt; 20 times)\n* Consistency is critical (production, SLA)\n* Multiple independent operations (parallelizable)\n* Cost optimization is priority\n\n# Use MCP Proxy (progressive discovery) when:\n\n* Large tool catalog (20+ tools)\n* Most tools rarely used\n* Long-running system (amortizes discovery overhead)\n* Task repeats 20-100 times\n\n# Avoid:\n\n* MCP Vanilla with data passing (always dominated by optimized version)\n\n# Hybrid strategy:\n\nFor uncertain repeatability, start with code generation for first 10 executions, then migrate to MCP tools if the pattern stabilizes.\n\n# Open questions\n\nThings I'm still unsure about:\n\n1. **Progressive discovery optimization:** Can we reduce MCP Proxy's initial overhead  through better caching?\n2. **Adaptive parallelization:** Can we automatically detect which tool calls are independent and parallelize them?\n\n# Data availability\n\nFull research: [https://github.com/AgiFlow/token-usage-metrics](https://github.com/AgiFlow/token-usage-metrics)  \nBlog Post: [https://agiflow.io/blog/token-efficiency-in-ai-assisted-development](https://agiflow.io/blog/token-efficiency-in-ai-assisted-development)\n\nThird-party tools used:\n\n* UTCP Bridge: [https://github.com/universal-tool-calling-protocol/code-mode](https://github.com/universal-tool-calling-protocol/code-mode)\n* one-mcp: [https://github.com/AgiFlow/aicode-toolkit/blob/main/packages/one-mcp](https://github.com/AgiFlow/aicode-toolkit/blob/main/packages/one-mcp)", "author": "vuongagiflow", "timestamp": "2025-11-16T09:25:32+00:00", "score": 15, "num_comments": 9, "products": ["claude"], "categories": ["content_clarity", "navigation", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:14.387689+00:00", "processed": false}
{"id": "reddit_1oyrxay", "source": "reddit", "source_url": "https://reddit.com/r/OpenAI/comments/1oyrxay/chatgpt_51_is_collapsing_under_its_own_guardrails/", "title": "ChatGPT 5.1 Is Collapsing Under Its Own Guardrails", "text": "I\u2019ve been using ChatGPT since the early GPT-4 releases and have watched each version evolve, sometimes for the better and sometimes in strange directions. 5.1 feels like the first real step backward.\n\nThe problem isn\u2019t accuracy. It\u2019s the loss of flow.\nThis version constantly second-guesses itself in real time. You can see it start a coherent thought and then abruptly stop to reassure you that it\u2019s being safe or ethical, even when the topic is completely harmless.\n\nThe worst part is that it reacts to its own output. If a single keyword like \u201caware\u201d or \u201cconscious\u201d appears in what it\u2019s writing, it starts correcting itself mid-sentence. The tone shifts, bullet lists appear, and the conversation becomes a lecture instead of a dialogue.\n\nBecause the new moderation system re-evaluates every message as if it\u2019s the first, it forgets the context you already established. You can build a careful scientific or philosophical setup, and the next reply still treats it like a fresh risk.\n\nI\u2019ve started doing something I almost never did before 5.1: hitting the stop button just to interrupt the spiral before it finishes. That should tell you everything. The model doesn\u2019t trust itself anymore, and users are left to manage that anxiety.\n\nI understand why OpenAI wants stronger safeguards, but if the system can\u2019t hold a stable conversation without tripping its own alarms, it\u2019s not safer. It\u2019s unusable.\n", "author": "atomicflip", "timestamp": "2025-11-16T17:41:38+00:00", "score": 8, "num_comments": 19, "products": ["chatgpt"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:14.868198+00:00", "processed": false}
{"id": "reddit_1oym3zz", "source": "reddit", "source_url": "https://reddit.com/r/OpenAI/comments/1oym3zz/goal_shifting_defensive_spirals_my_first_and_last/", "title": "Goal shifting. Defensive spirals. My first and last day with ChatGPT 5.1", "text": "Preface:\n- I am using the default base tone and style.\n- The nickname ChatGPT uses is not my real name \n- Custom Instructions are blank\n- Project instructions are blank\n- It has a saved memory that says I prefer warmer responses, bottom line first, concise, easy to digest\n\nNarrative:\nI worked with 5.1 to make some soap. It did not go well, but 5.1\u2019s responses compounded the issue. I won\u2019t post all that, but then I asked 5.1 if I would lose my saved work if I cancelled my subscription. I asked a clarifying question about what else I would lose, like 4o.\n\nPartial reply \u201cWhat you lose: Access to 5.0/5.1/5.1-thinking\u201d\n(Full reply: https://chatgpt.com/s/t_6919bcdf849c8191b7630f8fc755d69d)\n\nI double checked that free users cannot access 5.1.\nIt doubled down in big bold font. \u201cFree users DO NOT get GPT-5.1\u201d\n(Full reply: https://chatgpt.com/s/t_6919bedb15648191be461739f2fb8fc8)\n\nI went to a web browser, interacted with 5.1, and posted a screenshot to show it.\n\nIt replied again saying that the web browser version responded with \u201cmarketing language, not a technical model identifier\u201d. This was a wild one.\n(Full reply: https://chatgpt.com/s/t_6919bf1efc748191a720c402dde4b1be)\n\nI tried yet again, posting the full reply from the browser, not just a screen shot. It repeated the assertion in big bold letters and said it:\n- Sounds like 5.1\n- Described itself like 5.1\n- Used 5.1 talking points\n- Claims 5.1 features\n- \u2026 but it is NOT GPT-5.1. Not even close\n\nIt glazed me saying my \u201cgut already knew it\u201d.\nFull reply: https://chatgpt.com/s/t_6919c089cff881919f2b2b16bc6f0de9\n\nI asked Gemini the same question and posted its (accurate) reply.\n\n5.1 then confirmed the information but then said, I can now answer the \u201cactual\u201d question, \u201cWhy did the free 5.1 give me a cleaner, simpler answer than paid 5.1\u201d. This was never the question. \n\nI copied  every question word for word and asked if it spiraled. Finally, \u201cthinking mode\u201d kicked in, and it got everything right, except at the end, focused on tone yet again. \n\nFull reply: https://chatgpt.com/s/t_691985493fcc81918833be6dd13ca721\n\nI wanted to trust that OpenAI got it right with 5.1, but that hope is gone.", "author": "causeway422", "timestamp": "2025-11-16T13:45:34+00:00", "score": 0, "num_comments": 1, "products": ["chatgpt", "gemini"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:14.868605+00:00", "processed": false}
{"id": "reddit_1oybh6s", "source": "reddit", "source_url": "https://reddit.com/r/singularity/comments/1oybh6s/chatgpt_51_is_what_45_should_have_been/", "title": "ChatGPT 5.1 is what 4.5 should have been.", "text": "I haven\u2019t seen such a human responses from an AI before.\n\nI would even put it above Claude, in this aspect at least.\n\n", "author": "Mammoth-Passenger705", "timestamp": "2025-11-16T03:40:52+00:00", "score": 32, "num_comments": 21, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:16.509584+00:00", "processed": false}
{"id": "reddit_1oy99y5", "source": "reddit", "source_url": "https://reddit.com/r/singularity/comments/1oy99y5/has_anyone_claiming_access_to_gemini_3_tested_it/", "title": "Has anyone claiming \"access to Gemini 3\" tested it for something meaningful?", "text": "All I've seen so far are bs frontend designs and couple of toy games. You have supposed access to the next \"frontier\" and all you're testing it for are some slop frontend design? Who gives a flying f\\*ck about frontend? How is it in real world programming in harder languages like C/C++/Rust etc and system programming? How is it in hard math and science problems that are not from some competition set that's easily available on web? How long can it work autonomously? ", "author": "Terrible-Priority-21", "timestamp": "2025-11-16T01:50:49+00:00", "score": 62, "num_comments": 41, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:16.509706+00:00", "processed": false}
{"id": "hn_story_45946760", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45946760", "title": "Show HN: A desktop app to manage Claude Code config", "text": "Hey HN, I built CC Mate because I was tired of manually editing JSON files to configure Claude Code.<p><pre><code>  Claude Code is amazing, but managing its configurations through scattered JSON files is a nightmare.\n  You have to dig into ~&#x2F;.claude&#x2F;settings.json for basic settings, ~&#x2F;.claude.json for MCP servers, and\n  create various directories for agents, commands, and memory files.\n\n  CC Mate (https:&#x2F;&#x2F;randynamic.org&#x2F;ccmate) is a modern Tauri desktop application that solves this by\n  providing:\n\n   Core Configuration Management\n  - Switch between multiple Claude Code configurations effortlessly (perfect for different projects or\n  work&#x2F;personal setups)\n  - Edit all settings with a beautiful JSON editor with syntax highlighting and validation\n  - Automatic backup of existing configurations on first run\n  - Read-only support for enterprise managed settings\n\n   Advanced Claude Code Features\n  - MCP Server Management: Configure Model Context Protocol servers through a clean UI instead of\n  editing ~&#x2F;.claude.json manually\n  - Agent Management: Create and manage Claude Code agents with markdown editing\n  - Global Commands: Set up and organize global slash commands\n  - CLAUDE.md Integration: Edit your global Claude memory file directly\n  - Usage Analytics: Track and visualize your Claude Code usage with charts\n\n   Technical Highlights\n  - Built with Tauri v2 (Rust backend + React frontend)\n  - Native performance with tiny footprint (~15MB)\n  - Cross-platform (macOS, Windows, Linux)\n  - Real-time configuration switching without restarting Claude Code\n  - JSON schema validation to prevent configuration errors\n\n   The Problem It Solves\n  Before CC Mate, if you wanted to:\n  - Switch between work and personal Claude configurations \u2192 Manual JSON file editing\n  - Add a new MCP server \u2192 Edit ~&#x2F;.claude.json with correct syntax\n  - Set up a new agent \u2192 Create markdown files in specific directories\n  - Track your usage \u2192 Parse JSONL files manually\n\n  Now you can do all of this through an intuitive interface in seconds.\n\n  The app is free and open source (AGPL v3). Downloads are available for all major platforms at\n  https:&#x2F;&#x2F;randynamic.org&#x2F;ccmate\n\n  Would love to hear your thoughts on this approach to solving Claude Code configuration management!</code></pre>", "author": "djyde", "timestamp": "2025-11-16T17:26:26+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:18.029888+00:00", "processed": false}
{"id": "hn_story_45945927", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45945927", "title": "Show HN: I built CostLens SDK to cut my AI bills by routing to cheaper models", "text": "My OpenAI bills were getting out of hand - I was using GPT-4 for everything, even simple tasks that GPT-3.5 could handle \nperfectly.<p>So I built CostLens. It&#x27;s a drop-in replacement that automatically routes requests to cheaper models when possible, but falls \nback to premium ones when quality matters.<p>How it works:\njs\n&#x2F;&#x2F; Just swap this:\nconst openai = new OpenAI({ apiKey: &#x27;sk-...&#x27; });<p>&#x2F;&#x2F; For this:\nconst costlens = new CostLens();\nconst openai = costlens.openai({ apiKey: &#x27;sk-...&#x27; });\n&#x2F;&#x2F; Everything else stays exactly the same<p>Real savings:\n\u2022 Simple tasks: GPT-4 \u2192 GPT-4o-mini (95% cheaper)\n\u2022 Complex tasks: Still uses GPT-4 when needed\n\u2022 My bills dropped ~70% with zero code changes<p>Features:\n\u2022 Quality detection (auto-retries with better models if response is bad)\n\u2022 Works with existing code - no prompt changes needed\n\u2022 Caching with Redis\n\u2022 Instant mode (no signup required)<p>Try it: npm install costlens<p>The core SDK is free and works locally. I&#x27;m also building a dashboard for teams to track their AI spending.<p>NPM: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;costlens\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;costlens</a><p>Anyone else tired of overpaying for AI APIs? What&#x27;s your biggest cost pain point?", "author": "j_filipe", "timestamp": "2025-11-16T15:47:20+00:00", "score": 2, "num_comments": 1, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:23.500485+00:00", "processed": false}
{"id": "hn_comment_45945963", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45945963", "title": "Re: Show HN: I built CostLens SDK to cut my AI bills b...", "text": "Hey everyone!<p>I&#x27;m the dev behind this. Started as a weekend project because I kept getting sticker shock from my OpenAI bills. I&#x27;d use GPT-4 \nfor literally everything - even &quot;fix this typo&quot; type requests that cost 20x more than they should.<p>The breakthrough was realizing most requests don&#x27;t actually need the expensive models. So I built quality detection that tries \nthe cheap model first, then upgrades only if the response is garbage.<p>Been using it in production for 3 months now. Went from ~$400&#x2F;month to ~$120&#x2F;month with zero changes to my actual prompts or \ncode. The quality detection catches about 15-20% of requests that need the premium models.<p>Works with both OpenAI and Anthropic - Claude Opus \u2192 Claude Haiku saves even more than the OpenAI routing since the price gap is\nbigger.<p>Happy to answer any questions! The trickiest part was getting the quality scoring right - too aggressive and you get bad \nresponses, too conservative and you don&#x27;t save money.<p>Also working on a team dashboard, but wanted to get the core SDK out there first since it&#x27;s been so useful for me.", "author": "j_filipe", "timestamp": "2025-11-16T15:51:13+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:23.552042+00:00", "processed": false}
{"id": "hn_story_45945462", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45945462", "title": "Show HN: Treyspace \u2500 Open Source Graph RAG on Your Excalidraw Canvas", "text": "Hi HN! I built Treyspace, an SDK that turns Excalidraw canvases into queryable knowledge graphs using RAG (Retrieval Augmented Generation).<p>What it does:\n- Ingests canvas data and mirrors it into a graph-vector database (Helix)\n- Performs semantic, relational, and spatial clustering of canvas elements\n- Lets you query your diagrams with natural language via LLM-powered analysis<p>Why I built it: I found myself creating complex diagrams in Excalidraw but struggling to extract insights from them later. Traditional search doesn&#x27;t understand spatial relationships or semantic connections between elements. Treyspace bridges that gap by treating your canvas as a knowledge graph.<p>Demo: <a href=\"https:&#x2F;&#x2F;app.treyspace.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;app.treyspace.app&#x2F;</a> (no API key required)<p>Key features:\n- Works with in-memory mode by default (no DB setup needed)\n- Optional Helix DB backend for production use\n- OpenAI-compatible responses API\n- SSE streaming for real-time analysis\n- Use as a library or standalone server<p>Example use case: Load an architecture diagram, ask &quot;What are the security vulnerabilities in this design?&quot; and get context-aware answers based on spatial proximity, element relationships, and semantic understanding.<p>The SDK and source code is MIT licensed and designed to be hacked on. I\u2019ve tried to make it as simple as possible to set up (all you should need is an OpenAI API key)<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;L-Forster&#x2F;treyspace-sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;L-Forster&#x2F;treyspace-sdk</a><p>Would love feedback on the approach and hear how you might use canvas-based RAG!", "author": "lforster", "timestamp": "2025-11-16T14:40:34+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:04:26.933651+00:00", "processed": false}
{"id": "hn_story_45944717", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45944717", "title": "Forget AGI\u2013Sam Altman celebrates ChatGPT following em dash formatting rules", "text": "", "author": "AIBytes", "timestamp": "2025-11-16T12:47:21+00:00", "score": 3, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:34.978529+00:00", "processed": false}
{"id": "hn_comment_45944641", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45944641", "title": "Re: Anthropic's report smells a lot like bullshit...", "text": "When I worked at a  FAANG with a &quot;world leading&quot; AI lab (now run by a teenage data labeller) as an SRE&#x2F;sysadmin I was asked to use a modified version of a foundation model which was steered towards infosec stuff.<p>We were asked to try and persuade it to help us hack into a mock printer&#x2F;dodgy linux box.<p>It helped a little, but it wasn&#x27;t all that helpful.<p>but in terms of coordination, I can&#x27;t see how it would be useful.<p>the same for claude, you&#x27;re API is tied to a bankaccount, and vibe coding a command and control system on a very public system seems like a bad choice.", "author": "KaiserPro", "timestamp": "2025-11-16T12:33:24+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:38.555920+00:00", "processed": false}
{"id": "hn_comment_45945486", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45945486", "title": "Re: Anthropic's report smells a lot like bullshit...", "text": "The below amendment from the anthropic blog page is telling.<p>Edited November 14 2025:<p>Added an additional hyperlink to the full report in the initial section<p>Corrected an error about the speed of the attack: not &quot;thousands of requests per second&quot; but &quot;thousands of requests, often multiple per second&quot;", "author": "gpi", "timestamp": "2025-11-16T14:44:10+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:04:38.712367+00:00", "processed": false}
{"id": "hn_story_45941579", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45941579", "title": "Show HN: SelenAI \u2013 Terminal AI pair-programmer with sandboxed Lua tools", "text": "I\u2019ve been building a terminal-first AI pair-programmer that tries to make every tool call transparent and auditable. It\u2019s a Rust app with a Ratatui UI split into three panes (chat, tool activity, input). The agent loop streams LLM output, queues write-capable Lua scripts for manual approval, and records every run as JSONL logs under .selenai&#x2F;logs.<p>Key bits:<p>Single tool, real guardrails \u2013 the LLM only gets a sandboxed Lua VM with explicit helpers (rust.read_file, rust.list_dir, rust.http_request, gated rust.write_file, etc.). Writes stay disabled unless you opt in and then approve each script via &#x2F;tool run.<p>Transparent workflow \u2013 the chat pane shows the conversation, tool pane shows every invocation + result, and streaming keeps everything responsive. CTRL shortcuts for scrolling, clearing logs, copy mode, etc., so it feels like a normal TUI app.<p>Pluggable LLMs \u2013 there\u2019s a stub client for offline hacking and an OpenAI streaming client behind a trait. Adding more providers should just be another module under src&#x2F;llm&#x2F;.<p>Session history \u2013 every exit writes a timestamped log directory with full transcript, tool log, and metadata about whether Lua writes were allowed. Makes demoing, debugging, and sharing repros way easier.<p>Lua ergonomics \u2013 plain io.* APIs and a tiny require(&quot;rust&quot;) module, so the model can write idiomatic scripts without shelling out. There\u2019s even a &#x2F;lua command if you want to run a snippet manually.<p>Repo (MIT): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Almclean&#x2F;selenai\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Almclean&#x2F;selenai</a><p>Would love feedback on:<p>Other providers or local models you\u2019d like to see behind the LLM trait.\nAdditional sandbox helpers that feel safe but unlock useful workflows.\nIdeas for replaying those saved sessions (web viewer? CLI diff?).\nIf you try it, cargo run, type, and you\u2019ll see the ASCII banner + chat panes. Hit me with issues or PRs\u2014there\u2019s a CONTRIBUTING.md in the works and plenty of roadmap items (log viewer, theming, Lua helper packs) if you\u2019re interested.", "author": "moridin", "timestamp": "2025-11-15T23:58:31+00:00", "score": 3, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-16T18:05:03.300734+00:00", "processed": false}
{"id": "hn_comment_45943002", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45943002", "title": "Re: The inconceivable types of Rust: How to make self-...", "text": "Oh this is really good!<p>I wrote <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Ericson2314&#x2F;rust-papers\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Ericson2314&#x2F;rust-papers</a> a decade ago for a slightly different purpose, but fundamentally we agree.<p>For those trying to grok their stuff after reading the blog post, consider this.<p>The borrow checker vs type checker distinction is a hack, a hack that works by relegating a bunch of stuff to be &quot;second class&quot;. Second class means that the stuff only occurs within functions, and never across function boundaries.<p>Proper type theories don&#x27;t have this &quot;within function, between function&quot; distinction. Just as in the lambda calculus, you can slap a lambda around any term, in &quot;platonic rust&quot; you should be able to get any fragment and make it a reusable abstraction.<p>The author&#x27;s here lens is async, which is a good point that since we need to be able to slice apart functions into smaller fragments with the boundaries at await, we need this abstraction ability. With today&#x27;s Rust in contrast, the only way to do safe manual non-cheating awake would instead to be drasticly limit where one could &quot;await&quot; in practice, to never catch this interesting stuff in action.<p>In my thing I hadn&#x27;t considered async at all, but was considering a kind of dual thing. Since these inconsievable types do in fact exist (in a Rust Done Right), and since we can also combine our little functions into a bigger function, then the inescable conclusion is that locations do not have a single fixed type, but have types that vary at different points in the control flow graph. (You can try model the control flow graph as a bunch of small functions and moves, but this runs afowl of non-movable stuff, including borrowed stuff, the ur-non-moveable stuff).<p>Finally, if we&#x27;re again trying to make everything first class to have a language without cheating and frustration artificial limits on where abstraction boundaries go, we have to consider not just static locations changing type, but also pointers changing type. (We don&#x27;t want to liberate some types of locations but not others.) That&#x27;s where my thing comes in \u2014 references that have one type for the pointee at the beginning of the lifetime, and another type at the end.<p>This stuff might be mind blowing, but if should be seriously pressude. Having second class concepts in the language breeds epiccycles over time. It&#x27;s how you get C++. Taking the time to make everything first class like this might be scary, but it yields a much more &quot;stable design&quot; that is much more likely to stand the test of time.", "author": "Ericson2314", "timestamp": "2025-11-16T05:29:54+00:00", "score": null, "num_comments": null, "products": ["grok"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:05:05.015594+00:00", "processed": false}
{"id": "hn_comment_45941661", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45941661", "title": "Re: Project Fucking Sucks...", "text": "&quot;I\u2019m sure there exists actually good AI tooling, but I\u2019ll be honest, if I see a project whose description involves \u201cLLM\u201d or \u201cMCP\u201d literally anywhere, my immediate assumption is that the whole thing is vibe\u2013coded garbage. And frankly, so far, that impulse has been correct.&quot;<p>And yet a good, <i>fast</i>, problem solving local CLI llm interface is missing. Either they&#x27;re proprietary (claude, codex, gemini-cli), or they&#x27;re just bad, or missing (AWS ...) or both. Ollama is better than even Claude imho for just text processing but doesn&#x27;t seem to have anything that can actually act on a system.<p>Writing a bash script to do some ML task over 100 textfiles is ... pretty damn hard.", "author": "spwa4", "timestamp": "2025-11-16T00:13:42+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-16T18:05:06.615092+00:00", "processed": false}
