{"id": "hn_story_46058327", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46058327", "title": "Show HN: Generate documentation sites from Git repositories", "text": "I\u2019m sharing an MVP of a tool for building documentation sites directly from Git repositories: <a href=\"https:&#x2F;&#x2F;brodocs.io\" rel=\"nofollow\">https:&#x2F;&#x2F;brodocs.io</a> with auto conversion of PlantUML and draw.io diagrams.<p>All repos appear on left tree menu, but you can also create sites with top menu structure where each menu item directs to subsite with own left menu structure. Examples: <a href=\"https:&#x2F;&#x2F;brodocs.io&#x2F;94c8be738065bd0c559&#x2F;Backlog&#x2F;Intakebacklog&#x2F;README.html\" rel=\"nofollow\">https:&#x2F;&#x2F;brodocs.io&#x2F;94c8be738065bd0c559&#x2F;Backlog&#x2F;Intakebacklog...</a>, <a href=\"https:&#x2F;&#x2F;brodocs.io&#x2F;21a3986b137fb8f4ff8&#x2F;Backlog&#x2F;README.html\" rel=\"nofollow\">https:&#x2F;&#x2F;brodocs.io&#x2F;21a3986b137fb8f4ff8&#x2F;Backlog&#x2F;README.html</a>,<p>Who may like it:<p>Large organizations to build central and per team documentation sites from micro (and nano) services docs, Terraform&#x2F;Ansible modules, solution designs, architecture decision records. Keeping docs as markdown in git allows collaboration through standard PR workflows. Could be an input to construct agents.md or copilot-instructions.md in given area, describing architecture at high level, to get better vibe codes. I see quite often that teams build own sites using some static site generator and CI&#x2F;CD pipelines, moving away from wiki like Confluence, but it costs some effort to build&#x2F;maintain and security is missing.<p>Small distributed teams working on startups to have common docs space built from markdown files stored close to source code. When hiring starts, new people need to be on boarded quickly.<p>Individuals who use PKM (Personal Knowledge Management) tools based on markdown, such as VS Code, NeoVim, Obsidian, or Logseq. If you have spent some time to build PKM, you might be using it in read-only mode for some stable parts, so e.g. in restrictive corporate environments where your favorite PKM tool might not be allowed, or don&#x27;t want to have too many VSC windows open, quick access from a browser could be helpful.<p>The MVP does not require signing up. Login and management app will come next.<p>Happy to hear observations, criticism, and suggestions. How do you prefer to write tech docs at your work, wiki or markdowns in git repo? Do you publish them using some static site generators?", "author": "BroTechLead", "timestamp": "2025-11-26T15:29:01+00:00", "score": 1, "num_comments": 0, "products": ["copilot"], "categories": ["navigation"], "sentiment": null, "collected_at": "2025-11-26T17:10:38.570597+00:00", "processed": false}
{"id": "hn_story_46058210", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46058210", "title": "GPT-5.2-codex-rewardmaxx-ultra-think and products from AI labs", "text": "Model naming has seemingly been an issue recently, especially with OpenAi, and so I wanted to take a moment to discuss this.<p>Researchers consistently are, well, researchers. Their goal is to do research, not to name your model correctly. The product team on the other hand does have the job of naming models correctly. One of the biggest issues right now it seems is that the product team, engineering team, and research teams at most of these companies are separated.<p>Take a look at claude code for example. They hired a bunch of devs, despite the claims of &quot;claude code building itself&quot;. They have 2 product people that bounced around between companies, and the product is becoming so insanely bloated that I&#x27;m not sure what theyre focused on.<p>OpenAi is in a similar boat. The generality of the tools they are shipping, and the understanding that they should ship a general coding model on top of the rest of the &quot;gpt&quot; models is crazy from a consumer perspective. They have such general tech AND have a profit incentive to monopolize the stack. This led to the responses api which is significantly more stateful and painful to use as an end user. It really only serves to provide openai with more lock in.<p>As these features get baked into the apis (including things like caching reasoning blocks etc.) we are going to see more and more product scope increase and more and more confusing products as they try to bake more unrelated features into single products.", "author": "akira_067", "timestamp": "2025-11-26T15:19:21+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["content_clarity", "response_quality"], "sentiment": null, "collected_at": "2025-11-26T17:10:39.175906+00:00", "processed": false}
{"id": "hn_comment_46058180", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46058180", "title": "Re: Bloomberg-inspired market sentiment tracker built ...", "text": "I had been wanting to play around in a project with dense user interfaces and Bloomberg terminal aesthetics and, so an investment dashboard seemed like a good fit.<p>It aggregates market indicators that have been known to generate contrarian buy&#x2F;sell signals. CNN Fear &amp; Greed, Bank of America SSI, AAII Investor Sentiment Survey among others.<p>A few technical details:<p>* vibe coded ~70% of it \u2014 the parts that not were either UI polish that was faster to do directly or points of the data scraping pipeline where Claude got stuck<p>* architecture: vanilla JS frontend + Python&#x2F;Flask backend + background jobs that run an AI data extraction pipeline (Perplexity Sonar &#x2F; Exa for search + GPT-5)<p>* runs on render.com as web service + two cron jobs that run the data update process and send daily email notifications<p>At some point I did try Codex for a few PRs that were never merged and I re-did instead with Claude.", "author": "victordg", "timestamp": "2025-11-26T15:17:06+00:00", "score": null, "num_comments": null, "products": ["claude", "perplexity"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-26T17:10:39.414395+00:00", "processed": false}
{"id": "hn_comment_46058349", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46058349", "title": "Re: OpenAI needs to raise at least $207B by 2030 so it...", "text": "It&#x27;s sort of hard to judge this.<p>The article mostly focuses on ChatGPT uses, but hard to say if ChatGPT is going to be the main revenue driver. It could be! Also unclear if the underlying report is underconsidering the other products.<p>It also estimates that LLM companies will capture 2% of the digital advertising market, which seems kind of low to me. There will be challenges in capturing it and challenges with user trust, but it seems super promising because it will likely be harder to block and has a lot of intent context that should make it like search advertising++. And for context, search advertising is 40% of digital ad revenue.<p>Seems like the error bars have to be pretty big on these estimates.", "author": "matthewowen", "timestamp": "2025-11-26T15:30:22+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["content_clarity"], "sentiment": null, "collected_at": "2025-11-26T17:10:39.961411+00:00", "processed": false}
{"id": "hn_story_46057415", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46057415", "title": "Show HN: LLM-models \u2013 a CLI tool to list available LLM models across providers", "text": "I built a simple CLI tool to solve a problem I kept running into: which exact model names are actually available through OpenAI, Anthropic, Google, and xAI APIs at any given time?<p>The APIs themselves provide this info, but I got tired of checking docs or writing one-off scripts. Now I can just run:<p>$ llm-models -p Anthropic<p>and get the current list with human-readable names.<p>Installation:<p><pre><code>  macOS: brew tap ljbuturovic&#x2F;tap &amp;&amp; brew install llm-models\n  Linux: pipx install llm-models\n  Windows: pip install llm-models\n</code></pre>\nBuilt with help from Claude Code. The tool queries each provider&#x27;s API directly, so you get real-time availability rather than stale\ndocumentation.<p>Open to feedback and happy to add more providers if there&#x27;s interest!", "author": "ljubomir", "timestamp": "2025-11-26T13:54:48+00:00", "score": 3, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-26T17:10:44.365693+00:00", "processed": false}
{"id": "hn_story_46057283", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46057283", "title": "I built an open-weights memory system that reaches 80.1% on the LoCoMo benchmark", "text": "I\u2019ve been experimenting with long-term memory architectures for agent systems and wanted to share some technical results that might be useful to others working on retrieval pipelines.<p>Benchmark: LoCoMo (10 runs \u00d7 10 conversation sets)\nAverage accuracy: 80.1%\nSetup: full isolation across all 10 conv groups (no cross-contamination, no shared memory between runs)<p>Architecture (all open weights except answer generation)<p>1. Dense retrieval<p>BGE-large-en-v1.5 (1024d)<p>FAISS IndexFlatIP<p>Standard BGE instruction prompt:\n\u201cRepresent this sentence for searching relevant passages.\u201d<p>2. Sparse retrieval<p>BM25 via classic inverted index<p>Helps with low-embedding-recall queries and keyword-heavy prompts<p>3. MCA (Multi-Component Aggregation) ranking\nA simple gravitational-style score combining:<p>keyword coverage<p>token importance<p>local frequency signal\nMCA acts as a first-pass filter to catch exact-match questions.\nThreshold: coverage \u2265 0.1 \u2192 keep top-30<p>4. Union strategy\nInstead of aggressively reducing the union, the system feeds 112\u2013135 documents directly to a re-ranker.\nIn practice this improved stability and prevented loss of rare but crucial documents.<p>5. Cross-Encoder reranking<p>bge-reranker-v2-m3<p>Processes the full union (rare for RAG pipelines, but worked best here)<p>Produces a final top-k used for answer generation<p>6. Answer generation<p>GPT-4o-mini, used only for the final synthesis step<p>No agent chain, no tool calls, no memory-dependent LLM logic<p>Performance<p>&lt;3 seconds per query on a single RTX 4090<p>Deterministic output between runs<p>Reproducible test harness (10\u00d710 protocol)<p>Why this worked<p>Three things seemed to matter most:<p>MCA-first filter to stabilize early recall<p>Not discarding the union before re-ranking<p>Proper dense embedding instruction, which massively affects BGE performance<p>Notes<p>LoCoMo remains one of the hardest public memory benchmarks:\n5,880 multi-hop, temporal, negation-rich QA pairs derived from human\u2013agent conversations.\nWould be interested to compare with others working on long-term retrieval, especially multi-stage ranking or cross-encoder heavy pipelines.<p>Github: https:&#x2F;&#x2F;github.com&#x2F;vac-architector&#x2F;VAC-Memory-System", "author": "ViktorKuz", "timestamp": "2025-11-26T13:38:09+00:00", "score": 2, "num_comments": 2, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-26T17:10:45.326672+00:00", "processed": false}
{"id": "hn_comment_46056546", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46056546", "title": "Re: Open source Firefox extension to quickly interact ...", "text": "Annoyed with paid services that charge you to bring your own API key (getvoila.ai), I partnered with Claude Code and made my own and open sourced it. Enjoy!<p>---<p>A &quot;bring your own key&quot; Firefox extension that provides quick access to LLM assistants (OpenAI, Anthropic, Google Gemini) via a keyboard shortcut, with full page context.<p>Features:<p>- Quick access: Press `Ctrl+J` to open the assistant overlay on any webpage<p>- Multiple providers: Supports OpenAI, Anthropic, and Google Gemini<p>- Page context: Use `@page` in your prompt to include the current page&#x27;s content<p>- Markdown rendering: Responses are rendered with full markdown support (code blocks, lists, etc.)<p>- Session memory: Conversations persist within a session (cleared when you close the popup)<p>- Streaming responses: See responses as they&#x27;re generated in real-time", "author": "justdep", "timestamp": "2025-11-26T11:56:59+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-26T17:10:50.636740+00:00", "processed": false}
{"id": "hn_comment_46056633", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46056633", "title": "Re: Await Is Not a Context Switch: Understanding Pytho...", "text": "My New Year\u2019s Resolution will be to give up complaining about this on hn, but for now:<p>I find ChatGPT\u2019s style and tone condescending and bland to the point of obfuscating whatever was unique, thoughtful and insightful in the original prompt.<p>Trying to reverse-engineer the \u201cNot this: That!\u201d phrasing, artificial narrative drama &amp; bizarre use of emphasis to recapture that insight and thought is not something I\u2019m at all enthusiastic to do.<p>Perhaps a middle ground: HN could support a \u201cprompt\u201d link to the actual creative seed?", "author": "twoodfin", "timestamp": "2025-11-26T12:07:58+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["tone", "navigation"], "sentiment": null, "collected_at": "2025-11-26T17:10:52.124452+00:00", "processed": false}
{"id": "hn_story_46055811", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46055811", "title": "Show HN: Preshiplist \u2013 A fast way to ship waitlist websites without headaches", "text": "Hi there!<p>I\u2019m an experienced startup product designer and marketer who recently got into building small-scale software products as a one-person team. While working on various projects, the part of creating a waitlist website every time I need to validate an idea or gather early users just takes a ton of time and thinking that could be better spent elsewhere.<p>I spent hours designing landing pages. AI-generated sites required endless prompting and fixing, and full website builders like Framer or Webflow took a lot more time than I expected. Because of that extra effort, I often lost momentum and sometimes missed the window to test concepts quickly.<p>So I built Preshiplist \u2014 a tool that generates simple, clean, mobile-optimized waitlist pages with minimal setup. No builder UI. No templates. You write your description, choose a style, and publish.<p>The platform includes:\n- Fully working forms\n- A built-in database for signups with email validation\n- AI assistance to help you refine your page copy\n- A basic email drip system so new signups receive an automatic follow-up\n- Custom domains, short links, and an Open Graph editor to make sharing easier<p>All of this works out of the box without needing integrations, prompting, or configuration.<p>A few technical notes:\n\u2013 Built mainly with Next.js and Supabase for the backend\n\u2013 Deployed on Vercel with Cloudflare in front for stability and security\n\u2013 Email functionality is handled by the always-reliable Resend\n\u2013 AI features use OpenAI models for generating text variations\n\u2013 Custom domains come with automatic SSL via Vercel<p>If you\u2019re up for it, I\u2019d appreciate feedback on:\n\u2013 Simplicity vs flexibility \u2014 is a streamlined flow enough, or would you prefer more customization power?\n\u2013 Features you think would be valuable for makers or founders using waitlist pages\n\u2013 Whether AI-assisted copy is helpful in this context or unnecessary<p>Thanks for reading!", "author": "Frederick_22xAI", "timestamp": "2025-11-26T09:42:06+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-26T17:10:54.919667+00:00", "processed": false}
{"id": "hn_comment_46057771", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46057771", "title": "Re: Image Diffusion Models Exhibit Emergent Temporal P...", "text": "This is a cool result. Deep learning image models are trained on enormous amounts of data and the information recorded in their weights continues to astonish me. Over in the Stable Diffusion space, hobbyists (as opposed to professional researchers) are continuing to find new ways to squeeze intelligence out of models that were trained in 2022 and are considerably out of date compared with the latest \u201cflow matching\u201d models like Qwen Image and Flux.<p>Makes you wonder what intelligence is lurking in a 10T parameter model like Gemini 3 that we may not discover for some years yet\u2026", "author": "ttul", "timestamp": "2025-11-26T14:38:08+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-11-26T17:10:57.704025+00:00", "processed": false}
{"id": "hn_story_46054849", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46054849", "title": "Show HN: InterviewFlowAI \u2013 AI phone and Meet interviews for fast screening", "text": "Hi HN,<p>I\u2019ve been working on InterviewFlowAI, a tool that automates the first-round hiring workflow for teams that spend too much time on initial screening. It handles resume scoring, public job links, candidate applications, and full interviews conducted over phone or Google Meet.<p>I built this after spending years interviewing candidates as a Head of Engineering and realizing that most of the bottleneck happens before an engineer ever gets involved. Too many resumes, too many unqualified applicants, and a ton of repetitive phone screens that rarely lead anywhere.<p>Here\u2019s what the system does today:\n \u2022 Generates a public job link so candidates can apply directly\n \u2022 Scores resumes based on the job description and required skills\n \u2022 Lets you accept or reject candidates instantly\n \u2022 Runs a live interview (phone or Google Meet) using an AI agent\n \u2022 Produces a structured scorecard, transcript, and recording<p>Technical details for those interested:\n \u2022 Uses OpenAI real-time API for conversation flow\n \u2022 Voice handling and telephony via Vapi\n \u2022 Speech \u2192 text \u2192 scoring pipelines using AssemblyAI and custom logic\n \u2022 Resume scoring uses embeddings + rule-based signals to reduce LLM hallucination\n \u2022 All interviews are stateless interactions stored securely for evaluation<p>Pricing is $0.50 per interview, so teams can screen at scale without high per-candidate costs.<p>I\u2019d love feedback from the HN community, especially around accuracy, bias concerns, architecture, and scaling. Happy to answer any questions about design decisions or what\u2019s still rough.", "author": "mukulmunjal", "timestamp": "2025-11-26T06:56:07+00:00", "score": 1, "num_comments": 3, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-26T17:11:00.125671+00:00", "processed": false}
{"id": "hn_story_46054337", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46054337", "title": "Show HN: RankLens \u2013 Track your brand's visibility in AI answers reliably", "text": "We built RankLens because we couldn\u2019t answer a simple question for our own clients: \u201cHow often do AI assistants actually recommend your brand vs. competitors?\u201d<p>Instead of ad-hoc \u201cSEO prompts\u201d, RankLens uses structured entity-conditioned probes. Each probe is defined by a brand&#x2F;site entity + intent, and we resample across many runs to reduce prompt noise and random LLM variance.<p>For each probe we track:\n\u2013 Explicit mention of your brand&#x2F;site (Brand Match)\n\u2013 Precision of when you\u2019re recommended as the answer (Brand Target)\n\u2013 How often competitors get recommended instead (Brand Appearance + share of voice)-  \n- Likelihood of being recommended by the AI. (Brand Discovery)\n\u2013 A prominence &#x2F; \u201cconfidence\u201d score for how strongly the LLM backs that recommendation<p>We combine these into a visibility index so agencies and brands can:\n\u2013 See AI visibility trends over time\n\u2013 Compare engines (e.g., ChatGPT-style assistants vs. others)\n\u2013 Spot when they\u2019re losing AI \u201cmindshare\u201d to specific competitors in regions&#x2F;locale<p>Method &amp; code\n\u2013 We open-sourced the entity&#x2F;probe framework as RankLens Entities (code + configs): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jim-seovendor&#x2F;entity-probe\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jim-seovendor&#x2F;entity-probe</a>\n\u2013 We also wrote an in-depth study, \u201cEntity-Conditioned Probing with Resampling: Validity and Reliability for Measuring LLM Brand&#x2F;Site Recommendations\u201d: <a href=\"https:&#x2F;&#x2F;zenodo.org&#x2F;records&#x2F;17489350\" rel=\"nofollow\">https:&#x2F;&#x2F;zenodo.org&#x2F;records&#x2F;17489350</a><p>I\u2019d love HN feedback on:\n\u2013 Weak spots &#x2F; blind spots in the entity-conditioned probing methodology\n\u2013 Better baselines or evaluation strategies you\u2019d use to test validity &amp; reliability\n\u2013 Any ways this could be gamed in practice (e.g., by changing site content or prompts) that we haven\u2019t considered<p>Happy to go into implementation details (sampling design, resampling, scoring, engine differences, etc.) in the comments.", "author": "digitalpeak", "timestamp": "2025-11-26T05:00:34+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-26T17:11:02.100808+00:00", "processed": false}
