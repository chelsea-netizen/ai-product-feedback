{"id": "hn_comment_46755553", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46755553", "title": "Re: Nano agent: a minimalistic Python library for buil...", "text": "Over the weekend, I wrote this small Python library to teach myself the core idea behind modern agentic systems. This kind of software sits at the core of Claude Code, Codex, etc. I wanted to see if I could build it from scratch, so this is mostly educational for me.<p>The result is a surprisingly simple piece of software. At its core are immutable DAGs, which keep the design simple and easy to reason about.<p>I also added a set of built-in tools that are <i>inspired</i> by Claude Code&#x27;s built-in tools.<p>A bonus point: it can also capture Claude Code auth tokens, so you can use it with your Claude Code subscription. However, there is a chance that Anthropic will ban you if they detect this, so use it at your own risk.<p>P.S.: One additional point I also want to mention is that Claude Code (SDK) is closed-source, so I cannot modify it for my use case or fix its buggy UI on my own. This is one of the factors for why I&#x27;m creating this library.", "author": "xcodevn", "timestamp": "2026-01-25T16:36:48+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:11.082887+00:00", "processed": false}
{"id": "hn_comment_46755366", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46755366", "title": "Re: A macOS app that blurs your screen when you slouch...", "text": "<i>Once launched, Posturr runs in the background and displays a brief &quot;Claude Mode Active&quot; notification.</i><p>I haven\u2019t checked the code yet, but what does the \u201cClaude Mode\u201d mean? Is it a poor naming choice? It implies that the local app is somehow connected to Claude (?)", "author": "tanelpoder", "timestamp": "2026-01-25T16:15:10+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-01-25T17:10:13.858665+00:00", "processed": false}
{"id": "hn_story_46754444", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46754444", "title": "Show HN: Ask CLI \u2013 A simple, open-source tool to get command-line help", "text": "I want to share Ask CLI, a tool I developed to get help with commands and coding directly from the terminal. It is a simple app designed to do one thing well: provide instant command assistance. This isn&#x27;t a complex coding agent like Claude Code; it is built specifically to get short, fast answers without context switching.<p>As a developer, I\u2019ve always struggled to remember every command and its specific options. Whenever I need to use tools like Docker, Git, or psql, I find myself leaving the terminal to check documentation or scrolling through verbose --help text just to recall a specific flag. I usually know what I want to do, but I forget the exact syntax. I didn&#x27;t want to waste time switching to Google or ChatGPT just to find a one-line command.<p>I developed Ask CLI to solve this. It has been a game-changer for my workflow. Now, when I forget a command, I simply ask my terminal. It gives me a fast, precise answer\u2014exactly what I need\u2014without breaking my flow.<p>It is incredibly easy to use: just select an AI model, set your API key, and start chatting naturally with your terminal.<p>Examples:<p>$ ask how to run a docker container with env variables<p>$ ask how to setup my local git account<p>You can also use the &quot;what&quot; and &quot;how&quot; aliases for a more natural feel:<p>$ what is chmod<p>$ how to print all the env variables<p>You can use Ask CLI with popular hosted models (Gemini, Claude, ChatGPT) or with local models and external providers that support OpenAI-compatible APIs (Ollama, llama.cpp, LM Studio, etc.).<p>Ask CLI is free and open-source. Check it out here:<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;david-minaya&#x2F;ask\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;david-minaya&#x2F;ask</a>", "author": "david-minaya", "timestamp": "2026-01-25T14:45:22+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["tone", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:16.400722+00:00", "processed": false}
{"id": "hn_story_46754262", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46754262", "title": "Show HN: Humynize \u2013 A tool to fix the rhythmic stiffness of AI writing", "text": "I built Humynize because I realized that even with the best prompts, AI drafts still feel mechanical. I found myself spending hours fixing the &quot;flow&quot; rather than the content.<p>I used Next.js for the frontend and OpenAI for the core processing. The logic focuses on NLP structural re-architecture\u2014essentially breaking down the robotic patterns in the sentence structure and restoring a human-like &quot;burstiness&quot; and rhythm.<p>Key Technical Focus:<p>Technical Integrity: It is designed to keep data, citations, and specific meaning 100% intact while changing the voice.<p>Structural Variety: It moves away from the predictable sentence lengths that AI detectors look for.<p>Speed: Using a clean stack for fast processing and high-precision output.<p>I am the founder, and I would love for this community to test it. I am specifically interested in feedback regarding the quality of the &quot;Human-Voice Shield&quot; and how it handles technical or academic text.<p>I am around to answer any questions about the build", "author": "dumebioruche", "timestamp": "2026-01-25T14:19:41+00:00", "score": 2, "num_comments": 1, "products": ["chatgpt"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:17.745279+00:00", "processed": false}
{"id": "hn_story_46754009", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46754009", "title": "Show HN: JsonUI \u2013 Constrain AI agents through code structure, not prompts", "text": "I built an ecosystem for AI-driven development where breaking architectural rules is structurally impossible.<p>*The problem:* AI coding assistants produce inconsistent code. Every session yields different implementations, and AI &quot;forgets&quot; rules mid-conversation. Prompt engineering helps, but quality still depends on how well you explain things each time.<p>*The insight:* Don&#x27;t ask AI to follow rules\u2014make it impossible to break them.<p>*The approach:*<p>1. *Specialized agents with strict boundaries* - Instead of one AI doing everything, split responsibilities. Layout agent creates JSON UI structure (never touches data types). Data agent defines bindings (never writes business logic). ViewModel agent implements logic (never edits JSON).<p>2. *JSON as single source of truth* - One JSON definition generates iOS native (SwiftUI&#x2F;UIKit), Android native (Compose&#x2F;XML), Web (React&#x2F;Tailwind), tests, and docs. All in sync. Always.<p>3. *Cross-platform test runner* - Same test JSON runs on XCUITest, UIAutomator, and Playwright.<p>*Result:* Spec, implementation, and docs stay in sync because they&#x27;re generated from the same source. AI agents are productive because they have clear, narrow scopes.<p>Still in development. Repos:<p>- Core: SwiftJsonUI, KotlinJsonUI, ReactJsonUI\n- Test runner: jsonui-test-runner (CLI + platform drivers)\n- Agents: JsonUI-Agents-for-claude<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Tai-Kimura\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Tai-Kimura</a><p>Would love feedback on the agent design approach.", "author": "tai-kimura", "timestamp": "2026-01-25T13:43:14+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:19.427665+00:00", "processed": false}
{"id": "hn_story_46753781", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46753781", "title": "Claude Web Is Down", "text": "Can&#x27;t connect chat interface.", "author": "zkmon", "timestamp": "2026-01-25T13:06:02+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-25T17:10:21.177185+00:00", "processed": false}
{"id": "hn_story_46753402", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46753402", "title": "Show HN: AgentHub \u2013 A unified SDK for LLM APIs with faithful validation", "text": "Hi HN,\nI built AgentHub because I was frustrated by the trade-offs required to build multi-model agents in 2026. When you try to support GPT, Claude, and Gemini 3 simultaneously, you usually hit a wall: you either write thousands of lines of boilerplate code or use a &quot;standardizing&quot; wrapper that strips away what makes each model special.\nWhile projects like Open Responses focus on creating vital standards for model transparency and evaluation, AgentHub provides a simple and light-weight interface to adopt those standards in production with zero code changes.\nAgentHub takes a different approach: We don\u2019t want to &quot;standardize&quot; the models; we want to provide an intuitive yet faithful interface that keeps you 100% consistent with official API specifications.\n- Zero-Code Switching: You can transition your entire agent infrastructure from one provider to another via a simple configuration update. No refactoring, no logic changes\u2014it\u2019s a true zero-code conversion for your codebase.\n- Faithful Validation: Unlike simple API forwarders, we perform comprehensive validation to ensure your payloads perfectly match SOTA specifications. This maintains 100% consistency with official API SDKs, eliminating the &quot;intelligence loss&quot; often caused by fragile manual schema mapping.\n- Traceable Executions: We provide lightweight yet fine-grained tracing for debugging and auditing LLM executions, enabling deep post-mortem analysis of agent behavior.\nI\u2019m curious to hear from the HN community: In your production workflows, do you prefer a &quot;Universal Standard&quot; like Open Responses, or do you value 100% official SDK consistency more when switching between frontier models?", "author": "PrismShadow", "timestamp": "2026-01-25T12:09:46+00:00", "score": 2, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:24.237141+00:00", "processed": false}
{"id": "hn_story_46752922", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46752922", "title": "Show HN: I built a tool to stop my posts from getting shadowbanned", "text": "Hey HN,<p>I\u2019m Nikhil (<a href=\"https:&#x2F;&#x2F;nikhilp.online\" rel=\"nofollow\">https:&#x2F;&#x2F;nikhilp.online</a>). I&#x27;ve been building projects for the past few years, and decided to build ShillGuard because I kept hitting a wall when trying to share them on platforms like reddit and facebook.<p>I\u2019d spend weeks building something, write a post and get it instantly removed by Reddit\u2019s AutoMod or flagged as spam in Gmail without knowing why.<p>ShillGuard is a Chrome extension that analyzes your draft text in real-time against the specific context of where you are posting&#x2F;sending.<p>How it works under the hood:<p>Instead of just checking grammar or tone, the extension injects a content script (built with Plasmo) to scrape the DOM and fetch contextual metadata before you hit submit.<p>On Reddit: When you type in a text editor, it grabs the subreddit name from the URL and fetches the specific rules.json and about.json endpoints in parallel. It also checks your current account stats (Karma&#x2F;Account Age) against the community&#x27;s typical thresholds to predict if you&#x27;ll be filtered by AutoMod. I plan to add examples of recently popularized posts as well, to provide good examples of how the content &quot;should&quot; look in an ideal world.<p>On Facebook: It scrapes group metadata and privacy settings to warn you if your post (e.g., containing external links) violates specific group norms.<p>On Gmail: It analyzes your subject line and body for spam-trigger words and checks for &quot;attachment&quot; inconsistencies (e.g., saying &quot;attached&quot; but forgetting the file). I am enjoying building this feature out the most, as there are so many ways to make it produce high quality emails! Currently, I&#x27;m integrating a blacklist and spam check using an external API to help highlight if your account is being hidden by email providers.<p>The Tech Stack:<p>Framework: Plasmo (for the browser extension runtime)<p>Frontend: React + Tailwind CSS<p>Intelligence: Google Gemini Flash (via the new Google Gen AI SDK)<p>Architecture: It\u2019s strictly Local-First &#x2F; BYOK (Bring Your Own Key).<p>I decided to go with a Bring Your Own Key model for the AI analysis. Your API keys are stored in chrome.storage.local and the analysis requests go directly from your browser to Google. This keeps the extension privacy-focused and avoids me having to act as a middleman for your data.<p>It\u2019s currently a paid extension (with a lifetime deal) but I really wanted to solve the &quot;black box&quot; frustration of platform moderation for indie hackers.<p>This is my first time working on a Chrome Extension so I&#x27;d love to hear feedback on whether Plasmo is the best framework to use, or any ideas for additional features!", "author": "Nikp263", "timestamp": "2026-01-25T10:56:32+00:00", "score": 3, "num_comments": 9, "products": ["gemini"], "categories": ["tone", "onboarding", "response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:27.195531+00:00", "processed": false}
{"id": "hn_comment_46752298", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46752298", "title": "Re: Latest ChatGPT model uses Elon Musk's Grokipedia a...", "text": "I asked 6 llms &quot;What do you think of Grokipedia as a factual source of information?&quot;. Results: <a href=\"https:&#x2F;&#x2F;pastebin.com&#x2F;cuxfHAr4\" rel=\"nofollow\">https:&#x2F;&#x2F;pastebin.com&#x2F;cuxfHAr4</a><p>I then asked Claude Opus to sumup: <a href=\"https:&#x2F;&#x2F;markdownpastebin.com&#x2F;?id=aa29d92662ac4a9ea7f9b3c1d9aba4ec\" rel=\"nofollow\">https:&#x2F;&#x2F;markdownpastebin.com&#x2F;?id=aa29d92662ac4a9ea7f9b3c1d9a...</a><p>Bottom Line\nAll LLMs agree: Grokipedia is useful for quick orientation but unreliable for serious research, especially on political, controversial, or current event topics. Wikipedia remains the more trustworthy alternative.", "author": "guilamu", "timestamp": "2026-01-25T09:28:49+00:00", "score": null, "num_comments": null, "products": ["claude", "grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:30.452603+00:00", "processed": false}
{"id": "hn_story_46751546", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46751546", "title": "Show HN: Lumina \u2013 Open-source observability for LLM applications", "text": "Hey HN! I built Lumina \u2013 an open-source observability platform for AI&#x2F;LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I&#x27;ve been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5)\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes\n- Existing tools were either too expensive or missing features in free tiers<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.)\n- No vendor lock-in \u2013 standard trace format\n- Integrates in 3 lines of code<p>Key features:\n- Cost &amp; quality monitoring\n\u2013 Automatic alerts when costs spike or responses degrade\n- Replay testing\n\u2013 Capture production traces, replay them after changes, see diffs\n- Semantic comparison\n\u2013 Not just string matching \n\u2013 uses Claude to judge if responses are &quot;better&quot; or &quot;worse&quot;\n- Self-hosted tier \n\u2013 50k traces&#x2F;day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p>How it works:<p>Start Lumina<p>git clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\ncd Lumina&#x2F;infra&#x2F;docker\ndocker-compose up -d<p>&#x2F;&#x2F; Add to your app (no API key needed for self-hosted!)<p>import { Lumina } from &#x27;@uselumina&#x2F;sdk&#x27;;<p>const lumina = new Lumina({\n  endpoint: &#x27;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;traces&#x27;,\n});<p>&#x2F;&#x2F; Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: &#x27;openai&#x27;, model: &#x27;gpt-4&#x27;, prompt: &#x27;...&#x27; }\n);<p>That&#x27;s it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work\n\u2013 50k traces&#x2F;day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. Use standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast\n\u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces&#x2F;min on a single machine.<p>What I&#x27;m looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac&#x2F;Linux&#x2F;WSL2, but I&#x27;m sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently uses Claude, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it&#x27;s:\n- Free to use and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for teams who don&#x27;t want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\n- Demo video: [YouTube link]\n- Docs: <a href=\"https:&#x2F;&#x2F;docs.uselumina.io\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I&#x27;d love to hear what you think! Especially interested in:\n- What observability problems you&#x27;re hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you&#x27;re using (and what they do better)<p>Thanks for reading!", "author": "iggycodexs", "timestamp": "2026-01-25T07:08:52+00:00", "score": 4, "num_comments": 1, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:34.558592+00:00", "processed": false}
{"id": "hn_story_46751191", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46751191", "title": "Show HN: Voice to Text\u2013 Free browser-based speech-to-text with local projects", "text": "Hi HN,<p>I built a voice-to-text tool that runs entirely in your browser. No account required for the free tier, no data sent to my servers.<p>Try it: <a href=\"https:&#x2F;&#x2F;voicetotextonline.com\" rel=\"nofollow\">https:&#x2F;&#x2F;voicetotextonline.com</a><p>Why I built this:<p>- Existing tools require signups, have minute limits, or cost money\n- Google Docs voice typing requires a Google account\n- Dragon costs $150-500\n- Otter.ai has free tier limits<p>(A) Free Features (no account required):<p>1&#x2F; Core Transcription:<p>- Real-time voice-to-text using Web Speech API\n- 55+ languages supported\n- Auto-punctuation &amp; sentence case options\n- Works offline after first load (PWA)<p>2&#x2F; AI Enhance (added based on user survey \u2013 80% voted yes):<p>- Auto-fix grammar, punctuation &amp; formatting\n- One-click cleanup of transcripts<p>3&#x2F; My Projects (local storage):<p>- Save transcripts to browser localStorage\n- Organize with folders (Notes, Work, Personal, etc.)\n- Custom folders &amp; tags\n- Search across all transcripts\n- Edit, copy, download as TXT\n- 100% private \u2013 never leaves your device<p>- Export:<p>- Copy to clipboard\n- Download as TXT or DOCX<p>(B) Pro Features ($10&#x2F;month or $1&#x2F;hour pay-per-use):<p>1&#x2F; File Upload &amp; Transcription:<p>- Upload audio&#x2F;video files (MP3, WAV, M4A, MP4, MOV, AVI, MKV)\n- Up to 500MB per file\n- Batch upload (10 files at once)\n- Powered by AssemblyAI (95%+ accuracy)\n- 150 hours&#x2F;month transcription<p>2&#x2F; Advanced Features:<p>- Real-time progress with ETA\n- Speaker labels\n- In-browser audio recording (5 min with pause&#x2F;resume)\n- Translation to 25+ languages (GPT-4o)<p>3&#x2F; Export Formats:<p>- TXT, SRT, VTT, JSON with timestamps\n- Segment-level timestamp precision<p>4&#x2F; Cloud Storage:<p>- Transcription history in the cloud\n- 10 GB storage, 1,000 files&#x2F;month<p>(C) Data &amp; Privacy:<p>Free tier:<p>- All transcripts stored in browser localStorage only\n- Never touches our servers\n- 100% private<p>Pro tier:<p>- Audio files stored in Supabase (encrypted)\n- Files retained for 30 days for re-download, then auto-deleted\n- Transcripts stored permanently in your account\n- You can delete any transcript or your entire account anytime\n- We don&#x27;t use your data for training<p>Tech stack:<p>- Next.js 14 (App Router)\n- Web Speech API (free real-time transcription)\n- AssemblyAI (Pro file transcription, 95%+ accuracy)\n- OpenAI GPT-4o (AI Enhance &amp; translation)\n- Supabase (auth &amp; storage)\n- Stripe (payments)\n- Tailwind CSS\n- Hosted on Vercel<p>Limitations:<p>- Real-time transcription doesn&#x27;t work in Firefox (Web Speech API not supported)\n- Free tier accuracy depends on Chrome&#x27;s speech engine<p>Would love feedback on UX, pricing, or feature ideas. Considering open-sourcing the core transcription component.", "author": "digi_wares", "timestamp": "2026-01-25T05:57:22+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-25T17:10:35.726423+00:00", "processed": false}
{"id": "hn_story_46750752", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46750752", "title": "Ask HN: A good Model to choose in Ollama to run on Claude Code", "text": "Given that Claude Code supports a locally running model on Ollama, which is the best Thinking Model that supports tooling, can I  pick for good output?<p>Also, if anyone has tried, does it still require a Claude Subscription?<p>(I currently have an RTX 5060 machine with 8GB of VRAM)", "author": "sujayk_33", "timestamp": "2026-01-25T04:34:36+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:37.916412+00:00", "processed": false}
{"id": "hn_story_46750437", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46750437", "title": "Show HN: VM-curator \u2013 a TUI alternative to libvirt and virt-manager", "text": "I&#x27;ve long wanted to harness QEMU&#x2F;KVM for my desktop virtual machines, but I&#x27;m befuddled by virt-manager&#x27;s lack of support for working NVIDIA 3D acceleration, dogmatic embrace of ugly XML, and the puzzling UI decision of having to click what seems like 15 buttons to attach an ISO to a VM image. When I further learned that NVIDIA&#x27;s broken 3D acceleration is the fault of libvirt as opposed to QEMU&#x27;s virtio driver, I had an idea...<p>Behold, vm-curator! A fast and friendly VM management TUI written in Rust. You can create, configure, organize, and manage VMs directly with QEMU. No libvert. No XML. No wonky UI&#x27;s. Just the right level of friendliness, customization, and speed to be really really useful.<p>The best part? 3D para-virtualization works with NVIDIA cards (via virtio-vga-gl!) No jumping through hoops to get GPU passthrough working!<p>(Disclaimer: This works great with other guest Linux VMs, but is not suitable for Windows gaming. If you want to game on Windows within a VM, passthrough is a must. vm-curator will have fast and friendly support soon.)<p>Looking for contributors (especially to help with the ascii art,) and donations are welcome.   (Claude was a big help, but this was not a vibe-coded affair. We pair-programmed approx. 10,000 lines of code here. It was a great way to learn Rust, actually!)", "author": "theYipster", "timestamp": "2026-01-25T03:36:38+00:00", "score": 36, "num_comments": 7, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-25T17:10:39.673435+00:00", "processed": false}
{"id": "hn_story_46750255", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46750255", "title": "Show HN: PicoFlow \u2013 a minimal Python workflow for LLM agents", "text": "Hi HN,<p>I\u2019ve been experimenting with LLM agents for a while and often felt that\nfor simple workflows (chat, tool calls, small loops), existing\nframeworks add a lot of abstraction and boilerplate.<p>So I built a small Python library called PicoFlow. The goal is simple:<p>express agent workflows using normal async Python, not\nframework-specific graphs or chains.<p>Minimal chat agent<p>Each step is just an async function, and workflows are composed with &gt;&gt;:<p><pre><code>  from picoflow import flow, llm, create_agent\n\n  LLM_URL =\n  \u201cllm+openai:&#x2F;&#x2F;api.openai.com&#x2F;v1&#x2F;chat&#x2F;completions?model=gpt-4.1-mini&amp;api_key_env=OPENAI_API_KEY\u201d\n\n  @flow\n  async def input_step(ctx):\n      return ctx.with_input(input(\u201cYou:\u201d))\n\n  agent = create_agent(\n      input_step &gt;&gt;\n      llm(\u201cAnswer the user: {input}\u201d, llm_adapter=LLM_URL)\n  )\n\n  agent.run()\n</code></pre>\nNo chains, no graphs, no separate prompt&#x2F;template objects. You can debug\nby putting breakpoints directly in the async steps.<p>Control flow is just Python<p>Loops and branching are written with normal Python logic, not DSL nodes:<p><pre><code>  def repeat(step):\n      async def run(ctx):\n          while not ctx.done:\n              ctx = await step.acall(ctx)\n              return ctx\n          return Flow(run)\n</code></pre>\nThe framework only schedules steps; it doesn\u2019t try to own your control\nflow.<p>Switching model providers = change the URL<p>Another design choice: model backends are configured via a single LLM\nURL.<p>OpenAI:<p><pre><code>  LLM_URL =\n  \u201cllm+openai:&#x2F;&#x2F;api.openai.com&#x2F;v1&#x2F;chat&#x2F;completions?model=gpt-4.1-mini&amp;api_key_env=OPENAI_API_KEY\u201d\n</code></pre>\nSwitch to another OpenAI-compatible provider (for example SiliconFlow or\nlocal gateways):<p><pre><code>  LLM_URL =\n  \u201cllm+openai:&#x2F;&#x2F;api.siliconflow.cn&#x2F;v1&#x2F;chat&#x2F;completions?model=Qwen&#x2F;Qwen2.5-7B-Instruct&amp;api_key_env=SILICONFLOW_API_KEY\u201d\n</code></pre>\nThe workflow code doesn\u2019t change at all. Only runtime configuration\ndoes. This makes A&#x2F;B testing models and switching providers much cheaper\nin practice.<p>When this is useful (and when it\u2019s not)<p>PicoFlow is probably useful if you:<p>-   want to prototype agents quickly\n-   prefer explicit control flow\n-   don\u2019t want to learn a large framework abstraction<p>It\u2019s probably not ideal if you:<p>-   rely heavily on prebuilt components and integrations\n-   want a batteries-included orchestration platform<p>Repo:<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;the-picoflow&#x2F;picoflow\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;the-picoflow&#x2F;picoflow</a><p>This is still early and opinionated. I\u2019d really appreciate feedback on\nwhether this style of \u201cworkflow as Python\u201d is useful to others, or if\npeople are solving this in better ways already.<p>Thanks!", "author": "shijizhi_1919", "timestamp": "2026-01-25T02:58:46+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:41.519283+00:00", "processed": false}
{"id": "hn_comment_46751016", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46751016", "title": "Re: OpenAI's GPT-5.2 model cites Grokipedia...", "text": "I had duckduckgo return a grokapedia page for the first time. The search page has preview text making it seem like there was information so I clicked the link to check it out and it was a 404 page. What kind of SEO hack is that? Information for the crawler but nothing on the actual page?", "author": "kemotep", "timestamp": "2026-01-25T05:24:28+00:00", "score": null, "num_comments": null, "products": ["grok"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-25T17:10:42.043254+00:00", "processed": false}
{"id": "hn_comment_46752573", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46752573", "title": "Re: OpenAI's GPT-5.2 model cites Grokipedia...", "text": "Recently I asked an obscure question and it thought for awhile and it gave me a lot of output with sources.<p>Over half the citations were from Grok .. not even grokipedia .. just \u201cshare\u201d pages from questions other people asked.", "author": "ratg13", "timestamp": "2026-01-25T10:10:30+00:00", "score": null, "num_comments": null, "products": ["grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:42.183837+00:00", "processed": false}
{"id": "hn_comment_46751966", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46751966", "title": "Re: OpenAI's GPT-5.2 model cites Grokipedia...", "text": "This makes sense. I already use Grokipedia maybe 50% of the time. If you really dig into things, it is - incredibly - more accurate. I often find glaring errors or biases in Wikipedia, especially over the last 5 years.", "author": "lazzlazzlazz", "timestamp": "2026-01-25T08:34:08+00:00", "score": null, "num_comments": null, "products": ["grok"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-01-25T17:10:42.255219+00:00", "processed": false}
{"id": "hn_story_46748986", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46748986", "title": "Show HN: Ask CLI \u2013 A simple tool to get help with commands from the terminal", "text": "I want to share Ask CLI, a tool I developed to get help with commands and coding directly from the terminal. It is a simple app designed to do one thing well: provide instant command assistance. This isn&#x27;t a complex coding agent like Claude Code; it is built specifically to get short, fast answers without context switching.<p>As a developer, I\u2019ve always struggled to remember every command and its specific options. Whenever I need to use tools like Docker, Git, or psql, I find myself leaving the terminal to check documentation or scrolling through verbose --help text just to recall a specific flag. I usually know what I want to do, but I forget the exact syntax. I didn&#x27;t want to waste time switching to Google or ChatGPT just to find a one-line command.<p>I developed Ask CLI to solve this. It has been a game-changer for my workflow. Now, when I forget a command, I simply ask my terminal. It gives me a fast, precise answer\u2014exactly what I need\u2014without breaking my flow.<p>It is incredibly easy to use: just select an AI model, set your API key, and start chatting naturally with your terminal.<p>Examples:<p>$ ask how to run a docker container with env variables<p>$ ask how to setup my local git account<p>You can also use the &quot;what&quot; and &quot;how&quot; aliases for a more natural feel:<p>$ what is chmod<p>$ how to print all the env variables<p>You can use Ask CLI with popular hosted models (Gemini, Claude, ChatGPT) or with local models and external providers that support OpenAI-compatible APIs (Ollama, llama.cpp, LM Studio, etc.).<p>Ask CLI is free and open-source. Check it out here:<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;david-minaya&#x2F;ask\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;david-minaya&#x2F;ask</a>", "author": "david-minaya", "timestamp": "2026-01-24T23:47:45+00:00", "score": 4, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["tone", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-25T17:10:48.326827+00:00", "processed": false}
