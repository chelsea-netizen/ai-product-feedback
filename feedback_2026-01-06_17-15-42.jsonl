{"id": "hn_story_46514632", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46514632", "title": "Show HN: AI that edits your files directly, no approvals [demo]", "text": "Hey HN,<p>I am building Aye Chat, an open-source terminal workspace that integrates AI code generator directly into your shell, allowing you to edit files and run commands as well as prompt AI seamlessly.<p>The AI writes code directly to your files immediately, eliminating the &quot;review and approve&quot; out of the loop.<p>At the same time, every AI edit is snapshotted locally, so you can instantly undo any change with a single command. This automatic file update with a safety net is the core idea.<p>Also, in the same session, you can run shell commands, open Vim, and ask the AI to modify your code, turning it into an AI-powered terminal.<p>I built this because I got tired of the &quot;suggest -&gt; review -&gt; approve -&gt; changes applied&quot; loop in existing AI coding tools. As models improve and generate proper code more often than not, manual approval started to feel unnecessary as long as there is a strong safety net to allow easy rollback of the changes. Yes, the idea is not exactly groundbreaking: other tools already offer &quot;set a dangerous flag and remember to recover later&quot; settings, but what I am exploring is defaults, not capability. With Aye Chat, the default is the opposite: apply immediately and make undo trivial. There are no flags to remember, no mode switches, and you don&#x27;t need to exit the tool to roll back.<p>You can watch a 1-minute demo here: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;h5laV5y4IrM\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;h5laV5y4IrM</a><p>Basically, the typical workflow goes like this (instead of a chat window, you stay in your terminal):<p><pre><code>  $ aye chat # starts the session\n  &gt; fix the bug in server.py\n   Fixed undefined variable on line 42\n\n  &gt; vim server.py\n  [opens real Vim, returns to chat after]\n\n  &gt; refactor: make it async\n   Updated server.py with async&#x2F;await\n\n  &gt; pytest\n   Tests fail\n\n  &gt; restore\n   Reverted last changes\n</code></pre>\nI use Aye Chat both in my work projects and to build Aye Chat itself. Recently, I used it to implement a local vector search engine in just a few days.<p>Lower-level technical details that went into the tool:<p>The snapshot engine is a Python-based implementation that serves as a lightweight version control layer.<p>For retrieval, we intentionally avoided PyTorch to keep installs lightweight. Instead, we use ChromaDB with ONNXMiniLM-L6_V2 running on onnxruntime.<p>File indexing runs in the background using a fast coarse pass followed by AST-based refinement.<p>What I learned:<p>The key realization was that the bottleneck in AI coding is often the interface, not the model. By shifting to the AI writing code immediately with users still maintaining full control to undo those changes - we eliminate the cognitive load of the review-and-approve loop.<p>I also learned that early users do not accept a custom snapshot engine, so to make it professional-grade we are now integrating it with git refs.<p>What I\u2019d love feedback on:\n- After using it for a while, did replacing approvals with undo actually change how you work, or did it feel no different from existing terminal-based AI tools (GitHub Copilot CLI, Claude Code)?\n- Was there a moment when this started to feel natural to use, or did it never quite click?<p>There is a 1-line quick install:<p><pre><code>  pip install ayechat\n  </code></pre>\nHomebrew and Windows installer are also available.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;acrotron&#x2F;aye-chat\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;acrotron&#x2F;aye-chat</a>. If you find it interesting, a repo star would mean a lot!<p>It&#x27;s early days, but Aye Chat is working well for me. I would love to get your feedback. Feel free to hop into the Discord (<a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;ZexraQYH77\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;ZexraQYH77</a>) and let me know how it goes.", "author": "acro-v", "timestamp": "2026-01-06T16:42:06+00:00", "score": 1, "num_comments": 0, "products": ["claude", "copilot"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-01-06T17:15:46.158702+00:00", "processed": false}
{"id": "hn_story_46514270", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46514270", "title": "Show HN: Hacker News API on SerenAI X402 Gateway", "text": "We added the official Hacker News API to SerenAI&#x27;s x402 Gateway.<p>AI agents can now query HN stories, comments, and users through the same MCP interface they use for Seren&#x27;s paid publishers like Firecrawl and Perplexity. We&#x27;re aiming to deliver greater composability: an agent can pull HN discussions (free), scrape competitor sites with Firecrawl, and get AI analysis from Perplexity, all in one protocol.<p>We built a competitive intelligence example that tracks Show HN launches and competitor mentions across HN.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;serenorg&#x2F;x402-mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;serenorg&#x2F;x402-mcp-server</a> \nInstall: npx @serendb&#x2F;x402-mcp-server", "author": "taariqlewis", "timestamp": "2026-01-06T16:16:43+00:00", "score": 3, "num_comments": 0, "products": ["perplexity"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-06T17:15:47.887705+00:00", "processed": false}
{"id": "hn_story_46514188", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46514188", "title": "Show HN: Mocklantis \u2013 Mock server with live endpoint updates (no restart needed)", "text": "I built a desktop mock server that lets you update endpoints while it&#x27;s running. No restart required.<p>The problem: Every time I changed a mock endpoint in other tools, I had to restart the server. WebSocket connections dropped, test flows broke.<p>The solution: Catch-all handlers that read config on every request. Change anything \u2013 routes, responses, delays \u2013 and it takes effect immediately. WebSocket&#x2F;SSE connections stay alive.<p>Features that might interest you:<p>\u2022 State machine for multi-step API flows (visual editor)<p>\u2022 Chaos engineering: corrupt responses, inject latency, simulate rate limits<p>\u2022 WebSocket mocking with 3 modes (conversational, streaming, triggered)<p>\u2022 SSE streaming (perfect for mocking ChatGPT-style responses)<p>\u2022 API recording: proxy to real API, auto-capture as mock endpoints<p>\u2022 Response templating: {{request.path.id}}, {{request.body.user.name}}<p>Privacy: Runs 100% local. No cloud, no telemetry, no account.<p>Free for local development. macOS&#x2F;Windows&#x2F;Linux.<p><a href=\"https:&#x2F;&#x2F;mocklantis.com\" rel=\"nofollow\">https:&#x2F;&#x2F;mocklantis.com</a>", "author": "mstykt", "timestamp": "2026-01-06T16:11:26+00:00", "score": 3, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:15:48.448404+00:00", "processed": false}
{"id": "hn_story_46513952", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46513952", "title": "Show HN: Sentience \u2013 Semantic Visual Grounding for AI Agents (WASM and ONNX)", "text": "Hi HN, I\u2019m the solo founder behind SentienceAPI. I\u2019ve spent the last December building a browser automation runtime designed specifically for LLM agents.<p>The Problem: Building reliable web agents is painful. You essentially have two bad choices:<p>Raw DOM: Dumping document.body.innerHTML is cheap&#x2F;fast but overwhelms the context window (100k+ tokens) and lacks spatial context (agents try to click hidden or off-screen elements).<p>Vision Models (GPT-4o): Sending screenshots is robust but slow (3-10s latency) and expensive (~$0.01&#x2F;step). Worse, they often hallucinate coordinates, missing buttons by 10 pixels.\nThe Solution: Semantic Geometry Sentience is a &quot;Visual Cortex&quot; for agents. It sits between the browser and your LLM, turning noisy websites into clean, ranked, coordinate-aware JSON.<p>How it works (The Stack):<p>Client (WASM): A Chrome Extension injects a Rust&#x2F;WASM module that prunes 95% of the DOM (scripts, tracking pixels, invisible wrappers) directly in the browser process. It handles Shadow DOM, nested iframes (&quot;Frame Stitching&quot;), and computed styles (visibility&#x2F;z-index) in &lt;50ms.<p>Gateway (Rust&#x2F;Axum): The pruned tree is sent to a Rust gateway that applies heuristic importance scoring with simple visual cues (e.g. is_primary)<p>Brain (ONNX): A server-side ML layer (running ms-marco-MiniLM via ort) semantically re-ranks the elements based on the user\u2019s goal (e.g., &quot;Search for shoes&quot;).<p>Result: Your agent gets a list of the Top 50 most relevant interactable elements with exact (x,y) coordinates with importance value and visual cues, helping LLM agent make decision.<p>Performance:<p>Cost: ~$0.001 per step (vs. $0.01+ for Vision)<p>Latency: ~400ms (vs. 5s+ for Vision)<p>Payload: ~1400 tokens (vs. 100k for Raw HTML)<p>Developer Experience (The &quot;Cool&quot; Stuff): I hated debugging text logs, so I built Sentience Studio, a &quot;Time-Travel \nDebugger.&quot; It records every step (DOM snapshot + Screenshot) into a .jsonl trace. You can scrub through the timeline like a video editor to see exactly what the agent saw vs. what it hallucinated.<p>Links:<p>Docs &amp; SDK: <a href=\"https:&#x2F;&#x2F;www.sentienceapi.com&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;www.sentienceapi.com&#x2F;docs</a><p>GitHub (SDK):\nSDK Python: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;SentienceAPI&#x2F;sentience-python\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;SentienceAPI&#x2F;sentience-python</a><p>SDK TypeScript: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;SentienceAPI&#x2F;sentience-ts\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;SentienceAPI&#x2F;sentience-ts</a><p>Studio Demo: <a href=\"https:&#x2F;&#x2F;www.sentienceapi.com&#x2F;docs&#x2F;studio\" rel=\"nofollow\">https:&#x2F;&#x2F;www.sentienceapi.com&#x2F;docs&#x2F;studio</a><p>Build Web Agent: <a href=\"https:&#x2F;&#x2F;www.sentienceapi.com&#x2F;docs&#x2F;sdk&#x2F;agent-quick-start\" rel=\"nofollow\">https:&#x2F;&#x2F;www.sentienceapi.com&#x2F;docs&#x2F;sdk&#x2F;agent-quick-start</a><p>Screenshots with importance labels (gold stars):\n<a href=\"https:&#x2F;&#x2F;sentience-screenshots.sfo3.cdn.digitaloceanspaces.com&#x2F;Screenshot\" rel=\"nofollow\">https:&#x2F;&#x2F;sentience-screenshots.sfo3.cdn.digitaloceanspaces.co...</a> 2026-01-06 at 7.19.41 AM.png<p><a href=\"https:&#x2F;&#x2F;sentience-screenshots.sfo3.cdn.digitaloceanspaces.com&#x2F;Screenshot\" rel=\"nofollow\">https:&#x2F;&#x2F;sentience-screenshots.sfo3.cdn.digitaloceanspaces.co...</a> 2026-01-06 at 7.19.41 AM.png<p>I\u2019m handling the backend in Rust and the SDKs in Python&#x2F;TypeScript. The project is now in beta launch, I would love feedbacks on the architecture or the ranking logic!", "author": "tonyww", "timestamp": "2026-01-06T15:57:43+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:15:49.789194+00:00", "processed": false}
{"id": "hn_story_46513919", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46513919", "title": "Show HN: Ilseon, as a GTD \"Capture\" App", "text": "I\u2019ve always struggled with the Capture phase. Most apps feel too heavy in the moment. Even coming up with a title can be enough to break the flow.<p>My app Ilseon (Android) addresses that specific problem. It\u2019s not a full GTD system or project manager. It\u2019s a targeted task manager designed to reduce mental noise and help users focus on one thing at a time.<p>Ilseon has GTD elements in its workflow though:<p>* Fast capture\nTasks and ideas can be captured with almost no structure. No required titles, tags, or projects at the moment of capture.<p>* Voice capture for hands-busy moments\nWhen typing isn\u2019t practical (walking, driving), I record a short voice memo. Optionally, if you add a Gemini API key, Ilseon can extract tasks from the transcript, but this is strictly optional and happens after capture, not during.<p>* An idea inbox\nThere\u2019s a separate scratchpad for thoughts that aren\u2019t quite tasks yet. During a review, these can be promoted into tasks or notes, or discarded.<p>* Reflection after completion\nCompletion isn\u2019t the end. A small reflection step encourages reviewing what was actually done.<p>* Local-first storage\nAudio is saved as standard .m4a files in a local folder (Recordings&#x2F;ilseon&#x2F;).<p>I\u2019m looking for feedback from people who care about a clean capture process. Do you experience friction during capture with your current tools, especially when you\u2019re away from your desk?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;cladam&#x2F;ilseon\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;cladam&#x2F;ilseon</a><p>Play Store: <a href=\"https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.ilseon\">https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.ilseon</a>", "author": "cladamski79", "timestamp": "2026-01-06T15:55:58+00:00", "score": 2, "num_comments": 0, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-06T17:15:49.987707+00:00", "processed": false}
{"id": "hn_story_46513846", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46513846", "title": "Show HN: A lightweight, E2E encrypted pastebin built with Svelte 5 and Hono", "text": "I built this because I needed a simple way to send snippets to colleagues or copy&#x2F;paste text from my phone to a random computer without logging into anything. I used a few other services for a while, but the downtime and general bloat finally got to me. I decided to build my own over the New Year break.<p>It is live here: <a href=\"https:&#x2F;&#x2F;yp.pe\" rel=\"nofollow\">https:&#x2F;&#x2F;yp.pe</a><p>Full disclosure: I vibe coded the vast majority of this using Claude Code, but I kept a pretty tight leash on the logic. The full commit history is public if you want to see the fumbles and the process. I will be the first to admit that some of my initial architecture decisions were not the best, and I completely own up to that, but I am happy with where the end result landed.<p>The Features:<p>- Fast and Lightweight: No ads and no tracking. CORS policy blocks Cloudflare analytics.<p>- Real-time Collab: Uses Yjs&#x2F;CRDTs. It is limited to 10 concurrent editors by default on a first come first served basis, but it allows unlimited viewers.<p>- &quot;Smart&quot; Slugs: Slugs are kept as short as possible. I specifically removed ambiguous characters like capital I and lowercase l so it is easy to type the URL manually into another computer address bar.<p>- Note Controls: You can set notes to expire after a certain time or after a specific number of views. By default, any note not accessed in 90 days is automatically cleaned up.<p>- Privacy: No logins. E2E encryption for password protected notes. Passwords and hashes never leave your browser, only encrypted blobs do. There is a Playwright test in the repo that verifies this.<p>- The Rest: Custom slugs, syntax highlighting via highlight.js, rate limiting, PWA installable.<p>The Caveats:\nI wanted to avoid the complexity of ownership, so the rules are simple: anyone can edit or delete any note. It is designed for quick, ephemeral use rather than long-term storage. If someone takes your slug, they can delete it and you can take it back. It is a bit of a free-for-all, but it keeps the codebase clean.<p>Technical Stack:<p>- Frontend: Svelte 5 with runes<p>- Backend: Hono<p>- Infrastructure: Runs on Cloudflare Workers, using Durable Objects for the real-time sync and D1 for the database.<p>It has not been tested at scale, but since it is on Workers, I hope it holds up. Now that the holidays are over and I am heading back to work, I will not have a ton of time to maintain this, so PRs are very welcome if you find bugs.<p>I am hosting a public instance for now at yp.pe. If the costs get crazy I might not be able to keep it public, but I tried to make it as easy as possible to self-host with deployment scripts and documentation in the repo.", "author": "yashau", "timestamp": "2026-01-06T15:51:25+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:15:50.154197+00:00", "processed": false}
{"id": "hn_story_46513760", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46513760", "title": "Show HN: Claude Bootstrap \u2013 Opinionated Guardrails for Claude Code", "text": "I&#x27;ve been using Claude Code for more than 100 projects over the past year (since Feb 24, 2025 to be exact :)) and kept running into the same problem - the AI generates tons of code but then I&#x27;m stuck trying to review and understand it all. Created different workflows, approve line by line (too cumbersome), create manual reviews (also cumbersome) etc.<p>Then I realized the bottleneck isnt code generation anymore, its code comprehension. Like, AI can spit out infinite code but we still gotta review it, maintain it, debug it (sometimes at 2am when something breaks lol).<p>So I built this thing called Claude Bootstrap. Its basically a collection of &quot;skills&quot; (markdown files) that Claude reads before writing any code. Think of it like giving Claude a coding standards doc but one it actually follows.<p>The main stuff it enforces:<p>- TDD is mandatory - tests have to fail first before you write implementation. sounds annoying but it actually catches so many bugs \n- hard limits on complexity - 20 lines per function, 200 per file. forces you to keep things simple\n- theres a pre-push hook that runs code review and blocks if it finds critical issues\n- it tracks when your changes are getting too big for a reviewable PR<p>heres how to use it:<p>git clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;alinaqi&#x2F;claude-bootstrap.git\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;alinaqi&#x2F;claude-bootstrap.git</a> ~&#x2F;.claude-bootstrap\ncd ~&#x2F;.claude-bootstrap &amp;&amp; .&#x2F;install.sh<p># then in any project\nclaude\n&gt; &#x2F;initialize-project<p>it sets up skills files, pre-push hooks, session management for context across convos, todo tracking etc.<p>got like 39+ skills now covering react, node, python, supabase, shopify, even reddit ads api and ms teams bots<p>the key insight for me was that ppl struggle with claude code not becuase the tool is bad, but because theres no guardrails. the delta is in the instructions you give it.<p>honest limitations tho:<p>this isnt magic. sometimes i still have to remind claude to follow TDD - itll try to jump straight to implementation and i gotta be like &quot;nope, write the failing test first&quot;. the skills help but you gotta believe in them and keep enforcing them over and over.<p>the biggest mindset shift for me was bug fixing. when i find a bug now, i dont ask the AI to just fix it. i ask it &quot;why did our tests miss this?&quot; first. find the gap in test coverage, write a test that catches the bug, THEN fix it. otherwise you&#x27;re just playing whack-a-mole.<p>also the 20 line limit - claude will sometimes split things weirdly just to hit the limit. you gotta use judgement. the limit is there to make you think, not to follow blindly.<p>and context management is still annoying. long sessions = claude forgets stuff. thats why theres session management skills but its not perfect. i still lose context sometimes and have to re-explain things.<p>hygiene is a big one too. i constantly enforce organization - active todos go in active.md, completed ones move to completed.md, all specs live in _project_specs&#x2F;. claude will randomly dump files wherever if you dont watch it. i review file placements regularly and clean up anything thats in the wrong spot. sounds tedious but a messy repo = messy AI output. garbage in garbage out.<p>basically: these skills are guardrails not autopilot. you still gotta drive. and keep the car clean lol<p>curious what guardrails others have found useful? or if anyones tried something similar with cursor or other ai coding tools", "author": "naxmax", "timestamp": "2026-01-06T15:46:28+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["tone", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:15:50.556151+00:00", "processed": false}
{"id": "hn_comment_46512483", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46512483", "title": "Re: Claude Quick \u2013 TUI orchestrating multiple Claude C...", "text": "Hey HN, I built this because I was juggling too many Claude Code sessions across different branches and kept losing track.<p>Claude Quick gives you a single dashboard to manage devcontainers, each with its own Claude Code agent. The killer feature for me is git worktree integration spin up a new branch in an isolated container, have Claude work on it, and switch between them without context pollution.<p>It also handles credential injection (API keys from files, env vars, or 1Password&#x2F;etc.) so you&#x27;re not copy-pasting tokens around.<p>Bonus: because it&#x27;s a TUI, it works surprisingly well over SSH from mobile. I&#x27;ve been doing a lot of &quot;vibe engineering&quot; from my phone kick off a task, check back later, review the diff. Feels like the future.<p>Written in Go, uses Bubble Tea for the TUI. Would love feedback on the UX and what else would be useful.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;christophergyman&#x2F;claude-quick\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;christophergyman&#x2F;claude-quick</a>", "author": "int32max", "timestamp": "2026-01-06T14:13:01+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-06T17:16:00.713528+00:00", "processed": false}
{"id": "hn_story_46512398", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46512398", "title": "Small things that will make 3x better at vibe coding", "text": "I have been writing a few posts about improving your vibe coding efficiency lately and this post is one more such contribution. I am building products since 6+ years and this post is about things that you can just start doing and see your vibe coding skills improve with huge margins.<p>One thing to start off, this is neither a guru-kind of post where I will claim I know something you all don&#x27;t know, nor it is that I am any more intelligent than you. ItItss simply just my personal experience.<p>Here are the things (by increasing order) -<p>1. Building from 1st Principles : Most vibe coders start off by just chasing something extra complex which might look achievable till 90%, but gets much more complicated in the later 10% due to the engineering limitations of human or artificial intelligence. So, rather than telling the AI rigorously to &quot;fix the code&quot;, see the code, send parts of the code to other reasoning models, and think - &quot;Why at the point, this error is occurring?&quot;<p>If you start thinking in less complex questions, your reasoning capacity becomes strong thus making your overall interaction with vibe coding applications stronger.<p>2. Think in terms of structure : This is one of the key things vibe coders are missing. Most of us just write a prompt and expect AI to come up with a ready-to-deploy version. But honestly, AI is not our senior developer (yet).<p>If I am to start with a new project today, I will come up with the list of all stacks I will be using - Db, auth, vectors, ML libraries, etc. and then create a list of these libraries, and a folder structure to start off. I might use ChatGPT to create this but the main point is, I get it pre-built before starting to vibe code. Once done, I will let my IDE take over it.<p>3. Documenting &amp; Always be Knowing What Your Code Does : One major setback for vibe coders is that most of them don&#x27;t really understand what part of their code is doing exactly what? That&#x27;s the reason why a developer can do atleast 3 times better vibe coding than a regular person coz he can read the terminal for errors, see through code and apply his intelligence in writing prompts that will fix things.<p>Now, don&#x27;t start learning programming because of this. You are way better than that. But just get knowledge of how backend like HTTP, CD&#x2F;CI pipelines, SQL, guardrails, evals, Websockets, API, Redis, etc. works. Once you know what this is, you will be much better at mindful thinking and building stuff.<p>About documenting, you can just create one file (mind.md or something) that will contain all major changes and logs of things you are doing. Even you can go a step beyond and create proper documentation for you, your users and future dev team to help understand what your code is about. You can use SuperDocs[dot]cloud for using AI to create documents without actually writing it.<p>At last, the advice is to think and have knowledge of stack like a developer but execute with AI. That will help you make much better apps at scale. I know, I might have missed a few things, feel free to add stuffs in the replies.<p>Also, if you are building this by yourself and got stuck somewhere or have any question, please ask me, I will try to respond you.", "author": "udit_50", "timestamp": "2026-01-06T14:07:10+00:00", "score": 2, "num_comments": 1, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-06T17:16:01.077285+00:00", "processed": false}
{"id": "hn_story_46512032", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46512032", "title": "Show HN: I'd never touched Swift. Built a Mac app in 4 weeks", "text": "Hey HN!<p>3-4 weeks ago I was trying to record a webinar for another side project. Screen recording, full screen presentation, talking through slides \u2014 should be simple.<p>Except I kept forgetting what I wanted to say. And every time I glanced at my notes, it was obvious on camera. Worse \u2014 when recording full screen, there&#x27;s nowhere to put notes without them being captured.<p>Tried a few teleprompter apps. They either covered half my screen, scrolled at a fixed speed I couldn&#x27;t match, or showed up in the recording.<p>So I thought: I&#x27;ve been a full-stack dev for 20+ years. Never touched Swift or SwiftUI. Can I actually build a native Mac app?<p>Turns out \u2014 yes. Took about 4 weeks of evenings and weekends. Claude helped me get past the Swift learning curve when I got stuck, but the architecture and problem-solving was mine.<p>What it does:\n- Sits in the MacBook notch area (right below camera)\n- Scrolls based on voice \u2014 speak and it moves, pause and it waits\n- Invisible during screen sharing&#x2F;recording (uses a specific NSWindow level that screen capture APIs ignore)\n- Runs 100% locally, no cloud<p>The technically interesting bits:\n- Voice detection uses just audio input, mic \u2014 just detecting audio levels, not transcribing\n- The &quot;invisible during screen share&quot; trick is just the right window level + excluding from capture\n- SwiftUI made the UI surprisingly fast to build coming from React&#x2F;NodeJS<p>The biggest surprise: Launched on Product Hunt on Dec 31st. No prep, no audience, no hunter \u2014 just posted it myself. Hit #4 for the day, got into their newsletter (1M+ subs), 75 sales so far. Wild.<p>For a niche utility I built to scratch my own itch, this exceeded every expectation.<p>Curious if others here have jumped into unfamiliar stacks for side projects. How steep was the learning curve?<p>Site: notchie.app", "author": "amortka", "timestamp": "2026-01-06T13:35:50+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-06T17:16:03.009430+00:00", "processed": false}
{"id": "hn_story_46511704", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46511704", "title": "Beyond 1s and 0s: Can AI Reason Without the Ability to Ask \"Why?\"", "text": "Today at CES 2026, Jensen Huang stated: &quot;Physical AI requires three computers.&quot;<p>An AI Supercomputer (DGX) to train the brain.\nA Simulation Computer (Omniverse) to simulate the world (Expectation).\nA Robot Computer (Jetson) to act in the real world (Observation).<p>The core of this architecture is the intentional separation of Simulation and Reality\u2014designed to create a &quot;Sim-to-Real Gap.&quot; When the simulation says &quot;this floor is safe&quot; but the robot feels &quot;slippery,&quot; that gap forces the system to become smarter.<p>For months, I have been applying this same principle to pure information and logic.<p>My core argument: We must engineer intentional contradiction.<p>Current AI: Input -&gt; Pattern Match -&gt; Output (1 or 0). Fast. Efficient. Hollow.<p>What I propose: Input -&gt; Detect Gap (A \u2260 B) -&gt; Ask &quot;Why?&quot; -&gt; Search -&gt; Resolve -&gt; Output (1 or 0). Slower. But there is a process.<p>The final output is still binary. But the path mirrors human reasoning:\nRecognizing something does not fit.\nAsking &quot;Why?&quot;\nSearching for missing context.\nForming a conclusion.<p>Same destination. Different journey. That journey is what we call &quot;thinking.&quot;<p>We often talk about the &quot;Uncanny Valley&quot; of AI. It seems smart, yet we cannot fully trust it. I believe this exists because the world is not binary\u2014reality is messy, probabilistic, contradictory\u2014while AI collapses everything into 1 or 0 as quickly as possible.<p>This is why I am skeptical of current A2A (Agent-to-Agent) trends. If Agent A outputs a probability and Agent B processes it into another probability, we are just stacking 1s and 0s. For true collaboration, Agent A must output something else: a gap, a process, a question Agent B can meaningfully engage with.<p>I have been developing the Contextual Knowledge Network (CKN) to test this theory, focusing on Finance\u2014the most contradictory field I know.<p>The principle:\nScore Stream A (Logic&#x2F;Expectation) and Stream B (Observation&#x2F;Reality) independently.\nTrigger &quot;Why?&quot; only when dissonance occurs.<p>Example: Stream A (News): &quot;Positive earnings, price should rise&quot; -&gt; +9. Stream B (Chart): &quot;Price is dropping&quot; -&gt; -7. Dissonance detected -&gt; Trigger &quot;Why?&quot; -&gt; AI investigates hidden context.<p>This offers:\nEfficiency: Tag IDs and scores instead of full paragraphs reduce token consumption by 1,000x.\nEnergy: Lightweight reasoning on edge devices, not massive data centers.\nSovereignty: Reasoning structure independent of underlying models (OpenAI, Anthropic).<p>I searched for academic papers on &quot;contradiction handling.&quot; While there is research, I have yet to find: &quot;Use contradiction as the fundamental trigger for reasoning itself.&quot;<p>An AI once told me, &quot;Technology without proof has no value.&quot; So I built a proof of concept, and ironically, it became a business. That is life.<p>Discussion points:\nIs creativity just probability matching, or does it require conscious contradiction detection?\nShould we focus less on scaling GPUs and more on better triggers like contradiction detection?\nIf we reduce token consumption by 1,000x through structured reasoning, does &quot;Green AI&quot; become viable for agentic systems?<p>I realize these are bold claims, but I have phrased them strongly to spark genuine technical debate. I welcome critiques\u2014especially if you think I am completely wrong.<p>Note: I am Korean. I used an LLM to refine my English, which is ironically fitting for a post about AI. But the core ideas are mine.", "author": "RagAlgo", "timestamp": "2026-01-06T13:01:39+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:05.287973+00:00", "processed": false}
{"id": "hn_comment_46511628", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46511628", "title": "Re: Show HN: A file-based agent memory framework that ...", "text": "Feels like file-system-style storage is pretty similar, conceptually, to Claude\u2019s current Skills design.", "author": "mikasisiki", "timestamp": "2026-01-06T12:54:03+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-06T17:16:06.804138+00:00", "processed": false}
{"id": "hn_comment_46514723", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46514723", "title": "Re: System: Control your Mac from anywhere using natur...", "text": "Seems like it needs higher level stuff, although that&#x27;s a bit too sci-fi. Captain Picard can just ask &quot;Computer, where is Commander Riker?&quot; and the computer answers him, he doesn&#x27;t need to say &quot;Computer, launch people finder app&quot; and &quot;Computer, input &#x27;Commander Riker&#x27; in the people finder app&quot;...<p>I use Google Assistant for things like &quot;add a reminder&quot;, &quot;set an alarm&quot;, which is natural language processing but doesn&#x27;t seem to need so many neurons as LLM. And faster than this Gemini crap, anyway.<p>I saw a social media clip of a woman in the passenger car of a Chinese car (her - presumably husband - is driving) asking the car &quot;Has there been a woman in this car other than me?&quot;. The car seems to have an LLM app, because it responds saying &quot;I can&#x27;t see that&quot;, and then start giving tips how to find out (check the recent addresses list in th navigation, check the trips log if there has been long trips, see if the car is cleaner than he usually maintains it), and ending with talking about trust and communication in a relationship...<p>Hah, in our imagination we&#x27;d get KITT from Knight Rider. In reality...", "author": "netsharc", "timestamp": "2026-01-06T16:47:46+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:08.749052+00:00", "processed": false}
{"id": "hn_comment_46511226", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46511226", "title": "Re: Claude Code as my co-founder and COO...", "text": "I&#x27;m running a one-person AI consulting startup with Claude Code as my COO.\nNot a metaphor \u2014 it actually runs operations.<p>Every morning, agent squads execute: research competitors, draft content, monitor costs, update memory.\nI make decisions, Claude executes them across 16 domain squads.<p>What this actually looks like:<p>10 Claude Code sessions running in parallel right now\n  - 16 squads (marketing, engineering, finance, customer, etc.)\n  - ~100 agent definitions, all markdown files\n  - Autonomy and delegation with Task (subagents)\n  - Smart organization of agents, skills and permissions\n  - Hooks usage to local store and track the Goals, kpi, task resolutions, token economics.   \n  - Shared memory via Postgres, session coordination via Redis\n  - squads dash shows what&#x27;s running, what it cost, what changed\n - Claude is aware of the operation and priorities<p>Why parallel sessions need infrastructure?:<p>One Claude session is easy. But when you have 10 or more running \u2014 one researching, one writing, one monitoring costs \u2014 they need shared state. \nOtherwise they overwrite each other or duplicate work.<p>Agents     = Markdown files (.agents&#x2F;squads&#x2F;*.md)\n  Memory     = Postgres (persists across sessions)\n  Sessions   = Redis (coordination, locks)\n  Costs      = OpenTelemetry (what each run cost)<p><pre><code>  The results:\n</code></pre>\nMy GitHub contributions are up 10x since switching to this setup. Not because I&#x27;m working harder \u2014 because the COO handles the grunt work while I focus on decisions.<p>Why we built it this way:<p>We&#x27;re dogfooding. If we&#x27;re going to sell AI agent implementations to clients, we should run on them ourselves.\nEvery pain point we hit, every failure mode \u2014 that&#x27;s consulting IP.<p><pre><code>  Honest status:\n\n  -  16 squads running daily operations\n  -  6 parallel Claude sessions, fully coordinated\n  -  10x GitHub contributions\n  -  Every agent run tracked and costed\n  -  First consulting clients in pipeline\n</code></pre>\nThis is what running a company with an AI COO actually looks like in January 2026.\ncheck all what we were able to build since Opus 4.5 (11-24-25), the best COO llm so far.<p><pre><code>  Site: https:&#x2F;&#x2F;agents-squads.com\n  CLI (open source): https:&#x2F;&#x2F;github.com&#x2F;agents-squads&#x2F;squads-cli\n  Market intelligence Reports: https:&#x2F;&#x2F;agents-squads.com&#x2F;research&#x2F;enterprise-ai-agents-2025\n</code></pre>\nHappy to answer questions about what works, what doesn&#x27;t, and what surprised us.", "author": "koke_vidaurre", "timestamp": "2026-01-06T11:53:27+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["naming_terminology", "response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:09.408861+00:00", "processed": false}
{"id": "hn_comment_46511044", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46511044", "title": "Re: China reported EUV prototype, what it mean to glob...", "text": "Recent reports suggest China has completed a prototype extreme ultraviolet (EUV) lithography machine \u2014 a milestone long thought years away \u2014 as part of a concentrated state-led effort in Shenzhen. The system reportedly generates EUV light but hasn\u2019t yet produced working chips, and advanced lithography remains dominated by ASML\u2019s commercial machines.<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SiCarrier?utm_source=chatgpt.com\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SiCarrier?utm_source=chatgpt.c...</a>", "author": "nishilpatel", "timestamp": "2026-01-06T11:23:00+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-06T17:16:10.555510+00:00", "processed": false}
{"id": "hn_story_46511040", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46511040", "title": "Show HN: SummonAI Kit \u2013 One CLI to rule your .claude/ folder", "text": "For months I wandered the wilderness like Frodo without a map. Every Claude Code session began with the same ritual \u2014 re-explaining my stack, my patterns, my conventions. Context lost to the void.\nI thought this was the way. I was a fool of a Took.<p>Started handcrafting skills and agents manually. 40+ hours of writing markdown scrolls. Mass trial and error \u2014 what structure works, what Claude actually reads, what gets ignored. It worked \u2014 Claude transformed from &quot;helpful tavern stranger&quot; to a wizard who actually knew the realm.<p>Then I thought: why am I forging this ring by hand every time?<p>Built a CLI that scans your codebase and summons the entire .claude&#x2F; structure:<p>CLAUDE.md \u2014 your project&#x27;s sacred text<p>&#x2F;skills&#x2F; \u2014 deep lore for your stack (React patterns, API conventions, DB schemas)<p>&#x2F;agents&#x2F; \u2014 your fellowship (debugger who tracks bugs to their lair, reviewer who enforces your guild&#x27;s conventions)<p>Important bit: the outputs are generated, but they&#x27;re built on top of handcrafted prompts, skill templates, and agent structures that took weeks of iteration to get right. Not just &quot;throw your codebase at an LLM and pray.&quot; The scaffolding is battle-tested \u2014 the CLI customizes it to your specific project.\nIt reads your package.json, tsconfig, actual source files, folder structure \u2014 then generates context specific to YOUR project.<p>Not generic boilerplate from the Mines of Moria.<p>On the toll: $99 one-time, limited launch price \u2014 goes up once this window closes. I know HN has thoughts on paid tools \u2014 speaking friend and entering upfront. Solo dev here, mass time invested. The 40+ hours of trial and error I went through? You skip all of it.<p>Anyone else deep in the skills&#x2F;agents rabbit hole? Curious what setups the council is using.", "author": "viktorb01", "timestamp": "2026-01-06T11:22:32+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:10.625766+00:00", "processed": false}
{"id": "hn_story_46510884", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46510884", "title": "Ask HN: How are you using AI coding tools?", "text": "I am currently using Claude Code as my daily driver for coding as an assistant where I plan and it codes.<p>But oftentimes, I am hearing people are doing much more with it:<p>* Multiple worktrees<p>* Parallel feature development<p>* Managing multiple Claude Code instances from mobile phone&#x2F;web and so on<p>What are the ways you are using it and how are you managing the context in your brain?<p>For me, I am focusing only on a single task and doing multiple iterations to rewrite my plan, review the output, revert the changes and start over with a new plan and so on. Can&#x27;t imagine how can I parallelize this process", "author": "throwaw12", "timestamp": "2026-01-06T10:57:24+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:11.748142+00:00", "processed": false}
{"id": "hn_comment_46510779", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46510779", "title": "Re: Noi: A workspace browser for parallel AI workflows...", "text": "I recently came across this interesting open-source project called Noi. It&#x27;s an interaction-first browser designed specifically for power users who juggle multiple AI services and web workspaces.<p>Key features that stand out:\n- Multi-window management: Run parallel workspaces side-by-side.\n- Session isolation: Supports multiple accounts on the same website through cookie data isolation.\n- Noi Ask: Send batch messages to multiple AI chats (ChatGPT, Claude, Gemini, etc.) simultaneously.\n- Prompt Management: Built-in system to organize and sync LLM prompts.\n- Local-first: All data like history and settings stay on your device.<p>It is developed by lencx, the creator of the popular community ChatGPT desktop app. It seems like a great evolution for anyone looking to reduce &quot;tab chaos&quot; while working with various AI models.<p>(Note: I am not the author of this project, just sharing a tool I found useful.)", "author": "handystudio", "timestamp": "2026-01-06T10:38:58+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-06T17:16:12.525071+00:00", "processed": false}
{"id": "hn_comment_46512007", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46512007", "title": "Re: We Deleted Our Vector Database...", "text": "the dependency-graph approach makes sense - and its actually why local CLI tools like Cursor, Copilot, Aider etc struggle with impact analysis. They&#x27;re context-window-constrained by design. Theres no persistent graph tracking what depends on what across repos, config files, call paths, etc. &quot;Just put the whole codebase in context&quot; doesnt really work here. You need something indexed before the LLM even gets involved - infra that already knows the dependency relationships. Local tools are great for &quot;write this function.&quot; But &quot;what breaks if I change this migration?&quot; is a totally different beast. Thats not a generation problem - its a graph query that needs server-side indexing to answer properly.", "author": "varKing", "timestamp": "2026-01-06T13:32:14+00:00", "score": null, "num_comments": null, "products": ["copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:12.627826+00:00", "processed": false}
{"id": "hn_comment_46510265", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46510265", "title": "Re: Show HN: Reticle \u2013 Debug MCP Tool Calls from Claud...", "text": "When agents call tools, debugging is weirdly blind: the client UI often hides the raw request&#x2F;response, errors get swallowed, and you can\u2019t correlate \u201cwhy did it do that?\u201d with the actual tool traffic.<p>Reticle is a local proxy + UI that shows the raw MCP JSON-RPC traffic (requests&#x2F;responses), correlates calls, and makes it easy to spot slow&#x2F;failing tools.<p>---<p>Try it: \nInstall:<p># npm\nnpm install -g mcp-reticle<p># pip\npip install mcp-reticle<p># homebrew\nbrew install labterminal&#x2F;tap&#x2F;mcp-reticle\nRun with a demo MCP server:<p># Start the dashboard\nmcp-reticle<p># In another terminal, wrap any MCP server\nmcp-reticle run --name demo -- npx -y @modelcontextprotocol&#x2F;server-filesystem &#x2F;tmp<p>---<p>What I&#x27;d love feedback on:\nWhich MCP clients should I support next? (Cline? Continue? OpenAI Agents SDK?)<p>Do you want &quot;replay tool call&quot; &#x2F; &quot;redaction rules&quot; &#x2F; &quot;session export&quot; first?<p>Anyone else frustrated by the lack of observability in this space, or am I yelling into the void?", "author": "labterminal", "timestamp": "2026-01-06T09:19:57+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-06T17:16:15.848443+00:00", "processed": false}
{"id": "hn_comment_46510666", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46510666", "title": "Re: Rust's Downfall: From Rising Star to Rejected by M...", "text": "&gt; While Rust is undeniably powerful and stands out for its emphasis on safety, it comes with a relatively steep learning curve.<p>I&#x27;ve got...   60,172 total lines of Rust code across all my repositories.<p>I didn&#x27;t write a single line of it. Claude writes Rust very well because of the Compile-Error-Edit loop.<p>That&#x27;s what&#x27;s going to define the winners in the future.", "author": "delaminator", "timestamp": "2026-01-06T10:21:28+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-06T17:16:16.609078+00:00", "processed": false}
