{"id": "hn_story_46566689", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46566689", "title": "Show HN: Agent-of-empires: opencode & claudecode session manager", "text": "Monitor the status of all your coding agents to understand which ones are waiting for your input. Written in rust and relies on tmux", "author": "river_otter", "timestamp": "2026-01-10T15:50:31+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:40.610041+00:00", "processed": false}
{"id": "hn_comment_46567137", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46567137", "title": "Re: I replaced Windows with Linux and everything's goi...", "text": "Commercial OSes (both Windows and MacOS) now feel so insanely agenda driven, and the agenda no longer feels like anything close to making the user happy and productive. For Mac, it feels like Apple wants to leverage what came out of VisionOS and unify the look and feel of mobile and desktop--two things no one asked for. For Windows, it feels like ads for their partners and ensuring they don&#x27;t fumble the ai&#x2F;agent transition the way they did with mobile.<p>Linux is SUCH a breath of fresh air. No one wants it to be anything other than what you want it to be. Modern desktop Linux has a much improved out of the box experience with good support for all the hardware I&#x27;ve thrown at it. And Claude Code makes it very fast and trivial to personalize, adapt, automate, etc.", "author": "toddmorey", "timestamp": "2026-01-10T16:36:44+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:41.760444+00:00", "processed": false}
{"id": "hn_story_46566160", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46566160", "title": "Show HN: Sigma Runtime \u2013 model-agnostic identity control for LLMs", "text": "We\u2019ve validated the Sigma Runtime architecture (v0.4.12) on Google Gemini-3 Flash, confirming that long-horizon identity control and stability can be achieved without retraining or fine-tuning the model.<p>The system maintains two distinct personas (\u201cFujiwara\u201d, a stoic Edo-period ronin, and \u201cJames\u201d, a formal British analyst) across 220 dialogue turns in stable equilibrium.\nThis shows that cognitive coherence and tone consistency can be controlled at runtime rather than in model weights.<p>Unlike LangChain or RAG frameworks that orchestrate prompts, Sigma Runtime treats the model as a dynamic field with measurable drift and equilibrium parameters.\nIt applies real-time feedback \u2014 injecting entropy or coherence corrections when needed \u2014 to maintain identity and prevent both drift and crystallization.\nThe effect is similar to RLHF-style fine-tuning, but done externally and vendor-agnostic.<p>This decouples application logic from any specific LLM provider.\nThe same runtime behavior has been validated on GPT-5.2 and Gemini-3, with Claude tests planned next.<p>We use narrative identities like \u201cFujiwara\u201d or \u201cJames\u201d because their linguistic styles make stability easy to verify by eye.\nIf the runtime can hold these for 100+ turns, it can maintain any structured identity or agent tone.<p>Runtime versions \u2265 v0.4 are proprietary,\nbut the architecture is open under the Sigma Runtime Standard (SRS):\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;sigmastratum&#x2F;documentation&#x2F;tree&#x2F;main&#x2F;srs\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;sigmastratum&#x2F;documentation&#x2F;tree&#x2F;main&#x2F;srs</a><p>A reproducible early version (SR-EI-037) is available here:\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;sigmastratum&#x2F;documentation&#x2F;tree&#x2F;bf473712ada5a9204a65434e46860b03d5fbf8fe&#x2F;sigma-runtime&#x2F;SR-EI-037&#x2F;code\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;sigmastratum&#x2F;documentation&#x2F;tree&#x2F;bf473712a...</a><p>Regulated under DOI: 10.5281&#x2F;zenodo.18085782 \u2014\nnon-commercial implementations are fully open.<p>HN discussion focus:\n\u2013 Runtime-level vs weight-level control\n\u2013 Model-agnostic identity stability\n\u2013 Feedback-based anti-crystallization\n\u2013 Can cognitive coherence be standardized?", "author": "teugent", "timestamp": "2026-01-10T14:50:45+00:00", "score": 2, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-10T17:09:43.474097+00:00", "processed": false}
{"id": "hn_story_46566128", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46566128", "title": "Show HN: Revibing nanochat's inference model in C++ with ggml", "text": "Recently I wanted to see if I could vibe some serious C++ code.<p>The result is a C++ re-implementation of Andrej Karpathy&#x27;s nanochat&#x27;s inferencing part (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nanochat\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nanochat</a>), built on top of ggml. Unlike llama.cpp, this isn&#x27;t a standalone binary; it is a C++ library &amp; Python wrapper designed to swap out some core classes within the nanochat pipeline. For playability, I\u2019ve tried to keep the dependencies to a minimum: just ggml, nanobind, and gtest for unit tests.<p>Features and limitations:<p>- A drop-in replacement of nanochat\u2019s `GPT` and `KVCache` classes. So far I\u2019ve only tested this with `chat_web.py`. You can see how it&#x27;s integrated here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;k-ye&#x2F;nanochat&#x2F;pull&#x2F;1\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;k-ye&#x2F;nanochat&#x2F;pull&#x2F;1</a><p>- Supports CPU and GPU (Metal yes, CUDA probably?).<p>- Handles PyTorch-to-GGUF conversion automatically on demand.<p>- Only float32 is currently supported.<p>Benchmark:<p>On an M3 Max (Metal), throughput is roughly 1&#x2F;3 that of the original PyTorch implementation. I haven\u2019t profiled the code yet, but I suspect the bottleneck is the lack of bf16 support.<p>Motivation<p>- Writing meaningful (&amp; fun) C++ again: I used to spend a lot of my day-to-day time in C++ while working at various tech companies. These days, opportunities to use it for personal projects are rare, as it\u2019s often hard to find a use case where C++&#x27;s advantages truly matter.<p>- Testing &quot;Vibe Coding&quot; capabilities: Most of my current work is in UE5. Ironically, Blueprints\u2014which were designed to help non-coders\u2014have become a bottleneck in the LLM era... Admittedly, the AI agent has generated some FOMO in me, and I wanted to see if AI could handle a lower-level C++ implementation of a complex system from scratch.<p>- Understanding the LLM internals.<p>Why nanochat?<p>It hits the &quot;Goldilocks&quot; zone: popular enough to be relevant, concise enough to be educational, and practical enough to deserve a serious C++ implementation.<p>If you\u2019re like me \u2014 an infra guy from the old days who feels a bit threatened by LLM and&#x2F;or AI coding \u2014 I think nanochat is a great reference. Tinkering with it however you like is a nice way to demystify the tech. I relied heavily on Claude Code (CC) for the implementation. Overall, I am both impressed and genuinely pleased with the experience.<p>Happy to answer questions, hear feedback or further discuss AI coding!", "author": "makechan", "timestamp": "2026-01-10T14:46:46+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-10T17:09:43.538762+00:00", "processed": false}
{"id": "hn_story_46566103", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46566103", "title": "Have you ever been able to code in the first place?", "text": "Have you ever been able to code in the first place?<p>It started like this. I programmed in Pascal when I was in school. I learned basic. I learned assembler. I literally assembled code. Sometimes I thought it out, planned it, and created for that time quite elaborate code, controlling a panoramic head, for instance, things like that. So at this time, I was really proud of myself. I was good at this. I was getting better.<p>And fast forward to today, you have AI and vibe coding. Back in the day, often when I got an error message, I just looked on the internet and tried something out, did trial and error. After a while, it worked either with my own code or with code from a GitHub repository.<p>Today I don&#x27;t even do that. All I do is I start the Gemini CLI and have it code for me. And then I watch some video unless it has some issues. I noticed that the code is better and it all comes down to good description of the problem.<p>I noticed the problems that I fought with for ages, that I spent hours with debugging and shit. It just created in 15 minutes. So I was wondering, was I even ever able to code? Or was it just a huge waste of time?<p>Because now I can see it from the outside and see the amount of time that I would have used to debug this and to write it in the first place, then copy paste it from other sources, which might be outdated. I would have done something for five days.<p>Partly I would be excited about this. Yes, sure, it&#x27;s challenging your brain, but ultimately you have something and you&#x27;re probably not even that excited about it.<p>Have you ever been really able to code? Or was it more like trying something, it doesn&#x27;t really work, you ask in a forum like Stack Overflow, and if that still doesn&#x27;t work, you just do something else?<p>I had Pascal in school, later at university Java for the first time. I programmed a little afterwards for Android, given that it was also Java. But often it was Stack Overflow here, Stack Overflow there, trying to match my source code with what I found online, looking at manuals or whatever. It was tedious. Just tedious.<p>Ultimately I had something that was fun for five minutes at best. Often I just wanted to see if I liked something, but for that I had to create a prototype. And this prototype didn\u2019t work. It didn\u2019t compile.<p>I saw some repository, wanted it to work, then there was a compilation error. This module was lacking. This dependency wasn\u2019t there. All of this shit.<p>So I\u2019m coming back to it. Was I ever able to code? Or what is the benchmark for coding? And is it really a disservice to my intellect if I stop doing that and just have AI create it?<p>There are changes all the time. Repositories are not up to date. Libraries don\u2019t work with each other. One updates, the other doesn\u2019t. Then you have to fix it. It\u2019s depressing and annoying, and I don\u2019t really see the drawback of doing it differently now.<p>If a repository doesn\u2019t do what I want, I load it, start Gemini CLI, and have it corrected. Reverse engineering protocols or hardware, in my experience, is disgusting. Really hard and frustrating.<p>So what is your take on the whole situation?", "author": "Haeuserschlucht", "timestamp": "2026-01-10T14:42:20+00:00", "score": 2, "num_comments": 2, "products": ["gemini"], "categories": ["error_messages", "onboarding"], "sentiment": null, "collected_at": "2026-01-10T17:09:43.702747+00:00", "processed": false}
{"id": "hn_comment_46565770", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46565770", "title": "Re: Six New Tips for Better Coding with Agents...", "text": "\u201c 1. Software is now throwaway \u2014 expect &lt; 1 year shelf life\u201d<p>I\u2019m trying to understand this one and it doesn\u2019t really make sense to me.  Or maybe \u2014-<p>What is software?  How do you delete software and start over.  I think we\u2019re differing on our definitions of what software is.<p>If you have a clear spec, then maybe we have like, immutable software, like \u2014- you don\u2019t upgrade dependencies, because you can recreate software from spec.  But then to me, software is the spec not the code.  Like, do you just etch a sketch your product every year going back to customers to figure out how to rebuild from scratch?  I don\u2019t think that\u2019s what\u2019s being suggested.<p>I suspect what\u2019s being suggested, is that if you are able to codify all the business logic, edge cases, and optimizations, then you can generate software from that.  But to me that\u2019s what software is, not the code.<p>But if you don\u2019t have a clear spec?  Isn\u2019t one of the reasons you don\u2019t rewrite legacy mainframe software because the spec is unclear?  You don\u2019t want software that recreates that old software, you want software that makes new assumptions.<p>Also - I guess - it seems like we\u2019re assuming that Claude writes perfectly optimized code, is that right?<p>But then, why 1 year shelf-life?  Why are you keeping software around for a year?  Why not regenerate for every deploy?  One year seems like this weird middle ground.<p>Take I dunno, the OpenSSL library, are you having researchers re-imagining the encryption algorithms every year?<p>There\u2019s something about the way people talk about new paradigms that seems in bananas opposition to software engineering best practices of the past ten years.  But like BANANAS opposition.<p>And I feel like people think I\u2019m being a negative Nancy, but nothing in this doc feels like a realistic path to the 100x everyone wants, and I want the 100x gains!  Do people forget that when the rubber hits the road this software has to run in a hostile environment?<p>The solution in this doc seems to be; there will be 100 software companies run by the 100 180 IQ engineers in the world.  Do we not care about the bus factor?", "author": "techblueberry", "timestamp": "2026-01-10T13:57:12+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["content_clarity"], "sentiment": null, "collected_at": "2026-01-10T17:09:48.619435+00:00", "processed": false}
{"id": "hn_comment_46564971", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46564971", "title": "Re: Complete developer tutorial: Building AI agent UIs...", "text": "A comprehensive developer tutorial covering A2UI \u2013 a declarative protocol for AI agents \nto generate native UIs via JSON messages.<p>Technical highlights:\n- Adjacency list model (flat component list with ID refs) instead of nested trees \u2013 \n  designed for LLM streaming and incremental generation\n- Data binding via JSON Pointer paths (RFC 6901) for reactive updates without component \n  regeneration\n- Three-layer architecture: UI structure (surfaceUpdate), application state \n  (dataModelUpdate), client rendering\n- Transport-agnostic: works with A2A Protocol, SSE, WebSockets, or AG UI<p>The tutorial includes:\n- Step-by-step agent setup with Python ADK (code examples included)\n- Client implementation guides for Lit, Angular, and Flutter renderers\n- Message processing &amp; state management implementation checklist\n- Custom component catalog creation\n- Error handling and validation patterns<p>Production use: Google&#x27;s Opal, Gemini Enterprise, Flutter GenUI SDK. \nReact renderer coming Q1 2026. Full spec and samples on GitHub.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;a2ui\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;a2ui</a>", "author": "czmilo", "timestamp": "2026-01-10T11:56:08+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:50.145371+00:00", "processed": false}
{"id": "hn_comment_46564766", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46564766", "title": "Re: OpenAI/Codex now in OpenCode v1.1.11 after the Ant...", "text": "<a href=\"https:&#x2F;&#x2F;xcancel.com&#x2F;thsottiaux&#x2F;status&#x2F;2009876590789046315\" rel=\"nofollow\">https:&#x2F;&#x2F;xcancel.com&#x2F;thsottiaux&#x2F;status&#x2F;2009876590789046315</a><p>OpenAI jumped to take advantage of the Claude debacle. They helped OpenCode to integrate it and are updating ToS.<p>Yesterday: &quot;Anthropic blocks third-party use of Claude Code subscriptions&quot; <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46549823\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46549823</a>", "author": "alecco", "timestamp": "2026-01-10T11:24:24+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:51.398902+00:00", "processed": false}
{"id": "hn_comment_46566325", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46566325", "title": "Re: Org Mode Syntax Is One of the Most Reasonable Mark...", "text": "I&#x27;ve recently begun replacing Markdown with Gemini&#x27;s .gmi&#x2F;gemtext format. It is Markdown with fewer features. I appreciate the simplicity and it&#x27;s tremendously easy for custom tools to parse.<p>It has no inline formatting, only 3 levels of ATX headers (without trailing #s), one level of bullet points using only asterisk and not dash to delimit, does not merge touching non-whitespace lines (thus expecting one line per paragraph), and supports only triple-backtick fenced preformatted text areas that just flip on and off.<p>Maybe the biggest change is that links are necessarily listed on their own line, proceeded by a `=&gt;` and optionally followed by alt-text.<p>My gemtext parser is maybe 70 lines and it is arguably 95% of what one needs from Markdown.", "author": "tel", "timestamp": "2026-01-10T15:10:50+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:55.616373+00:00", "processed": false}
{"id": "hn_comment_46563923", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46563923", "title": "Re: FFmpeg 8.0...", "text": "Found out that FFmpeg is now somewhat broken.<p>&quot;I thought about improving old video-8 by  discarding fuzzy frames and interpolating between sharp ones. Can ffmpeg do that?&quot;<p>Gemini produced this:<p>ffmpeg -i input_video8.mp4 -vf &quot;\n  blurdetect=block_width=32:block_height=32,\n  select=&#x27;gt(metadata(lavfi.blurdetect.blur), 0.15)&#x27;,\n  setpts=N&#x2F;FRAME_RATE&#x2F;TB,\n  minterpolate=fps=30:mi_mode=mci:mc_mode=aobmc&quot; \n-c:v libx264 -crf 18 output_clean.mp4<p>After couple of hours back-and-forth Gemini gave up. Syntax problems seemed unsolvable. Something to do how different layers of FFmpeg interpret quote-characters?<p>After couple of more hours Grok solved the problem: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;timonoko&#x2F;Skipperi\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;timonoko&#x2F;Skipperi</a><p>Unfortunately results in improving shitty old videos were not too impressive. :-&#x2F;", "author": "timonoko", "timestamp": "2026-01-10T08:35:41+00:00", "score": null, "num_comments": null, "products": ["gemini", "grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-10T17:09:57.282863+00:00", "processed": false}
{"id": "hn_story_46563672", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46563672", "title": "Show HN: Let your Claude Code message you on Telegram when it needs decisions", "text": "I\u2019ve been running longer AI agent tasks (mostly in Claude Code), and I kept running into the same problem:\nthe agent would finish or get stuck asking a question, and I wouldn\u2019t notice until much later because I wasn\u2019t watching the terminal.<p>So I built a small tool called Agent Reachout.<p>It lets an AI agent send me messages on Telegram when:\n \u2022 it finishes a task\n \u2022 it hits a blocker\n \u2022 it needs a human decision to continue<p>I can reply directly from Telegram, and the agent continues where it left off.<p>This turned long-running agent work into something asynchronous \u2014 I don\u2019t have to babysit the CLI anymore.<p>What it does\n \u2022 Simple Telegram bot integration\n \u2022 One-way notifications or two-way conversations\n \u2022 Designed for \u201chuman-in-the-loop\u201d agent workflows\n \u2022 Works today as a Claude Code plugin<p>Why I built it\nFully autonomous agents sound nice, but in practice I often want:\n \u2022 approvals before destructive actions\n \u2022 clarification on ambiguous decisions\n \u2022 a quick \u201cyes&#x2F;no\u201d without stopping my day<p>Telegram was already where I am, so I used that.<p>What it\u2019s not\n \u2022 Not a general chatbot framework\n \u2022 Not a workflow engine\n \u2022 Just a small bridge between agents and humans<p>Repo\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;vibe-with-me-tools&#x2F;agent-reachout\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vibe-with-me-tools&#x2F;agent-reachout</a><p>Would love feedback on:\n \u2022 whether others hit this problem\n \u2022 what notification channels would be useful next (Slack, WhatsApp, etc.)\n \u2022 whether this should stay a plugin or evolve into something broader<p>Thanks!", "author": "brainsofbots", "timestamp": "2026-01-10T07:42:53+00:00", "score": 1, "num_comments": 1, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:57.315857+00:00", "processed": false}
{"id": "hn_story_46563650", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46563650", "title": "Cursor vs. Claude Code: parallel vs. focus, not code quality", "text": "I\u2019ve been using Cursor and Claude Code daily for real work, not just experiments.<p>One thing that surprised me is how quickly code quality converges between tools once you plan clearly. At this point, I don\u2019t feel a meaningful difference in output quality itself.<p>What does feel different is the workflow mode each tool supports.<p>When I want many things moving at once, spawning parallel agents, delegating background tasks, or running async work, Claude Code feels more natural to me. The CLI and agent-first model fits that style well.<p>When I need to slow down, review plans, read diffs, understand context, and make careful changes, Cursor feels more friendly. It\u2019s easier for focused thinking and sense-making.<p>So for me, it\u2019s parallel vs focus mode.<p>We\u2019re also starting to run Claude Code in CI&#x2F;CD for well-scoped tasks like tests, refactors, and reproducible bug fixes. That background delegation is where CLI-first tools start to matter.<p>Curious how others are splitting work between these tools, or if you see it differently.", "author": "hoangnnguyen", "timestamp": "2026-01-10T07:38:12+00:00", "score": 1, "num_comments": 2, "products": ["claude"], "categories": ["naming_terminology", "response_quality"], "sentiment": null, "collected_at": "2026-01-10T17:09:57.576794+00:00", "processed": false}
{"id": "hn_comment_46563616", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46563616", "title": "Re: ChatGPT browser extension that turns your account ...", "text": "I have been working on building projects using my ChatGPT account. However, there is currently no free tier for the ChatGPT API, and the paid plans can be quite expensive, especially for testing purposes.\nTo address this, I developed a browser extension that converts your ChatGPT account into an API-like interface. This allows you to use ChatGPT programmatically at no cost.\nThe project is fully open source, and contributions are welcome. Feel free to submit issues, suggestions, or improvements.", "author": "Nitesh17", "timestamp": "2026-01-10T07:28:58+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-10T17:09:57.968875+00:00", "processed": false}
{"id": "hn_comment_46562827", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46562827", "title": "Re: Best Practices for Coding with Agents...", "text": "I&#x27;ve cursor and vscode both installed but i use vscode with github copilot since its cheaper. debug mode in cursor sounds cool. vscode should just replace the &quot;edit&quot; function with debug mode. Does anyone even use &quot;edit&quot; mode?", "author": "anishgupta", "timestamp": "2026-01-10T04:39:21+00:00", "score": null, "num_comments": null, "products": ["copilot"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-10T17:10:03.217163+00:00", "processed": false}
{"id": "hn_comment_46562238", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46562238", "title": "Re: Scaffold \u2013 Add AI features to any site, no API key...", "text": "I built Scaffold to solve a problem I kept hitting: I wanted to embed AI features (chatbots, content generators) in my projects, but didn&#x27;t want to manage API keys, build backends, or pay per request.<p>The approach: Instead of calling OpenAI&#x27;s API, Scaffold generates optimized prompts that users send to ChatGPT themselves. You build a form with custom fields, write a prompt template using {{variables}}, and your users get a perfectly formatted ChatGPT prompt.<p>Why this works:\n- Zero infrastructure (no backend needed)\n- Zero API costs (free forever)\n- Works anywhere (iframe embed on any site)\n- Users control the AI interaction<p>What it can&#x27;t do:\n- Conversational AI with memory (each submission is a new prompt)\n- Embedded responses (opens in ChatGPT, not your site)\n- Server-side processing (no programmatic access to responses)<p>Tech stack: Next.js 14, Supabase (auth + PostgreSQL), Vercel<p>Use cases:\n- Blog post generators\n- Email writers\n- Study tutors with specific knowledge (via &quot;fixed fields&quot;)\n- Product finders<p>Try it: scaffoldtool.com<p>I&#x27;m 17 and this is my first real launch. Looking for technical feedback - what would make this a no-brainer for you?", "author": "niqhtcrawler", "timestamp": "2026-01-10T02:43:20+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-10T17:10:05.654632+00:00", "processed": false}
