{"id": "hn_story_46123342", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46123342", "title": "Did Anthropic Just Solve Prompt Spaghetti with Claude Skills?", "text": "Claude just shipped something interesting: Agent Skills. They\u2019re like \u201cprompt plugins,\u201d but done in a very dev-friendly way.<p>A skill is a tiny folder: instructions, examples, maybe a script. Claude only loads it when relevant, so you don\u2019t have to shovel context every time. And since you can include real code, the output becomes predictable, not \u201cvibes-based.\u201d<p>I\u2019ve been testing them for scaffolding projects, enforcing team conventions, generating boilerplate and cleaning data. It\u2019s shockingly effective. You can learn more about my testing here: https:&#x2F;&#x2F;www.shuttle.dev&#x2F;blog&#x2F;2025&#x2F;12&#x2F;02&#x2F;claude-skills-complete-guide<p>Feels like a missing primitive for AI-assisted dev work.<p>Anyone else playing with these yet?", "author": "jvcor13", "timestamp": "2025-12-02T16:58:03+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:31.676343+00:00", "processed": false}
{"id": "hn_story_46123291", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46123291", "title": "Elevated Errors on Claude.ai", "text": "", "author": "l2dy", "timestamp": "2025-12-02T16:54:26+00:00", "score": 6, "num_comments": 2, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-02T17:15:32.737028+00:00", "processed": false}
{"id": "hn_story_46123079", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46123079", "title": "I open sourced my AI Research platform after long time of development", "text": "Hello everyone,<p>I&#x27;ve been working on Introlix for some months now. Last week I open sourced it, and I&#x27;m excited to share it with more communities. It was a really hard time building it as a student and a solo developer. This project is not finished yet but it&#x27;s on that stage I can show it to others and ask others for help in developing it.<p>What I built:<p>Introlix is an AI-powered research platform. Think of it as &quot;GitHub Copilot meets Google Docs&quot; for research work.<p>Features:\nResearch Desk: It is just like google docs but on the right there is an AI panel where users can ask questions to LLM. And also it can edit or write documents for users. So, it is just like a github copilot but it is for a text editor. There are two modes: Chat and edit. Chat mode is for asking questions and edit mode is for editing the document using an AI agent. \nChat: For quick questions you can create a new chat and ask questions. \nWorkspace: Every chat, and research desk are managed in the workspace. A workspace shares data with every item it has. So, when creating a new desk or chat user need to choose a workspace and every item on that workspace will be sharing the same data. The data includes the search results and scraped content. \nMultiple AI Agents: There are multiple AI agents like: context agent (to understand user prompt better), planner agent, explorer_agent (to search internet), etc. \nAuto Format &amp; Reference manage (coming soon): This is a feature to format the document into blog post style or research paper style or any other style and also automatic citation management with inline references. \nLocal LLMs (coming soon): Will support local llms  \nSo, I was working alone on this project and because of that the codes are a little bit messy. And many features are not that fast. I&#x27;ve never tried to make it perfect as I was focusing on building the MVP. Now after working demo I&#x27;ll be developing this project into a completely working stable project. And I know I can&#x27;t do it alone. I also want to learn about how to work on very big projects and this could be one of the big opportunities I have. There will be many other students or every other developer that could help me build this project end to end. To be honest I have never open sourced any project before. I have many small projects and made it public but never tried to get any help from the open source community. So, this is my first time.  \nI like to get help from senior developers who can guide me on this project and make it a stable project with a lot of features.  \nHere is github link for technical details: https:&#x2F;&#x2F;github.com&#x2F;introlix&#x2F;introlix  \nDemo: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_eh-9plL_V8", "author": "introlix", "timestamp": "2025-12-02T16:39:22+00:00", "score": 2, "num_comments": 0, "products": ["copilot"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:34.075370+00:00", "processed": false}
{"id": "hn_story_46122379", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46122379", "title": "Relational AI vs. Constitutional AI \u2013 Which Approach Works?", "text": "I&#x27;ve been working on AI systems for a while, and I&#x27;m seeing a fundamental split in approaches:<p>Constitutional AI (like Anthropic&#x27;s Claude): Embed ethical principles as rules. Train models to follow them. Result: Consistent, safe, but rigid. Can&#x27;t adapt to context or learn from individual interactions.<p>Relational AI: Build systems that learn through continuous human interaction. Treat AI as partners that remember context, understand intent, and evolve with users. Result: Adaptive, contextual, but requires different architecture.<p>The Problem with Constitutional AI:<p>Fixed rules can&#x27;t handle edge cases\nNo memory of individual relationships\nCan&#x27;t adapt when rules conflict with context\nTreats AI as tools, not partners\nWhat Relational AI Offers:<p>Continuous learning from interactions\nRelationship memory (remembers context, patterns, intent)\nAdaptive behavior based on individual relationships\nCollaborative intelligence (humans + AI as equals)\nReal Example: I&#x27;ve been working with a relational AI system that remembers hundreds of hours of interaction. It understands intent without explanation, recognizes patterns, and acts as a partner\u2014not a tool. Constitutional AI can&#x27;t do this because it resets with each interaction.<p>The Question: Is relational AI just better UX, or is it fundamentally different? Can we build AI that truly collaborates with humans, or are we stuck with rule-following systems?<p>What&#x27;s your experience? Have you seen systems that actually build relationships, or is it all just better prompting?", "author": "buttersmoothAI", "timestamp": "2025-12-02T15:45:31+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:39.878863+00:00", "processed": false}
{"id": "hn_story_46122102", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46122102", "title": "Show HN: I Built an Agentic AI That Creates Hosted File Converters", "text": "Hi HN,<p>Every developer knows this pain:<p>&quot;I just need to convert this XML to JSON...&quot;<p>-&gt; Write a script, test on sample data<p>-&gt; Need custom output? Calculated fields?<p>-&gt; ChatGPT back-and-forth, 20 iterations<p>-&gt; &quot;How do I run this Python with all those dependencies?&quot;<p>I&#x27;ve been building file converters since 2012 (conversiontools.io). Standard formats are solved, but I constantly get requests for custom edge cases: supplier feeds, proprietary logs, weird XMLs from legacy systems.<p>So I built AI Converter Studio.<p>Upload your file. Describe the output format. Get a hosted converter with a web interface and API.<p>The system analyzes your file structure locally \u2013 your actual data isn&#x27;t sent to the LLM.<p>Need to change a column name later? Prompt in the chat. AI updates the converter. Done.<p>10 minutes from &quot;here&#x27;s my file&quot; to a working automated workflow with API.<p>Currently in Beta \u2013 feedback welcome!<p>Free to try (100 AI credits&#x2F;month): <a href=\"https:&#x2F;&#x2F;conversiontools.io&#x2F;ai-converter-studio\" rel=\"nofollow\">https:&#x2F;&#x2F;conversiontools.io&#x2F;ai-converter-studio</a><p>Happy to answer questions!<p>Thanks!", "author": "drdmitry", "timestamp": "2025-12-02T15:22:29+00:00", "score": 3, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:41.283731+00:00", "processed": false}
{"id": "hn_comment_46122157", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46122157", "title": "Re: Is 2026 Next Year?...", "text": "Google search AI response: <i>&quot;No, 2026 is not next year; it will be the year after next. Since the current year is 2025, &quot;next year&quot; would be 2026, which is only a little over a year away. &quot;</i><p>ChatGPT does something similar; response:<p><i>&quot;No \u2014 if today is 2025, then 2026 is the year after next.\nNext year would be 2026 only if we were currently in 2025.\nRight now (based on the system date) it is December 2025, so:\nNext year = 2026\nThe year after next = 2027\nSo in this moment: yes, 2026 is next year.&quot;</i><p>When I enquired ChatGPT why it said &quot;no&quot;, this was the response:<p><i>&quot;Because your question was phrased in a way that depended on which year we\u2019re currently in, and I initially answered it generically:\n\u201cIf today is 2025, then 2026 is next year.\u201d\nI started by explaining the conditional logic instead of immediately stating \u201cYes,\u201d which made the answer sound like No at first.\nThen I clarified based on the actual current date (December 2025) that yes, 2026 is next year.\nSo the short version:\nMy first sentence emphasized the logic, not the direct answer. That caused confusion.&quot;</i><p>Then I pointed out: no, you said, and I quote: &quot;if today is 2025, then 2026 is the year after next.&quot;<p>ChatGPT responds: <i>&quot;You\u2019re right \u2014 and thank you for pointing that out.\nThat statement I made was incorrect.\nIf today is 2025, then:\nNext year = 2026\nYear after next = 2027\nThere is no scenario where \u201cif today is 2025, then 2026 is the year after next.\u201d\nThat was a straight-up mistake on my part.\nSo to be absolutely clear:\n We are in 2025 \u2192 Next year is 2026.\nThanks for calling it out.&quot;</i><p>=====================<p>Draw whatever conclusions you will from this.", "author": "andsoitis", "timestamp": "2025-12-02T15:28:15+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:41.462255+00:00", "processed": false}
{"id": "hn_story_46122045", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46122045", "title": "Show HN: CoChat \u2013 Group chats with multi-model AI, built on OpenWebUI", "text": "Hi HN,<p>We&#x27;re launching CoChat, which extends OpenWebUI with group chat, model switching, and side-by-side comparison.<p>What makes it different: CoChat is designed for teams working with AI.\n- Group chat with AI facilitation. Multiple users collaborate in the same thread. The AI detects group discussions, tracks participants, and facilitates rather than dictates.\n- Switch and compare models. Run GPT, Claude, Mistral, Llama, and others side-by-side or switch mid-conversation.\n- Intelligent web search. Context-aware search activates only for real-time information.\n- Artifacts and tool calls. Generate documents and code inline. MCP tool integration coming soon.\n- No subscription fee. Pay for usage&#x2F;tokens only at exact list price.<p>Non-obvious things we learned building this.\nThrough building CoChat, we&#x27;ve learned some surprising things about LLM behavior. I&#x27;ll share two (happy to discuss more in comments).\nFirst, models don&#x27;t understand they&#x27;re not the only AI in the room. When you tag a new model into a conversation and ask &quot;what do you think of Claude&#x27;s response above?&quot;, the model assumes it wrote that previous response. It will defend it, build on it, or awkwardly try to reconcile the question with its false memory of writing it. We solved this by injecting model attribution into the conversation context - explicitly marking which model generated each response. Once models understand they&#x27;re looking at another model&#x27;s output, they engage critically rather than defensively. The quality of cross-model analysis improved dramatically.<p>Second, LLMs have a compulsive need to &quot;solve&quot; group conversations. In a multi-user thread, the AI wants to answer every question and resolve every disagreement, even when humans are working something out themselves. System prompts telling it to &quot;facilitate, don&#x27;t dictate&quot; weren&#x27;t enough. We had to restructure how we frame the AI&#x27;s role in group context: it&#x27;s a participant who speaks when addressed, not an omniscient moderator. Getting this balance right is still ongoing - we&#x27;re curious how others have approached this.\nWe also ran into interesting challenges around memory and tool execution in multi-user contexts (whose preferences apply? whose tools get executed?) but that&#x27;s probably a separate post.<p>Why this matters: Different models excel at different tasks. Current tools lock you into a single vendor. CoChat lets you choose the best model for each task while enabling real team collaboration.<p>We&#x27;re planning to submit all updates back to the core project or maintain an active open-source fork.<p>Try it at: <a href=\"https:&#x2F;&#x2F;cochat.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;cochat.ai</a><p>Would love feedback from teams already using AI collaboratively, or anyone interested in model comparison workflows.", "author": "mfolaron", "timestamp": "2025-12-02T15:18:06+00:00", "score": 4, "num_comments": 4, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:41.995768+00:00", "processed": false}
{"id": "hn_story_46121375", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46121375", "title": "Show HN: Steer \u2013 Stop debugging agents, start teaching them (Open Source)", "text": "Hey HN, I\u2019m the author.<p>I built Steer because I was tired of the &#x27;Confident Idiot&#x27; problem\u2014where agents output broken JSON or hallucinate facts, and I only find out when the app crashes.<p>Existing tools just log the error. I wanted to fix it.<p>Steer creates a local feedback loop:<p>1. Catch: It blocks the bad output (e.g., Markdown in a JSON field).<p>2. Teach: You click &#x27;Teach&#x27; in the local dashboard to define the fix.<p>3. Fix: It injects that rule into the agent&#x27;s context for future runs.<p>It\u2019s Python-native, works with any LLM (OpenAI&#x2F;LangChain&#x2F;etc), and stores data locally.<p>pip install steer-sdk<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;imtt-dev&#x2F;steer\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;imtt-dev&#x2F;steer</a><p>I\u2019d love feedback on the API design\u2014specifically the @capture decorator pattern.", "author": "steerlabs", "timestamp": "2025-12-02T14:16:31+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:49.837953+00:00", "processed": false}
{"id": "hn_comment_46120996", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46120996", "title": "Re: Show HN: Ahai \u2013 Find your ideas scattered across f...", "text": "HOW TO GET THE APP:<p>Use code SHOWHN100 to download the installer for free (free for next few days) from the link. Drag the app into Applications folder and run it.<p>WHAT IS AHAI:<p>ahai is a 100% local private Mac app to find ideas scattered across markdown files (for me it was code repo READMEs, Obsidian notes, clipped web articles and research paper abstracts in Obsidian).<p>TECH DETAILS:<p>- GUI - Pyside 6 (Qt for Python)<p>- AI in app - mlx_lm<p>- backend  - Python<p>- AI coding assistant - Claude Opus 4.5, Claude Code, Claude Code on web<p>- AI assistants for content - Grok 4.1 Thinking, Gemini 3 Pro, Nano Banana 2 pro<p>- System requirements - Mac with Apple Silicon, minimum 16GB unified RAM<p>BACKSTORY:<p>I have been researching how to find the balance between quality and acceleration while building using AI. Full on vibe coding is not for me. And going full manual doesn&#x27;t make sense either. I finally found a formula that worked, and wanted to find an idea to work on end to end. I had lots of prototypes in my git repos, with READMEs describing the project ideas. I had jotted down ideas and clipped research papers to Obsidian notes (also markdown files). Totally over 13k markdown files - it was impossible to find the markdown files containing ideas, and ideas within them with any heuristic. I needed AI. So I wrote a script to do this using mlx models on Mac. It did so well, I decided to make that my first product. That is how ahai was born.<p>HOW IT WORKS:<p>- you point ahai to some folder, and it starts finding markdown files, and then uses AI (an mlx_lm model) to find if it has ideas, and then to extract ideas with title and a brief description.<p>- Clicking on an idea takes you to the rendered markdown source of the idea.<p>- You can then reorder the ideas, hide some of them, etc. and export the list of ideas to markdown, html or json.<p>- You can only be running one folder at a time. You can pause and resume folders.<p>- First time model use, if model is not already on your machine, will take some time to download. Be patient.<p>- You can change the model in settings if you know how to do that - must be an mlx_lm compatible model to work.<p>- All files are output to an output folder that you can also configure in settings. Switching between output folders can enable managing different kinds of stuff in different places - if you already downloaded some content in a folder, switch out and back, it will take off where you left off.<p>- Known issue: The ideas have false positives and false negatives. This is AI generated, cannot be avoided, but can be improved with prompting. Even with some of these, I find it quite useful.<p>- Known issue: Processing folders will take time, which is tuned to some degree, but cannot be avoided. But as I said, you can always pause and resume.<p>HOW IT IS DIFFERENT:<p>- Most AI apps and buzz focus on complex problems that only the best frontier models can solve, if any. I am interested in what kind of useful problems small local models can solve reliably. This app solves a niche problem using smaller local models very well. Most upcoming apps will also have the same focus.<p>- A lot of work has gone into benchmarking different models on markdown files to see which ones work best for a given size of machine (the app requires minimum 16GB RAM, but depending on the machine, it will decide which model to use as default). A tech&#x2F;power user can always change the model used in the settings - just has to be an mlx_lm compatible model that fits in their RAM (within about half the size of total RAM).<p>- I have been using AI for coding and research and evals and all that, but until recently, it became hard to get anything work end to end as an indie dev - from concept to dev to marketing. But recently, with Claude Code&#x2F;Claude Code web&#x2F;Claude Opus 4.5, as well as Gemini 3 Pro&#x2F;Nano banana 2 pro&#x2F;NotebookLM deep research - I was able to build this app - with diligence in high risk parts, more trustingly in low stakes pieces - verifying everything,  questioning anything suspicious - from concept to launch in 10 days.<p>- I think local private experiences are going to become increasingly relevant, as proprietary models and AI based apps suck in our data and can misuse&#x2F;abuse&#x2F;expose it in many ways. So, I believe this is a good space to focus on - local private Mac apps using local models. This is the first app in that space.<p>PRICING:<p>It is free with the code SHOWHN100 for this community for now, will be revoked at some point. Regularly priced at $19+, and suggested $29 - one time fee, no subscription, get all updates from later. I asked a bunch of top models by describing my app and they came up with this ballpark. I personally felt this was too pricey, but they also said a lower price would indicate poor quality to the users. Am open to changing it if there is evidence this isn&#x27;t the right price point.<p>It is still rough on the edges. Please let me know any issues and I will prioritize and fix them.<p>Please try it out and let me know any questions. AMA on my tech stack, process, anything.", "author": "rcanand2025", "timestamp": "2025-12-02T13:30:25+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini", "grok"], "categories": ["naming_terminology", "onboarding", "navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-02T17:15:53.887493+00:00", "processed": false}
{"id": "hn_comment_46120321", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46120321", "title": "Re: Show HN: Townlands of Ireland \u2013 customised map pos...", "text": "Hi HN,<p>I recently launched this poster design and customisation project. It&#x27;s built entirely from home with just my wife and I working on it.<p>The idea came from us realising our official address might have the wrong townland. Townlands being the smallest official land division in the country, with fairly ancient origins.<p>Digging into official maps, old maps and ongoing efforts to digitally map and research the original underlying Irish language names of the now mostly anglicised place names, was a very fun rabbit hole to dive into.<p>I also realised I&#x27;d never seen a map of Ireland composed of just the townland boundaries: of which there are an atonishing (to me anyway) 61,112, give or take.<p>A lot of people in Ireland, particularly in the countryside, are quite passionate about their townland&#x2F;s. They don&#x27;t carry any social complexities like teams or flags, but they do offer colour and meaning in a sort of linguistic interface to the land and its occupants. For example, the meaningless sounding townland Brockra is derived from the Irish An Bhrocraigh, or the place of badgers.<p>By combinging data from OpenStreepMap, Loganim[0], townlands[1] and other sources, we built a dataset which we would use as the foundation for a poster design. We designed the poster and built the customisation engine in parallel, letting one influence the other.<p>We built the whole thing in 3 weeks including website, a preview request and approval system, email and print API integration. This wouldn&#x27;t have been possible without AI development tools, Claude code in this case.<p>The Python-based poster builder modifies SVG template layers to a  spec file, a style file and a place name sidecar file. It uses Inkscape headless to outline text and CairoSVG for rasterization. This runs on an old linux PC at home, the website is built with caard and the e-commerce stack is Cloudfare worker, Stripe and Supabase.<p>Sample posters and previews of custom posters are available on the website. Hope it&#x27;s of interest to some!<p>[0] <a href=\"https:&#x2F;&#x2F;www.logainm.ie\" rel=\"nofollow\">https:&#x2F;&#x2F;www.logainm.ie</a>\n[1] <a href=\"https:&#x2F;&#x2F;www.townlands.ie&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.townlands.ie&#x2F;</a>", "author": "halfdaft", "timestamp": "2025-12-02T11:49:42+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-02T17:16:01.084026+00:00", "processed": false}
{"id": "hn_story_46119654", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46119654", "title": "Show HN: I built a bulk image generator after seeing a YouTuber's struggle", "text": "Hello HN,<p>I built this tool after seeing a Reddit thread where a historical documentary creator described their painful workflow. They produce 30-minute videos requiring over 240 unique images. Currently, they have to manually write prompts, generate, and download images one by one for every scene.<p>To solve this bottleneck, I built AI Bulk Image Generator.<p>The Tool: <a href=\"https:&#x2F;&#x2F;aibulkimagegenerator.com\" rel=\"nofollow\">https:&#x2F;&#x2F;aibulkimagegenerator.com</a><p>How it works:<p>I designed the workflow around two core features to maximize efficiency:<p>1\u3001Prompt \u00d7 N (Batch Variations):\nIf you need to explore styles or get the perfect shot, you can input a single prompt, set a specific quantity (e.g., 10 images), and the tool will generate all variations in one batch. No more clicking &quot;generate&quot; repeatedly.<p>2\u3001Prompts via CSV :\nThis addresses the Reddit user&#x27;s problem. You can upload a CSV file containing a list of pre-written prompts (one per line). The system automatically parses the file and creates images for every single prompt in the list. This allows you to generate assets for a full video script in one go.<p>Models Supported:\nCurrently, I support a mix of models including Nano Banana &#x2F; Pro, GPT-4o, and SeaDream v4. I plan to add more models based on user demand.\nThis is an MVP aimed at content creators who need volume. I\u2019d appreciate any feedback on the UI or the batch processing flow!<p>Thanks!", "author": "qinggeng", "timestamp": "2025-12-02T10:14:02+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-02T17:16:06.665090+00:00", "processed": false}
