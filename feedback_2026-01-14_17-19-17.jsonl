{"id": "hn_story_46618481", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46618481", "title": "Show HN: AIOStack \u2013 Using eBPF to Secure AI Services in Kubernetes", "text": "Hey HN! We built a tool that uses eBPF to discover AI services and their data flows in Kubernetes clusters.<p>Modern AI apps often follow this pattern:\n1. Service receives request\n2. Queries database (PostgreSQL&#x2F;Redis&#x2F;MongoDB)\n3. Sends data to LLM API (OpenAI&#x2F;Anthropic&#x2F;Bedrock)\n4. Consumes or returns the AI generated response<p>Security teams often don&#x27;t know:\n- Which services are making AI calls\n- What databases they&#x27;re accessing first\n- Whether PII is being sent to third-party APIs\n- What libraries and packages are being used for AI<p>Our eBPF based tool attaches to network and fs syscalls to observe:\n- Outbound connections to AI API endpoints (pattern matching on domains&#x2F;IPs)\n- Database protocol detection (PostgreSQL, MySQL, MongoDB wire protocols)\n- Service-to-service communication within the cluster\n- Libraries invoked by processes (PyTorch, HF, OpenCV etc)<p>Architecture:\n- eBPF with C in kernel space\n- Go userspace agent processes events\n- Results sent to in-cluster exporter\n- Next.js for visualization<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aurva-io&#x2F;AIOstack\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aurva-io&#x2F;AIOstack</a>\nDemo: <a href=\"https:&#x2F;&#x2F;aurva.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;aurva.ai</a><p>Questions for you guys:\n1. What classifications&#x2F;buckets would you like to see for apps?\n2. Other protocols&#x2F;services we should detect?\n3. Performance overhead-what&#x27;s acceptable in prod?", "author": "sniner", "timestamp": "2026-01-14T16:52:53+00:00", "score": 3, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-14T17:19:19.452359+00:00", "processed": false}
{"id": "hn_story_46617026", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46617026", "title": "Show HN: Cowork \u2013 A curated list of resources for Claude Cowork", "text": "Hi HN,<p>Like many of you, I&#x27;ve been exploring the new Claude Cowork capabilities. While the official docs are great, I found that practical examples, working prompts, and configurations are scattered across GitHub issues, Twitter, and various blogs.<p>I built Awesome Cowork to aggregate these resources in one place.<p>Currently, it includes:<p>- Prompts for file management and web scraping.\n- A list of troubleshooting tips for common errors.<p>It&#x27;s still a work in progress. If you have any scripts or use cases you&#x27;ve tested, I&#x27;d love to add them to the list (or feel free to submit a PR to the GitHub repo).<p>Hope this saves you some setup time!", "author": "horatio_li", "timestamp": "2026-01-14T15:20:40+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-14T17:19:28.993381+00:00", "processed": false}
{"id": "hn_story_46616668", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46616668", "title": "Show HN: Your Domains Wrapped - A 2025 domain recap", "text": "Hi HN,<p>You know that feeling when you have a brilliant idea at 2am, buy the domain immediately, and then never touch it again?<p>Last month I sat down to figure out how much money I&#x27;ve been bleeding on domain renewals, and honestly, I was scared to look. I had domains scattered across GoDaddy, Namecheap, and a few other registrars I barely remember signing up for.<p>I did what we all do: I built a tool for it.<p>It&#x27;s basically &quot;Spotify Wrapped&quot; but for all those domains you bought and forgot about. Connect your registrars manually, and get:<p>- How many domains you have\n- Total renewal costs\n- Estimated portfolio value \n- Domains added in 2025\n- How much you&#x27;re spending on unused domains<p>No DNS configuration. No API keys. No registrar access.<p>Just import your domains via CSV or add them manually. Tried to make it as intuitive as possible.<p>Curious to hear your feedback, and how many you registered last year :)<p>Try it out: <a href=\"https:&#x2F;&#x2F;domained.app&#x2F;wrapped\" rel=\"nofollow\">https:&#x2F;&#x2F;domained.app&#x2F;wrapped</a><p>--<p>Tech-wise: Nothing fancy. Just Next.js + Supabase. BUT the design process was interesting. I used Figma Make with Gemini 3 Pro for all the initial iterations and honestly, it&#x27;s pretty impressive. If you pay $20 you get unlimited usage until March or something, which is insane value if you need to pump out a lot of design variations quickly. Highly recommended.", "author": "joggez", "timestamp": "2026-01-14T14:50:58+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-14T17:19:31.921962+00:00", "processed": false}
{"id": "hn_story_46616562", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46616562", "title": "Show HN: Nori CLI, a better interface for Claude Code (no flicker)", "text": "Hi HN, my name&#x27;s Clifford and I&#x27;m one of the creators of Nori. I\u2019ve been using Claude Code heavily since last summer, and after understanding some of the tradeoffs with their TUI implementation, I knew I couldn&#x27;t see myself living for years with this interface as one of my daily-driver tools.<p>It is <i>not</i> a hard problem to make monospace text output performant, so why does Claude Code suffer from flicker and strobing in the terminal (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;1913\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;1913</a>)? Even after they&#x27;ve released multiple improvements for this, I still see the issue in terminal splits with fewer rows, or in less performant emulators, and even within a virtual TTY (the absolute simplest environment to run an interactive program in). After digging in throughout the past half year, the issue is mostly inevitable because Claude reprints full terminal history without using alt screen mode and uses a React-based framework (Ink) to render and style their text. That&#x27;s great for JS+CSS being &quot;on distribution&quot; for LLMs in order to vibecode the continued development of Claude Code, but it doesn&#x27;t deliver the experience I&#x27;d like. The frameworks they&#x27;ve chosen also have limitations around [terminal input parsing (i.e. the shift enter issues from last year: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode&#x2F;issues&#x2F;1505#issuecomment-3411334883\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode&#x2F;issues&#x2F;1505#issuecomme...</a>). Great terminal interfaces I&#x27;ve lived with for years (neovim, btop, helix, Cataclysm DDA, etc) don&#x27;t sacrifice user experience as a tradeoff for development convenience. They build resilient terminal interfaces on languages more appropriate for this problem, like C or C++ or Rust.<p>Finally, while I&#x27;m definitely rooting for Anthropic to continue improving their products, I can&#x27;t see myself coupling a commandline tool I use often with a single LLM provider. It would be insane if pushing my code to GitHub required me to edit it in VSCode \u2014 I want my tooling to do one thing well, and that&#x27;s display the read-eval-tool-loop from talking to an agent. Opus 4.5 has been stellar, but it&#x27;s nonnegotiable to me that I can try out varied providers with the same tools I plan to use everyday. Claude Code will not be working long term on how best to interface with multiple agents, from varying providers, in one terminal pane, and that makes perfect sense for their business. However based on our other experiences building out profiles and skillsets for agents, deeper customizations of agent instructions and subagents, and parallel worktrees for local agents, we have a lot of vision for how to handle local agentic work. And with the current design to integrate at the agent-level, we don&#x27;t plan on working around the OAuth flows or spoofing the system prompt outside of the Claude Code SDK (like with the OpenCode situation), and risk the tools coming into conflict with the providers.<p>These were the main considerations that went into designing Nori CLI. It&#x27;s a very thin and very fast TUI wrapper around multiple agent providers. It integrates with providers at the agent level, instead of the model level. Not only does that provide better performance in our experience, but that is also *compliant with current ToS for subscription based usage.* This is a very early version, but given the timing this week it might give you a flicker-free way to code with Claude Code!<p>The project is open source, and built on the stellar work by folks at Zed (on the abstraction over varied coding agents), and the folks working on Codex CLI (who have put together one of the nicest proprietary terminal experiences).<p>I&#x27;m very curious:\nWhat are the Claude Code features you couldn&#x27;t give up, to make the switch to a tool like this?\nWhat are the Claude Code features that work as intended, but you can&#x27;t stand?", "author": "csressel", "timestamp": "2026-01-14T14:40:51+00:00", "score": 10, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-14T17:19:32.761486+00:00", "processed": false}
{"id": "hn_comment_46618138", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46618138", "title": "Re: 'Havana Syndrome' Device Could Finally Solve Myste...", "text": "I&#x27;m sceptical. This sounds like a James Bond type machine: a size that will fit in a backpack, yet somehow putting out all kinds of energy. That alone is suspicious.<p>The second thing that makes me sceptical is that it came from ODNI. That&#x27;s Tulsi Gabbard, who has zero qualms about bald faced lying. The entire Trump administration has little care for the truth, Gabbard is probably worse. There are also rumors that Gabbard is on the outs, so she may be doing a big, splashy thing to regain status in the Trump inner circle.", "author": "bediger4000", "timestamp": "2026-01-14T16:31:58+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-14T17:19:34.303279+00:00", "processed": false}
{"id": "hn_story_46616344", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46616344", "title": "Show HN: LogiCart \u2013 Agentic shopping using Generative UI (A2UI pattern)", "text": "Hey HN, I\u2019m the solo builder behind LogiCart.<p>I recently refactored my frontend to use a Generative UI pattern (inspired by Google&#x27;s new A2UI framework) because I realized a static chat interface fails for complex shopping intents.<p>The Problem: A user buying a single item needs a completely different UX than a user planning a complex project. A standard &quot;list of cards&quot; doesn&#x27;t work for both.<p>The Solution: I built an Intent-to-UI engine where the LLM decides the interface structure based on the query.<p>How the Logic Works:<p>Intent Classification: The LLM first classifies the prompt into one of three modes.<p>Dynamic Rendering: It returns a JSON schema that my custom React renderer maps to specific components:<p>Single Item Intent (e.g., &quot;Best Gaming Monitor&quot;): Triggers a Comparison View. It renders a &quot;Best Match&quot; card with detailed specs alongside 3 alternatives for quick comparison.<p>Bundle Intent (e.g., &quot;Build an AMD Gaming PC&quot;): Triggers a Grouped View. It clusters products by category (CPU, GPU, RAM) to ensure the build is complete.<p>DIY&#x2F;Project Intent (e.g., &quot;How to build a deck&quot;): Triggers a Plan View. It renders a step-by-step timeline mixed with the required materials. The number of steps and product complexity dynamically adjusts based on the user&#x27;s stated experience level.<p>The Stack:<p>Backend: Node.js &#x2F; TypeScript<p>Search: pgvector (PostgreSQL) for semantic retrieval of Amazon&#x2F;Retailer SKUs.<p>Frontend: React (with a custom renderer for the A2UI schemas).<p>Context: I pivoted to this &quot;deep complexity&quot; approach after Microsoft Copilot launched their generic shopping agent 24 hours after my initial beta. I realized I couldn&#x27;t compete on generic search, so I\u2019m focusing on the complex&#x2F;messy projects that require dynamic UI adaptation.<p>It\u2019s live in Beta. I\u2019d love feedback on the &quot;Intent Router&quot;\u2014try breaking it by asking for something ambiguous like &quot;Coffee&quot; vs &quot;Coffee Station&quot; to see if the UI adapts correctly.<p>Link: <a href=\"https:&#x2F;&#x2F;logicart.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;logicart.ai</a>", "author": "ahmedm24", "timestamp": "2026-01-14T14:20:26+00:00", "score": 1, "num_comments": 2, "products": ["copilot"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-14T17:19:35.006412+00:00", "processed": false}
{"id": "hn_comment_46615569", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46615569", "title": "Re: I Built Videos with Soro2...", "text": "I Built Videos with Soro2 So You Don&#x27;t Have to Wait on Another Waitlist\nLook, I&#x27;m tired of waitlists. We all are. OpenAI drops Sora, everyone gets hyped, then... crickets. You&#x27;re stuck waiting while watching demo videos on Twitter from the 47 people who actually got access.\nSo I tried Soro2 instead. No waitlist. Just works. Here&#x27;s what I found.\nThe Character Thing Actually Works\nThis was the first thing that surprised me. You know how AI video usually can&#x27;t keep a character consistent? Like, frame 1 shows a woman with brown hair, frame 50 she&#x27;s suddenly blonde?\nSoro2 lets you upload a reference image and tag it with @username. Then you can reuse that character across different videos. I tested this with a cartoon mascot I made\u2014generated 10 different videos, and the character actually looked the same in all of them. Not perfect, but way better than I expected.\nPhysics That Don&#x27;t Look Broken\nWater actually flows like water. Fabric moves like fabric. I made a test video of someone walking through a puddle, and the splash looked... right?\nMost AI video tools give you this uncanny valley motion where things sort of float or glitch. Soro2&#x27;s physics engine seems to understand weight and momentum. Hair bounces naturally. Objects fall with proper gravity. It&#x27;s the small stuff that makes it not look immediately &quot;AI-generated.&quot;\nThey Baked in Audio\nEvery video comes with sound effects already synced. Footsteps line up with walking. If you generate a scene with rain, you get rain sounds.\nIs it perfect? No. But it saves you from having to hunt down stock audio or do Foley work yourself. For quick prototypes or social media content, this is huge.\nMultiple Models to Pick From\nThey&#x27;re not just running one model. You get:<p>Sora 2 (standard and Pro)\nVeo 3.1 &amp; 3.1 Fast\nNanobanana &amp; Nanobanana Pro\nSeedream 4.5<p>I honestly don&#x27;t know the technical differences between all of these, but having options means you can experiment with different styles without switching platforms.\nHow It Actually Works<p>Type what you want in plain English. &quot;A dog running through autumn leaves&quot; or get detailed with camera angles and lighting.\nOptionally upload reference images for characters or style.\nPick your model and duration (10-25 seconds depending on which model).\nHit generate. Cloud GPUs do the work.\nDownload 1080p video with audio included.<p>No local GPU needed. No Docker containers. No Python environments. Just a web interface.\nThe Workflow is Fast\nI&#x27;m used to AI video tools taking 10-20 minutes per generation. Soro2 was noticeably faster\u2014most of my tests came back in under 5 minutes. Not instant, but fast enough that I could iterate on ideas without losing momentum.\nWhat Could Be Better\nPrompt engineering still matters. Vague prompts give you vague results. You need to describe camera movements, lighting, time of day, specific actions. The more detail, the better.\n25 seconds is still short. Yeah, it&#x27;s longer than most tools, but you&#x27;re not making a short film here. Think social media clips, not YouTube videos.\nNo geographic blocks, but... they claim worldwide access with no VPN needed. I&#x27;m in the US so I can&#x27;t verify this, but several testimonials mention it working in Germany and other regions where official tools are blocked.\nUse Cases I&#x27;ve Tested<p>Concept visualization for client pitches (way cheaper than hiring a videographer for mockups)\nSocial media content (Instagram Reels, TikTok)\nStoryboarding (generate rough scenes before committing to real production)\nProduct demos (for products that don&#x27;t exist yet)<p>The Elephant in the Room\nIs this using OpenAI&#x27;s actual Sora model? The branding says &quot;Sora 2&quot; but I have no idea if this is licensed, reverse-engineered, or just marketing. The output quality is good, but I can&#x27;t verify the underlying tech.\nThat said\u2014it works, it&#x27;s accessible, and it&#x27;s not asking me to join a waitlist or verify my use case. For prototyping and experimentation, that&#x27;s enough for me right now.", "author": "xbaicai", "timestamp": "2026-01-14T13:04:43+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-01-14T17:19:40.883391+00:00", "processed": false}
{"id": "hn_comment_46614442", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46614442", "title": "Re: Why AI works better on existing codebases...", "text": "This reads like a ChatGPT response", "author": "ageitgey", "timestamp": "2026-01-14T10:37:53+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-14T17:19:52.011383+00:00", "processed": false}
{"id": "hn_comment_46614026", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46614026", "title": "Re: Show HN: Visibility and Controls for Browser Agent...", "text": "P.S.: The extension has as many permissions as Claude in Chrome itself. But, the only network requests from the extension are to posthog, just for us to know which features are being used.<p>Here is a youtube video where I show the network requests of the extension: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=J356Nquxmp4\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=J356Nquxmp4</a><p>To know what posthog collects and how to disable it (change in a single line of code), please refer to this file: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ContextFort-AI&#x2F;ContextFort&#x2F;blob&#x2F;main&#x2F;POSTHOG.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ContextFort-AI&#x2F;ContextFort&#x2F;blob&#x2F;main&#x2F;POST...</a>", "author": "ashwinr2002", "timestamp": "2026-01-14T09:23:32+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-14T17:19:54.338673+00:00", "processed": false}
{"id": "hn_story_46613943", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46613943", "title": "Show HN: AI Contract Reviewer \u2013 Flags Risks and Suggests Fixes in Minutes", "text": "Hey HN,<p>I&#x27;m building an AI tool that helps non-lawyers and busy procurement&#x2F;legal teams quickly review vendor&#x2F;client contracts, NDAs, employment agreements, etc. \u2014 without uploading sensitive data to the cloud (offline&#x2F;local-first option) or replacing lawyers.<p>Background: As someone who&#x27;s wasted days manually hunting for risky clauses, vague terms, hidden overrides in amendments, or unfair liability language in vendor deals, I decided to prototype this after seeing how much time&#x2F;money gets burned on basic reviews.<p>What it does right now (early MVP&#x2F;beta):\n- Upload PDF&#x2F;Word&#x2F;plain text contract\n- Scans for common risks: indemnity caps missing, auto-renewals, one-sided termination, IP ownership traps, liability exceeding fees, non-compete overreach, etc.\n- Flags issues with plain-English explanations + confidence score\n- Suggests safer alternative clauses (based on standard templates&#x2F;best practices)\n- Basic redlining&#x2F;highlighting output (exportable)\n- Offline mode using local models (no data leaves your machine)<p>Tech stack (simple &amp; transparent):\n- Frontend: React + Tailwind\n- Backend: Python + fine-tuned open models (e.g., Llama-3 or similar legal-tuned variants) + some rule-based checks for accuracy\n- No cloud LLM calls in core flow (privacy focus); optional Grok&#x2F;Claude integration for deeper suggestions\n- Processes docs locally via Ollama or similar<p>Current status:\n- Tested on ~50 real-ish contracts (NDAs, SaaS agreements, freelance templates)\n- Average time: 2-5 minutes vs. hours&#x2F;days manual\n- ~75-85% of obvious risks caught (still misses nuanced stuff \u2014 not lawyer-grade yet)\n- Free beta, no signup required (just drag &amp; drop on the demo page)<p>I&#x27;m looking for brutal feedback, especially from:\n- In-house counsel&#x2F;procurement folks: What clauses cause you the most pain&#x2F;headaches?\n- Developers&#x2F;freelancers&#x2F;small biz owners: Would you trust this for quick scans before signing vendor deals?\n- Anyone who&#x27;s used Spellbook&#x2F;LegalFly&#x2F;Ironclad: How does this compare? What gaps do you see?\n- Trust&#x2F;accuracy concerns: Hallucinations, false positives, liability disclaimers?<p>Happy to share more on training data approach, offline setup, or why I focused on negotiation basics vs. full lifecycle.<p>Thanks for any thoughts \u2014 this is day-early, so roast away!", "author": "Saurabh_Kumar_", "timestamp": "2026-01-14T09:11:17+00:00", "score": 1, "num_comments": 0, "products": ["claude", "grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-14T17:19:55.363135+00:00", "processed": false}
{"id": "hn_story_46613652", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46613652", "title": "ChatGPT Voice While Driving", "text": "Tldr: we are living in the future.<p>I tried for the first time, having a conversation with ChatGPT using voice mode, while I was driving (handsfree of course).<p>It was on of those moments where I take a beat and really consider what was happening. The same like when I tried VR for the first time. Or when I got off a train in London quite a few years back and saw a no vaping sign (instead of a no smoking sign). Or when I first saw a mobile phone that was playing a video natively in the device (showing my age here...).<p>It dawned on me, that here I was talking to an artificial intelligence about an idea, with nearly no latency, over about 20 minutes as I drove. It seemed like I was talking to a smart friend who could have just as easily been riding along as a passenger. It&#x27;s amazing to think of all of the technological advances required to make this happen.<p>It feels like we are living in the future, yet, we so easily accept the changes. It&#x27;s fascinating.", "author": "smarri", "timestamp": "2026-01-14T08:26:35+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-14T17:19:56.615293+00:00", "processed": false}
