{"id": "hn_story_45993636", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45993636", "title": "Show HN: Explanans \u2013 Personalized video lectures for any topic", "text": "I created this as a product for solving the long-tail of education, specifically with video lectures. YouTube obviously has great videos on subjects like &quot;What is a derivative&quot;, &quot;Germany post world war 2&quot; or &quot;History of the roman empire&quot; but it won&#x27;t always have great videos for more niche subjects like say &quot;Swedish monetary theory through history&quot; or &quot;deep dive on the nutrients of raspberries vs blackberries vs blueberries&quot;.<p>Here are some videos that I&#x27;ve created under a few different categories:<p>Content with a personalized angle:<p>- Biology from a computational perspective: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j9714e2pyxm0qysbkth4hn0tf57ve18x\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j9714e2pyxm0qysbkth4hn0tf57ve18...</a><p>- Machine learning for a non-technical person: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j97a9d5xpqjq65r9s8zye66ak57vs671\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j97a9d5xpqjq65r9s8zye66ak57vs67...</a><p>Niche topics<p>- Covid 19 clinical studies: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j97dvfc5r7xen50t3d1yg3jfvx7vsqvp\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j97dvfc5r7xen50t3d1yg3jfvx7vsqv...</a><p>- Swedish monetary policy: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j978hwrz0kp7dpr43zgv0bgwrd7vq1h0\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j978hwrz0kp7dpr43zgv0bgwrd7vq1h...</a><p>- Estimating North Korea&#x27;s GDP: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j974ccfjrbbb6e4zmfgjqfwwtn7vrncc\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j974ccfjrbbb6e4zmfgjqfwwtn7vrnc...</a><p>And some random subjects:<p>- Things to do in Ireland: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j971aex478rbthv5n0f6vk8m6h7vsynt\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j971aex478rbthv5n0f6vk8m6h7vsyn...</a><p>- Clinical studies from start to finish: <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j979m7w6ba8xf4a4gnbjttcmh57vrm69\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;videos&#x2F;j979m7w6ba8xf4a4gnbjttcmh57vrm6...</a><p>The accuracy of these videos are (and should be) about the same as you expect from the best LLMs, because this product is fundamentally based on LLMs with access to some tools.<p>You can try it for free at <a href=\"https:&#x2F;&#x2F;explanans.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;explanans.com&#x2F;</a>, every new user gets 2 free videos. Would love to give out more, but inference costs for generating a video are quite high. Free users can watch as many existing videos as they want, and you don&#x27;t even have to sign up for that part.<p>These videos are far from perfect right now, but I think one of the few killer use cases for LLMs are for learning. I think Explanans can be used in the same way that we use ChatGPT&#x2F;Claude&#x2F;Gemini, but where the output is in a different format than text. For some mediums I think that&#x27;s a superior experience, for the same reasons that lots of people prefer YouTube over blog posts.<p>There are a lot of features I want to add to this such as chat with video, some kind of quizzing for retention, or chat before generating to get a more specific output (like how deep research products do it) but I didn&#x27;t wanna clutter the user experience for v1.<p>Here to answers any questions you might have!", "author": "lapurita", "timestamp": "2025-11-20T15:30:10+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["naming_terminology", "onboarding", "response_quality"], "sentiment": null, "collected_at": "2025-11-20T17:09:38.413945+00:00", "processed": false}
{"id": "hn_comment_45994337", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45994337", "title": "Re: Nano Banana Pro...", "text": "I...worked on the detailed Nano Banana prompt engineering analysis for months (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45917875\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45917875</a>)...and...Google just...Google released a new version.<p>Nano Banana Pro <i>should</i> work with my gemimg package (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;minimaxir&#x2F;gemimg\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;minimaxir&#x2F;gemimg</a>) without pushing a new version by passing:<p><pre><code>    g = GemImg(model=&quot;gemini-3-pro-image-preview&quot;)\n</code></pre>\nI&#x27;ll add the new output resolutions and other features ASAP. However, looking at the pricing (<a href=\"https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;pricing#standard_1\" rel=\"nofollow\">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;pricing#standard_1</a>), I&#x27;m definitely not changing the default model to Pro as $0.13 per 1k&#x2F;2k output will make it a tougher sell.<p>EDIT: Something interesting in the docs: <a href=\"https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;image-generation#thinking-process\" rel=\"nofollow\">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;image-generation#think...</a><p>&gt; The model generates up to two interim images to test composition and logic. The last image within Thinking is also the final rendered image.<p>Maybe that&#x27;s partially why the cost is higher: it&#x27;s hard to tell if intermediate images are billed in addition to the output. However, this could cause an issue with the base gemimg and have it return an intermediate image instead of the final image depending on how the output is constructed, so will need to double-check.", "author": "minimaxir", "timestamp": "2025-11-20T16:22:07+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-20T17:09:39.448189+00:00", "processed": false}
{"id": "hn_comment_45994298", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45994298", "title": "Re: Talking to Windows' Copilot AI makes a computer fe...", "text": "Sounds more like the opposite to me. Copilot isn\u2019t making the computer \u201cincompetent\u201d\u2014it\u2019s surfacing complexity in plain language. A PC has always been capable of running scripts, automating workflows, or pulling data, but most people don\u2019t speak in PowerShell or Python. Copilot bridges that gap. If anything, it makes the machine feel more competent because now you can ask for things in natural language and get results without digging through menus or writing code.<p>The real question is whether you measure competence by raw capability or by accessibility. Copilot tilts toward accessibility, which is why it feels different.", "author": "ovo101", "timestamp": "2025-11-20T16:18:56+00:00", "score": null, "num_comments": null, "products": ["copilot"], "categories": ["tone", "navigation", "response_quality"], "sentiment": null, "collected_at": "2025-11-20T17:09:42.118935+00:00", "processed": false}
{"id": "hn_comment_45991983", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45991983", "title": "Re: Ask HN: How Is Gemini 3?...", "text": "I posted this in another thread,but I think it better belongs here:<p>&quot;So Gemini 3 Pro dropped today, which happens to be the day I proofread a historical timeline I&#x27;m assisting a PhD with. I do one pass and then realize I should try Gemini 3 Pro on it. I give the same exact prompt to 3 Pro as Claude 4.5 Sonnet. 3 pro finds 25 real errors, no hallucinations. Claude finds 7 errors, but only 2 of those are unique to Claude. (Claude was better at &quot;wait, that reference doesn&#x27;t match the content! It should be $corrected_citation!). But Gemini&#x27;s visual understanding was top notch. It&#x27;s biggest flaw was that it saw words that wrapped as having extra spaces. But it also correctly caught a typo where a wrapped word was misspelled, so something about it seemed to fixate on those line breaks, I think. A better test would have been 2.5 Pro vs. 3.0&quot;<p>After continuing to use it, I genuinely think &quot;It&#x27;s a good model sir&quot; and plan to add it to my rotation.", "author": "Redster", "timestamp": "2025-11-20T12:41:02+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["navigation"], "sentiment": null, "collected_at": "2025-11-20T17:09:51.036957+00:00", "processed": false}
{"id": "hn_comment_45991851", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45991851", "title": "Re: Ask HN: How Is Gemini 3?...", "text": "I tried it via their antigravity code editor.<p>I was expecting better.<p>I have a frontend code in VUE that had some obvious visual styling problems. I asked it to fix them by providing the screenshot.<p>Gemini kept switching between two versions, both looked wrong. When I asked it to fix the problems, like for example the buttons are two big and doesn&#x27;t match the overall theme of the ui, it just toggle the other version of implementation, which had another set of visual problems.<p>I switched back to claude code to fix those issues, still not in one go, but I seemed to be smoother.<p>Today, I asked gemini to start a project from scratch by looking at a reference code. it told me that the implementation had done and it had compiled and run it, but I saw tons of compiling errors.", "author": "billconan", "timestamp": "2025-11-20T12:21:06+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-20T17:09:51.073444+00:00", "processed": false}
{"id": "hn_comment_45991605", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45991605", "title": "Re: Interactive World History Atlas Since 3000 BC...", "text": "I always wanted something like a &quot;History of human progress&quot; which when zoomed out shows me something like this:<p><pre><code>    -2000000 Stone tools\n    -1000000 Using fire\n       -6000 Metal tools\n       -6000 Agriculture\n       -4000 Writing\n        1550 Printing\n        1888 Telephones\n        1888 Cars\n        1903 Planes\n        1941 Penicillin\n        1941 First computer\n        1982 Homecomputers\n        1983 Mobile phones\n        1990 The internet\n        2001 Wikipedia\n        2004 Facebook \n        2007 IPhone\n        2022 ChatGPT\n</code></pre>\nAnd then I can zoom in on particular areas of time and see smaller milestones.", "author": "mg", "timestamp": "2025-11-20T11:42:32+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["tone"], "sentiment": null, "collected_at": "2025-11-20T17:09:54.815905+00:00", "processed": false}
{"id": "hn_story_45990507", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45990507", "title": "Show HN: Worqlo \u2013 A Conversational Layer for Enterprise Workflows", "text": "Most enterprise work isn\u2019t slow because of bad data.\nIt\u2019s slow because the interface to that data is scattered.<p>A single question like \u201cWhich deals are stalled?\u201d touches dashboards, spreadsheets, a CRM, BI tools, internal scripts, and a few Slack threads. Acting on the answer requires switching between systems again. The friction is in the middle.<p>Worqlo is an experiment in removing that friction by using conversation as the interface layer and deterministic workflows as the execution layer.<p>The idea is simple:\nnatural language in \u2192 validated workflow out.<p>The LLM handles intent.\nA structured workflow engine handles execution: CRM queries, field updates, notifications, permissions, and audit logging.\nThe model never executes actions directly.<p>Below is how it works.<p>Why Conversation?<p>People think in questions.\nSystems think in schemas.\nDashboards sit between them.<p>Interfaces multiply because every system exposes its own UI. Engineers end up building internal tools, filters, queries, analytics pages, and one-off automations. That\u2019s the UI tax.<p>Conversation removes the surface area.\nWorkflows add safety and determinism.<p>Architecture (simplified)\nUser \u2192 LLM (intent) \u2192 Router \u2192 Workflow Engine \u2192 Connectors \u2192 Systems<p>LLM<p>Extracts intent and parameters.\nNo execution privileges.<p>Intent Router<p>Maps intent to a known workflow template.<p>Workflow Engine<p>Executes steps in order:<p>schema validation<p>permission checks<p>CRM queries<p>API updates<p>notifications<p>audit logs<p>Connectors<p>Strict adapters for CRMs, ERPs, internal APIs, and messaging systems.<p>The workflow engine will refuse to run if:<p>fields don\u2019t exist<p>data types mismatch<p>permissions fail<p>workflow template doesn\u2019t match user intent<p>This prevents the usual LLM failure cases: hallucinated fields, incorrect API calls, unsafe actions, etc.<p>Example Query<p>User:<p>&quot;Show me this week&#x27;s pipeline for DACH&quot;<p>Internal flow:<p>intent = llm.parse(&quot;pipeline query&quot;)\nvalidate(intent)\nfetch(data)\naggregate(stats)\nreturn(summary)<p>Follow-up:<p>&quot;Reassign the Lufthansa deal to Julia and remind Alex to follow up&quot;<p>Workflow:<p>find deal by name\nvalidate ownership change\nwrite CRM update\nsend Slack notification\nwrite audit log<p>Everything runs through deterministic steps.<p>Why Start With Sales<p>Sales CRMs are structured and predictable.\nWorkflows repeat (reassign, nudge, follow-up).\nLatency matters.\nOutput is measurable.\nIt makes the domain a good test environment for conversational workflows.<p>The long-term idea is not sales-specific.\nThe same pattern applies to operations, finance, marketing, and HR.<p>Why Not Just Use \u201cChatGPT + API\u201d?<p>Because that breaks fast.<p>LLMs are not reliable execution engines.\nThey hallucinate field names, IDs, endpoints, and logic.\nEnterprise systems require safe, auditable actions.<p>Worqlo treats the LLM as a parser, not a worker.<p>Execution lives in a controlled environment with:<p>workflow templates<p>schema contracts<p>RBAC<p>logs<p>repeatable results<p>This keeps the convenience of natural language and the reliability of a classic automation engine.<p>What We\u2019re Testing<p>We want to see whether:<p>conversation can replace UI for narrow, structured tasks<p>deterministic execution can coexist with natural language intent<p>multi-turn workflows can actually reduce operational load<p>a connector model can scale without creating another integration mess<p>engineers prefer exposing functionality through workflows instead of UI layers<p>It\u2019s still early.\nBut the model seems promising for high-volume, low-level operational work.", "author": "andrewdany", "timestamp": "2025-11-20T08:56:08+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-11-20T17:09:56.995634+00:00", "processed": false}
{"id": "hn_comment_45990046", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=45990046", "title": "Re: Nano Prompt UI \u2013 Local-Only Gemini Nano Side Panel...", "text": "Hi HN,<p>I built Nano Prompt UI, a Chrome side panel that runs entirely on-device using Chrome\u2019s Gemini Nano API.<p>What it does<p>Side panel chat UI that sits next to whatever tab you\u2019re on<p>One-click \u201cSummarize tab\u201d (7 bullets) using the page\u2019s text content<p>Multiple chat sessions with rename &#x2F; duplicate &#x2F; export to Markdown<p>Prompt templates, image attachments (downscaled in-browser), mic input, and read-aloud replies<p>Why I built it\nI wanted something like the ChatGPT sidebar, but:<p>no accounts or servers<p>safe to use with NDA\u2019d docs or internal tools<p>still usable when I\u2019m offline &#x2F; on flaky Wi-Fi<p>How it works<p>Chrome MV3 extension with side panel UI<p>Uses chrome.ai.languageModel &#x2F; Gemini Nano for all completions (no network calls)<p>Simple heuristics to decide when to include page context vs just answer the question<p>Sessions + settings (temperature, top-K, custom system prompt) are stored locally in chrome.storage<p>No backend; I don\u2019t run any servers and nothing is sent off-device<p>Limitations<p>Requires a recent Chrome build with the on-device model enabled (details in the README)<p>Only tested on desktop Chrome so far<p>Context&#x2F;window size is whatever Chrome exposes for Nano right now, so huge pages get truncated<p>Repo (with install + flags):\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;theodedra&#x2F;nano-prompt-ui\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;theodedra&#x2F;nano-prompt-ui</a><p>I\u2019d love feedback on:<p>UX of the side panel vs popup<p>The way I\u2019m handling page context (when to include tab text, when not to)<p>Any ideas to keep it privacy-first but more powerful (better summaries, code workflows, etc.)<p>Happy to answer implementation questions in the comments.", "author": "theodedra", "timestamp": "2025-11-20T07:47:13+00:00", "score": null, "num_comments": null, "products": ["chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-20T17:09:59.449304+00:00", "processed": false}
