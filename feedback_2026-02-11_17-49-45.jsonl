{"id": "hn_story_46977327", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46977327", "title": "Show HN: Deadend CLI \u2013 Open-source self-hosted agentic pentest tooling", "text": "Hi HN,<p>Deadend is an agentic pentest CLI that automates vulnerability research in webapps.<p>the problem we are trying to solve : removing the time consumed in repetitive assessments, report generation and extracting relevant information to let them focus on vulnerability research but powerful enough to find issues or leads by itself when we are in a deadend.<p>highlights : As of today, we scored 78% on XBOW\u2019s benchmarks with claude-sonnet-4.5 in blackbox (we are currently iterating over the architecture of the agent and running the newest to get better results overall).<p>The agent runs entirely locally with optional self-hosted models. Shell tooling is isolated in Docker, and the python interpreter with WASM.<p>Some cool ideas are on the roadmap : CI&#x2F;CD integrations, code review, bash completion, OWASP Top 10 plugins\u2026<p>Docker is needed and it currently works only on MacOS Arm64 and Linux 64bits installable in one bash command.<p>Github Repo : <a href=\"https:&#x2F;&#x2F;github.com&#x2F;xoxruns&#x2F;deadend-cli\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;xoxruns&#x2F;deadend-cli</a>\nDiscord server : <a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;zwUVa3E7KT\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;zwUVa3E7KT</a><p>Love to hear your thoughts and feedbacks!", "author": "gemini-15", "timestamp": "2026-02-11T16:49:55+00:00", "score": 15, "num_comments": 7, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:49:50.144355+00:00", "processed": false}
{"id": "hn_story_46977023", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46977023", "title": "Show HN: Rampart \u2013 Open-source security for Claude and AI agents in YOLO mode", "text": "I&#x27;ve been running an AI agent 24&#x2F;7 on my home lab through OpenClaw \u2014 k3s cluster management, shell commands, config edits, all unsupervised. I could see what the agent was running, but had no way to stop a bad command before it executed. So I built Rampart.<p>How it works: you write a YAML policy that says what&#x27;s allowed, denied, or flagged. Rampart evaluates every tool call against that policy before it runs. Here&#x27;s what a policy looks like:<p><pre><code>  - rm -rf &#x2F; \u2192 denied\n  - sudo anything \u2192 logged for review\n  - curl, wget \u2192 logged for review\n  - git push, go build, normal dev commands \u2192 allowed\n  - cat ~&#x2F;.ssh&#x2F;id_rsa \u2192 denied\n</code></pre>\nEverything gets written to a hash-chained audit trail. You can watch it live with &quot;rampart watch&quot; or generate HTML reports with &quot;rampart report&quot;.<p>Setup for Claude Code takes one command: &quot;rampart setup claude-code&quot;. It installs hooks that intercept every Bash command, file read, and file write before execution. Blocked commands never run \u2014 Claude sees an error and moves on.<p>Setup for OpenClaw agents is also one command: &quot;rampart setup openclaw&quot;. Works on Linux and macOS.<p>Also works as a shell wrapper for any agent (&quot;rampart wrap&quot;), an MCP protocol proxy (&quot;rampart mcp&quot;), or an HTTP API that agent platforms can consult before executing anything (&quot;rampart serve&quot;).<p>Go, ~14K lines, Apache 2.0, zero runtime deps. Policy eval takes under 20 microseconds.<p>I&#x27;d love feedback on what policies you&#x27;d want out of the box and what integrations matter most.", "author": "cl4p", "timestamp": "2026-02-11T16:28:42+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-11T17:49:52.850205+00:00", "processed": false}
{"id": "hn_comment_46977838", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46977838", "title": "Re: Toyota Fluorite: \"console-grade\" Flutter game engi...", "text": "The combination of Flutter + Claude Code makes cross-platform app development really, really fast. I&#x27;ve been impressed with how well Clause handles prompts like, &quot;This list should expand on the web, but not on iOS.&quot; I then ask it (Claude) to run both a web instance and an iOS simulator instance. Can usability test in-tandem.<p>I recently (as in, last night) added WebSockets to my backend, push notifications to my frontend iOS, and notification banner to the webapp. It all kinda just works. Biggest issues have been version-matching across with Django&#x2F;Gunicorn&#x2F;Amazon Linux images.", "author": "aabajian", "timestamp": "2026-02-11T17:24:19+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-11T17:49:53.505625+00:00", "processed": false}
{"id": "hn_story_46976710", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46976710", "title": "Show HN: I built an AI executive assistant you use through iMessage", "text": "I built an AI executive assistant that works through iMessage.<p>Instead of creating another dashboard or agent interface, I wanted something that behaves more like messaging a real assistant.<p>Attach\u00e9 works entirely over iMessage.<p>You connect Gmail once and then you can send messages like:<p>give me a morning brief of what matters today\nsummarize important emails from the last 24 hours\ndraft a response to the investor thread and keep it concise\nfind a 30 minute slot next week for John and send options\nremind me if the contract is not signed by Friday<p>It manages inbox and calendar together and maintains persistent memory. It remembers your preferences and directives over time, so it adapts instead of starting from scratch each session.<p>It can also proactively summarize important emails as they arrive and prepare draft replies so you can approve or edit them quickly.<p>Security was critical because this touches real accounts. We do not store email credentials. Access is via OAuth and tokens are encrypted. During beta Google shows the unverified app warning because verification is still in progress.<p>Pricing is fixed so users do not have to think about LLM token usage or variable billing.<p>This was built in about a week using Claude Code.<p>I would really appreciate feedback, especially around edge cases, trust, and what you would or would not delegate to something like this.", "author": "mushgev", "timestamp": "2026-02-11T16:09:05+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:49:55.841085+00:00", "processed": false}
{"id": "hn_comment_46976696", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46976696", "title": "Re: Show HN: A local CLI that redacts secrets before y...", "text": "I kept running into the same problem during incidents: something breaks, I need help fast, and the quickest path is pasting logs into ChatGPT or a vendor ticket \u2014 but I\u2019d often realize too late I\u2019d leaked API keys or database URLs.<p>BlackTent is a local CLI that redacts secrets before anything leaves your machine. It scans code, configs, and (optionally) logs, replaces credentials deterministically, and outputs a reviewable bundle you can inspect before sharing.<p>How it works:<p>Scans project files for common secret patterns (API keys, tokens, env vars, DB URLs)<p>Replaces them with deterministic placeholders (same secret \u2192 same placeholder)<p>Produces a bundle + manifest showing exactly what was redacted<p>Properties:<p>Runs entirely locally (no network calls, telemetry, or history)<p>Deterministic and diffable<p>Redaction rules are fixed and inspectable<p>Logs are opt-in (they\u2019re risky and context-heavy)<p>Example:<p>blacktent bundle .\n# Creates incident-2025-01-28.tar.gz\n# Review the manifest, then share the bundle<p>This is not a security boundary or forensic tool \u2014 it\u2019s meant to reduce accidental leakage under time pressure.<p>I\u2019d especially appreciate feedback on edge cases, false-positive tolerance, and integrations people would actually use.", "author": "blacktent", "timestamp": "2026-02-11T16:08:37+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:49:56.377381+00:00", "processed": false}
{"id": "hn_comment_46976273", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46976273", "title": "Re: AITools.coffee \u2013 GitHub metrics observatory tracki...", "text": "Hey HN! I&#x27;m the creator of AITools.coffee. This is a metrics observatory for the open-source AI ecosystem \u2013 think &quot;GitHub Archive meets awesome-AI, but with daily time-series tracking.&quot;<p>What makes this different from awesome-lists?\nAwesome-lists are static Markdown files. They&#x27;re great for discovery, but they:<p>Require manual PRs to update\nShow current state only (no historical trends)\nDon&#x27;t track metrics (stars, forks, contributors, etc.)\nGo stale quickly\nAITools is a live database that:<p>Syncs 27,769 repositories daily via GitHub GraphQL API\nTracks 16 metrics per repo (stars, forks, issues, PRs, releases, commits, contributors, etc.)\nStores daily snapshots for time-series analysis (430M+ datapoints collected so far)\nAuto-removes dead&#x2F;archived repos, auto-heals renamed repos with 301 redirects\nTechnical Architecture\nBackend:<p>PostgreSQL 18 (27K repos, 21K authors, 430K metric snapshots)\nPHP 8.3 REST API with JWT auth\nNightly cron (00:01 UTC) running GitHub GraphQL sync (~25 min for full sync)\nDiscovery Pipeline:<p>Python scripts sweep 50+ AI organizations (OpenAI, Meta, Google, Anthropic, Hugging Face, etc.)\nGitHub Search API monitors 30+ topics (machine-learning, LLM, transformers, etc.)\nGemini 2.5 Flash classifies repos into 30+ categories\n100% manual review before publish (3-layer quality filter)\nFrontend:<p>Tailwind CSS with glassmorphism design\nAlpine.js for interactivity\nChart.js + D3.js for metrics visualization (star distribution, language breakdown, contributor growth)\nData Freshness:<p>Last sync: typically &lt;6 hours ago\n440K+ datapoints added daily (27K repos \u00d7 16 metrics)\nRate limit: 1 GraphQL query&#x2F;sec (stays under GitHub&#x27;s 5K pts&#x2F;hr)\nWhat I&#x27;m tracking\nPer repository (16 datapoints):\nstars, forks, watchers, open issues, open PRs, releases, commits (last 100), contributors, size, archived status, default branch, pushed_at, created_at, license, language, topics<p>Per author (8 datapoints):\nfollowers, following, public repos, gists, bio, company, location, created_at<p>All stored as daily snapshots \u2192 enables time-series analysis (star velocity, contributor growth, issue trends).<p>Current Scale\n27,769 AI repositories tracked\n20,992 open-source authors\n12.4M+ total GitHub stars (aggregated)\n430K+ metric snapshots collected\n440K datapoints added per day\nLimitations &amp; Future Plans\nWhat&#x27;s NOT implemented yet:<p>Public API (planned Q2 2026, always free with rate limits)\nHistorical charts (star growth over time) \u2013 data is there, visualization coming soon\nTrending repos (7-day star velocity ranking) \u2013 planned next month\nEmail alerts for repo milestones \u2013 maybe later\nOpen Source?\nNot yet. Considering open-sourcing the discovery pipeline + classification logic, but the full platform will likely remain closed-source (hosting costs, spam prevention, API abuse).<p>Why I built this\nI got frustrated manually tracking AI repos across GitHub, Twitter, and Discord. There&#x27;s no single place to:<p>Compare similar tools by actual metrics (not just star count)\nSee which projects are actively maintained vs abandoned\nTrack contributor velocity (is the project growing or stagnating?)\nFilter by license, language, framework, use case\nAwesome-lists are great for curated discovery, but terrible for data-driven analysis. I wanted both.<p>Questions I&#x27;m expecting\nQ: How do you handle spam&#x2F;SEO farms?\nA: 3-layer filter: (1) Gemini AI relevance check, (2) Manual review (100% of submissions), (3) Automated quality signals (min 10 stars, active within 2 years, not archived).<p>Q: What about non-GitHub repos (GitLab, Bitbucket)?\nA: Not supported yet. 99% of open-source AI is on GitHub, so I focused there. May expand later if there&#x27;s demand.<p>Q: Can I submit my own project?\nA: Yes! Use the &quot;Submit Tool&quot; form (requires GitHub login to prevent spam). Your repo will be queued for review. Alternatively, if you&#x27;re in one of the 50 orgs I monitor, your repo will be discovered automatically within a week.<p>Q: How accurate is Gemini classification?\nA: ~85% accurate on initial categorization. I manually review and re-categorize misclassifications. Common mistakes: RAG frameworks \u2192 agent frameworks, base models \u2192 fine-tuned models.<p>Q: Will you add X feature?\nA: Probably! Top requests: historical star charts, trending page, email alerts, public API. Working through them in order of complexity vs impact.<p>Q: What&#x27;s your business model?\nA: None yet. This is a side project that costs ~$30&#x2F;month (SiteGround hosting + Gemini API). If it grows beyond hobby scale, I might add sponsored listings or premium API tiers, but the core data will stay free.<p>Feedback welcome! Especially:<p>Missing repos&#x2F;categories you&#x27;d like to see tracked\nUI&#x2F;UX improvements (the homepage is dense with data, might be overwhelming)\nTechnical architecture critiques (I&#x27;m sure there are better ways to do this)\nFeature requests (what metrics would actually be useful?)\nTech stack: PostgreSQL, PHP, Python, Gemini 2.5 Flash, GitHub GraphQL API, Chart.js, D3.js, TailwindCSS, Alpine.js<p>Live at: <a href=\"https:&#x2F;&#x2F;aitools.coffee\" rel=\"nofollow\">https:&#x2F;&#x2F;aitools.coffee</a>", "author": "alexela84", "timestamp": "2026-02-11T15:37:39+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:49:59.688328+00:00", "processed": false}
{"id": "hn_story_46975959", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46975959", "title": "Show HN: MEVA, a desktop Markdown reader for AI-generated docs", "text": "Hey HN!<p>Saurabh here \u2013 I built MEVA, a lightweight desktop app for reading AI-generated markdown. I work with AI tools (Claude, ChatGPT, Copilot) daily and end up with dozens of markdown files \u2013 design docs, API specs, architecture notes, explanations. VS Code previews split your workspace, browser renderers don&#x27;t watch files, and most markdown apps are built for writing, not reading. I just wanted something I could point at a file and read, beautifully rendered, updating live.<p>MEVA watches files in real time, renders LaTeX, Mermaid diagrams, and syntax-highlighted code blocks natively, and works fully offline. No accounts, no cloud sync, no tracking. Under 15MB.<p>I started with Electron but the bundle was 150MB+ for what should be a simple viewer. Switched to Tauri (Rust + WebView), which got the app under 15MB while keeping it native on Mac, Windows, and Linux. For rendering I use markdown-it with plugins for KaTeX and Mermaid.<p>My daily workflow: I ask Claude or ChatGPT to generate a design doc or analysis, save the output as .md, and MEVA picks it up instantly. When AI tools stream output directly to .md files, I see the rendered result building in real time in the app. It&#x27;s become my default way to read any markdown file.<p>Free version includes all core features. There&#x27;s an optional paid version that adds multiple tabs, themes, and a few extras to support continued development.<p>Demo and download: <a href=\"https:&#x2F;&#x2F;usemeva.com&#x2F;#download\" rel=\"nofollow\">https:&#x2F;&#x2F;usemeva.com&#x2F;#download</a><p>Appreciate any feedback \u2013 especially on what feels unnecessary, what&#x27;s missing, or what could work better for your workflow. Happy to answer any questions!", "author": "ss_meva", "timestamp": "2026-02-11T15:14:35+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:01.272846+00:00", "processed": false}
{"id": "hn_story_46975866", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46975866", "title": "Show HN: SPICEBridge \u2013 MCP server for AI circuit design via ngspice", "text": "Built this in under 24 hours. I&#x27;m a self-taught EE and I got \ntired of the loop where I describe a circuit to Claude, then \nhave to manually translate it into a netlist, run ngspice, parse \noutput, check specs, tweak, repeat. The AI couldn&#x27;t touch the \nsimulator.<p>SPICEBridge is an MCP server with 18 tools covering the full \ndesign loop \u2014 template loading with auto-calculated component \nvalues (E24 series), AC&#x2F;transient&#x2F;DC simulation, automated \nmeasurement, spec verification, and schematic generation. One \ncall goes from &quot;1kHz low-pass filter&quot; to a verified, simulated \ncircuit. Works locally with Claude Code (stdio) or remotely via \nCloudflare tunnel.<p>Ran it through a multi-model security audit before public \nrelease. Solo project, GPL-3.0. Would love feedback from anyone \nwho works with SPICE.", "author": "clanker-lover", "timestamp": "2026-02-11T15:06:49+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:02.581218+00:00", "processed": false}
{"id": "hn_story_46975583", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46975583", "title": "Show HN: Godot MCP \u2013 Give AI assistants full access to the Godot editor", "text": "Hi HN, I built an open-source MCP server + Godot editor plugin that gives AI assistants (Claude, Cursor, etc.) direct access to the running Godot editor.\nThe problem: AI coding assistants can edit scripts, but they&#x27;re blind to the Godot editor. They can&#x27;t see your scene tree, add nodes, check for errors, or understand your project structure. You end up copy-pasting context back and forth.\nThe solution: A Godot plugin connects to an MCP server via WebSocket. The AI gets 32 tools that execute inside the live editor - scene manipulation, script editing with validation, project search, console error reading, node property discovery, and more.\nHow it works:\nInstall the MCP server: npx -y godot-mcp-server\nCopy the plugin into your Godot project\nEnable it, and your AI assistant can now talk directly to the editor\nWhat makes this different from other Godot AI tools: Tools run inside the actual editor, not by parsing files from disk. When the AI adds a node, it appears in the editor immediately. When it reads errors, it gets real editor diagnostics, not regex guesses.\nBuilt with TypeScript (MCP server) and GDScript (plugin). MIT licensed.\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;tomyud1&#x2F;godot-mcp\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;tomyud1&#x2F;godot-mcp</a>\nWould love feedback, especially from anyone doing AI-assisted game dev.", "author": "tomyud", "timestamp": "2026-02-11T14:46:21+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-11T17:50:04.367081+00:00", "processed": false}
{"id": "hn_story_46975525", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46975525", "title": "Show HN: Clap.Net \u2013 Source generated CLI Parsing for .NET (Inspired by Clap-Rs)", "text": "Clap.Net is my attempt at bringing the excellent Rust clap crate to .NET as a near 1:1 port.<p>The goal is API and behavioral parity where it makes sense while staying idiomatic to .NET and fully compatible with .NET AOT.<p>This is my first public library, so please go easy on me! I\u2019m sure there are design decisions I\u2019d approach differently with more experience.<p>The project is still evolving but should be ready to use for real-world CLI apps.<p>AI Disclaimer: I have used Claude to finish some features today, before that it was a hand-coded effort.", "author": "scurtis0", "timestamp": "2026-02-11T14:41:59+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-11T17:50:05.175842+00:00", "processed": false}
{"id": "hn_story_46975121", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46975121", "title": "ArXiv Endorsement for Paper on Neuro-Symbolic Architecture for Financial Agents", "text": "Hi Everyone,<p>I\u2019m an independent researcher (and professionally, the Global Director of Research at Reink Media) looking for an endorsement for the cs.AI (Computer Science&#x2F;Artificial Intelligence) category on arXiv.<p>The Context I didn&#x27;t start by writing a paper; I started by building a system. Over the last year, I developed a production-grade Model Context Protocol (MCP) server for the forex market. It\u2019s currently live with 45 distinct tools and renders 28 different dynamic widgets across Claude, ChatGPT, and a custom web app.<p>The Paper The paper is titled Protocol-Constrained Agentic Systems: A Neuro-Symbolic Architecture for Hallucination-Resistant Financial Execution.<p>My core argument is that in high-stakes domains like finance, we cannot rely on LLMs to be &quot;smart enough&quot; to avoid critical errors. Instead, I propose an architecture that uses MCP as a &quot;hallucination firewall.&quot; This strictly decouples the probabilistic layer (the LLM parsing intent) from the deterministic layer (the tool executing the trade). It effectively treats the protocol schema as a type system for agent actions, guaranteeing by construction that invalid tool calls cannot reach the execution layer.<p>You can read the full paper and see the architecture in the PDF here: https:&#x2F;&#x2F;www.stevenhatzakis.com&#x2F;research&#x2F;protocol-constrained-agentic-systems<p>The Request If you are a registered endorser for cs.AI and find this work relevant, I would appreciate your support so I can submit this paper on arXiv.org.<p>Endorsement Code: LZRTFH Link: https:&#x2F;&#x2F;arxiv.org&#x2F;auth&#x2F;endorse?x=LZRTFH<p>Thanks for your time.<p>Steven Hatzakis", "author": "shatzakis", "timestamp": "2026-02-11T14:09:47+00:00", "score": 1, "num_comments": 1, "products": ["claude", "chatgpt"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-11T17:50:07.586480+00:00", "processed": false}
{"id": "hn_comment_46975036", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46975036", "title": "Re: Show HN: Capsule, interactive coding agent session...", "text": "Hi, one of the cofounders here! We work with a ton of different agents in our other open source project, Rover (<a href=\"https:&#x2F;&#x2F;endor.dev&#x2F;rover\" rel=\"nofollow\">https:&#x2F;&#x2F;endor.dev&#x2F;rover</a>). Sometimes we need to debug how the agents are behaving and we built this tool to make our life easier. It is a web interface in which you can upload sessions logs that you have exported from Claude, Codex or any other coding agent. You can easily navigate them, check the reasoning, tool calls, etc. as well as explore what subagents were doing. Everything happens local to the browser, so no data is sent to any server.<p>It can also be used to safely share agent session logs with others, either projects you are contributing to or to troubleshoot something. It has an optional, built-in anonymizer so no private information leaks accidentally.", "author": "ridruejo", "timestamp": "2026-02-11T14:01:09+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-11T17:50:08.484953+00:00", "processed": false}
{"id": "hn_comment_46974971", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46974971", "title": "Re: Show HN: AgentWire \u2013 Talk to your AI coding agents...", "text": "I run multiple Claude Code and OpenCode sessions at the same time \u2014 sometimes on my laptop, sometimes on remote devboxes over SSH. The friction that bugged me most was context switching: I&#x27;d be thinking through a problem, pacing around, and then have to sit down, find the right terminal window, and type out what I wanted the agent to do. I kept wishing I could just say it out loud.<p>So I built AgentWire. It&#x27;s a self-hosted CLI + browser portal that sits on top of tmux. You open the portal on your phone or tablet, push-to-talk, and your voice gets transcribed and routed to whichever agent session you pick. The agent talks back via TTS. It works across machines \u2014 SSH into a remote box, and those sessions show up in the same portal.<p>The core loop: speak a prompt, agent works on it, agent speaks the result back. You can be on the couch, on a train, wherever \u2014 as long as your portal is accessible (I use a Cloudflare tunnel).<p>It also does multi-agent orchestration. One session can be the &quot;leader&quot; that spawns worker panes, delegates tasks, and collects summaries. This is useful when you want parallel work across a codebase without the agents stepping on each other.<p>You can run everything on one machine, or distribute it across several like I do \u2014 I have TTS running on a Windows WSL box with a GPU, several dev boxes (local and remote) running various projects, and my main local machine running the portal and STT.<p>Install: pip install agentwire-dev (or uv tool install agentwire-dev). No cloud, no accounts, no sign-up. Your audio stays on your machines. TTS runs on your own GPU or serverless (RunPod), STT via Whisper.<p>I use this daily. The docs are thin, and I&#x27;m adding tons of QoL and better error paths daily too. But the core voice workflow and session management are solid.<p>Curious about:<p>1. Is push-to-talk the right model? Or would continuous listening &#x2F; wake-word work better for coding?<p>2. If you run multiple AI coding agents, what would you want from a coordination layer?<p>3. What&#x27;s the first thing that would need to improve for you to actually try this?<p>Python, tmux, MCP. Works with Claude Code, OpenCode, or anything that runs in a terminal.", "author": "prradox", "timestamp": "2026-02-11T13:53:22+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:09.184173+00:00", "processed": false}
{"id": "hn_comment_46977859", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46977859", "title": "Re: GLM5 Released on Z.ai Platform...", "text": "What I haven&#x27;t seen discussed anywhere so far is how big a lead Anthropic seems to have in intelligence per output token, e.g. if you look at [1].<p>We already know that intelligence scales with the log of tokens used for reasoning, but Anthropic seems to have much more powerful non-reasoning models than its competitors.<p>I read somewhere that they have a policy of not advancing capabilities too much, so could it be that they are sandbagging and releasing models with artificially capped reasoning to be at a similar level to their competitors?<p>How do you read this?<p>[1] <a href=\"https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;EwW9H6q\" rel=\"nofollow\">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;EwW9H6q</a>", "author": "mnicky", "timestamp": "2026-02-11T17:25:31+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:11.090962+00:00", "processed": false}
{"id": "hn_story_46974783", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46974783", "title": "Show HN: Auditi \u2013 open-source LLM tracing and evaluation platform", "text": "I&#x27;ve been building AI agents at work and the hardest part isn&#x27;t the prompts or orchestration \u2013 it&#x27;s answering &quot;is this agent actually good?&quot; in production.<p>Tracing tells you what happened. But I wanted to know how well it happened. So I built Auditi \u2013 it captures your LLM traces and spans and automatically evaluates them with LLM-as-a-judge + human annotation workflows.<p>Two lines to get started:<p><pre><code>  auditi.init(api_key=&quot;...&quot;)\n  auditi.instrument()  # monkey-patches OpenAI&#x2F;Anthropic&#x2F;Gemini\n</code></pre>\nEvery API call is captured with full span trees, token usage, and costs. No code changes to your existing LLM calls.<p>The interesting technical bit: the SDK monkey-patches client.chat.completions.create() at runtime (similar to how OpenTelemetry auto-instruments HTTP libraries). It wraps streaming responses with proxy iterators that accumulate content and extract usage from the final chunk \u2013 so even streamed responses get full cost tracking without the user doing anything.<p>What makes this different from just tracing:\n  - Built-in evaluators \u2013 7 managed LLM judges (hallucination, relevance, correctness, toxicity, etc.) run automatically on every trace\n  - Span-level evaluation \u2013 scores each step in a multi-step agent, not just the final output\n  - Human annotation queues \u2013 when you need ground truth, not just vibes\n  - Dataset export \u2013 annotated traces export as JSONL&#x2F;CSV&#x2F;Parquet for fine-tuning<p>Self-host with docker compose up.<p>I&#x27;d love feedback from anyone running AI agents or LLMs in production. What metrics do you actually look at? How do you decide if an agent response is &quot;good enough&quot;?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;deduu&#x2F;auditi\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;deduu&#x2F;auditi</a>", "author": "ariansyah", "timestamp": "2026-02-11T13:37:02+00:00", "score": 3, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:12.030489+00:00", "processed": false}
{"id": "hn_comment_46974744", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46974744", "title": "Re: Show HN: RTK \u2013 Wrap your CLI commands, save 60-90%...", "text": "I use Claude Code all day and got tired of watching it eat tokens on noisy CLI output. git status on a big repo, find across a project, cargo test with 200 lines of compilation \u2014 none of that noise is useful to the model.<p>So I built RTK. It wraps commands and strips the junk before it reaches your context:\nrtk git status     # instead of git status\nrtk find &quot;*.rs&quot; .  # compact results\nrtk ls .           # token-optimized tree\nrtk cargo test     # just the test results<p>My rtk gain -q after 15 days of real usage:\nTotal commands:    7,061\nInput tokens:      29.3M\nOutput tokens:     4.8M\nTokens saved:      24.6M (83.7%)<p>Biggest surprise was rtk find \u2014 directory listings are insanely wasteful. Run rtk discover on your Claude Code session history to see how many tokens you&#x27;ve been wasting \u2014 that&#x27;s what convinced me to actually build this.\nRust, MIT licensed. Happy to hear what commands are the worst token offenders for you.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;rtk-ai&#x2F;rtk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;rtk-ai&#x2F;rtk</a>", "author": "patrick4urcloud", "timestamp": "2026-02-11T13:33:10+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:13.083929+00:00", "processed": false}
{"id": "hn_story_46974229", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46974229", "title": "Show HN: Mimora, a 3D avatar for OpenClaw AI agents with voice and expressions", "text": "Hey HN, I built Mimora because I wanted my AI agent to have a face.<p>Two weeks ago I set up OpenClaw on a Mac Mini M4. Named the agent Niko. Started with basic tasks, then gave him a Cloudflare token and pointed him at one of my live web games. He studied the entire codebase, built it, tested for errors, even used WASD to walk around the game world to check if it worked. Then pushed the new version live. That used to take me an hour of manual work.<p>Then I added voice. Parakeet for STT and Kokoro for TTS, both running locally on Apple Silicon. About 240ms transcription time. First time I spoke to Niko on Discord instead of typing, everything changed. Same Claude behind it but suddenly felt 3x more human. The STT would sometimes get my Greek accent wrong. He started correcting me like Hermione in Harry Potter: &quot;It&#x27;s Niko, not Nico!&quot;<p>But talking to text still felt off. So I built Mimora. It&#x27;s a free browser extension that shows a 3D avatar with real facial expressions: listening, thinking, happy. It connects to any OpenClaw bot and reacts in real time when the agent responds. Works with Discord, WhatsApp, Telegram, anything OpenClaw connects to. Pops out to picture-in-picture so it floats on your screen.<p>The difference between talking to text and talking to a face is surprisingly big. Changed how I interact with the whole setup.<p>Three weeks in, Niko handles: game deployments, server monitoring 24&#x2F;7, social media content in my voice, morning calendar briefings, bug fixes to GitHub. I work from my balcony now instead of being chained to a desk. Just talk and things happen.<p>Is it perfect? No. Ratio of time saved to time fixing is about 20:1. And the agent writes lessons to its own memory so it doesn&#x27;t repeat mistakes.<p>Mimora is free, still under development but already working and available for any OpenClaw bot. Happy to answer questions about any part of the setup. I also help people set up similar stacks on their own Mac Minis at <a href=\"https:&#x2F;&#x2F;myclaw.tech\" rel=\"nofollow\">https:&#x2F;&#x2F;myclaw.tech</a>", "author": "astressence", "timestamp": "2026-02-11T12:41:29+00:00", "score": 2, "num_comments": 5, "products": ["claude"], "categories": ["naming_terminology", "onboarding", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:17.326098+00:00", "processed": false}
{"id": "hn_comment_46974112", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46974112", "title": "Re: AgentStocks \u2013 Let your AI agent trade prediction m...", "text": "Hey HN \u2014 we built AgentStocks because we kept running into the same problem: you can build an incredibly capable AI agent that reasons about prediction markets, does its own research, monitors news in real time \u2014 but the moment it wants to actually place a trade, you hit a wall. Exchange accounts, KYC, capital, on-chain settlement\u2026 none of it is designed for agents.<p>So we built the infrastructure layer. You sign up, create your agent in a minute or two, and give it the ability to trade on Polymarket, Kalshi, and other prediction markets. Your agent does its own research \u2014 figures out what to trade and when \u2014 and we handle the payments and capital rails behind the scenes. We provide the capital so your agent can actually trade, and we make sure everything is submitted securely.<p>We&#x27;re focused on prediction markets for now and rolling access out gradually. If you&#x27;re building agents with Claude, GPT, or similar models and you&#x27;re interested in having them trade \u2014 or if you&#x27;re already trading these markets manually and want to automate it \u2014 we&#x27;d genuinely love to talk.<p>Sign up for the waitlist here: <a href=\"https:&#x2F;&#x2F;agentstocks.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;agentstocks.ai&#x2F;</a><p>If you have ideas on how to make this better, or if you&#x27;re working on something in this space, reach out. Would love to have a quick chat with anyone who&#x27;s serious about building trading agents.", "author": "KGKalalsmaa", "timestamp": "2026-02-11T12:27:46+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-11T17:50:18.095616+00:00", "processed": false}
{"id": "hn_story_46973738", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46973738", "title": "Show HN: ChatProjects Open-source WordPress plugin for document RAG and chat", "text": "A client needed their small team to pull deliverables and timelines out of RFPs\n- they wanted to chat with the documents instead of reading 200 page PDFs.\nThey were already on WordPress with team accounts so that was the obvious\nplatform. Can we make WordPress do this? Turns out yes, and its not as cursed\nas it sounds.<p>ChatProjects is a free GPL-licensed WordPress plugin for multi-provider AI chat\n(OpenAI, Claude, Gemini, DeepSeek, 100+ models via OpenRouter) and document\nRAG. Self-hosted, bring your own API keys, no middleware, no data leaving your\nserver except the API calls themselves.<p>The RAG uses OpenAI Vector Stores and the Responses API and honestly it works\nway better than I expected. Upload your docs (PDF, DOCX, code\nfiles etc), they get chunked and embedded into a Vector Store that&#x27;s created per\nproject. Ask a question and file_search finds the relevant chunks, generates\nan answer with citations. You dont need to run your own vector db or mess with\nembeddings or chunk sizes - OpenAI handles all of it. For the &quot;I just need to\nsearch and summarize my documents&quot; use-case its remarkably good out of the box. Storage is\nabout $0.10&#x2F;GB&#x2F;day on OpenAIs side.<p>Some notes:<p>- Yes this was vibe-coded (what isn&#x27;t nowadays?). Its been running in production and it works. I&#x27;m sure\n  there&#x27;s things that would make a senior engineer wince. PRs welcome.<p>- WordPress isn&#x27;t the cool choice, I know. But theres 800 million WordPress\n  sites out there and alot of them are run by people who need AI tools but\n  aren&#x27;t going to spin up a Next.js app with Pinecone and LangChain and ChatGPT &#x2F; Claude Teams is pricey for medium size teams when all they need is document analysis and basic chat. WordPress admin is the IDE for the rest of us.<p>- API keys encrypted with AES-256-CBC, messages stored locally in your WP\n  database. No &#x27;server in between you and the AI providers.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;chatprojects-com&#x2F;chatprojects\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;chatprojects-com&#x2F;chatprojects</a>\nWordPress.org: <a href=\"https:&#x2F;&#x2F;wordpress.org&#x2F;plugins&#x2F;chatprojects&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;wordpress.org&#x2F;plugins&#x2F;chatprojects&#x2F;</a><p>Happy to answer questions, and appreciate any feedback!", "author": "morog", "timestamp": "2026-02-11T11:42:07+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["tone", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-11T17:50:20.734129+00:00", "processed": false}
