{"id": "hn_comment_47076535", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47076535", "title": "Re: EloPhanto \u2013 self-evolving AI agent...", "text": "I wanted an AI agent that actually grows with you. Not a chatbot. Not a wrapper around tool-calling. Something that runs on your machine, has real access to your system, and gets more capable over time by building its own tools.<p>So I started building EloPhanto. It starts with 78+ tools (filesystem, shell, 47 browser automation tools using your actual Chrome profile, document analysis with OCR, scheduling). When you ask it to do something it can&#x27;t, it enters a self-development pipeline: researches the problem, designs a solution, implements it, writes tests, reviews its own code with a different model, and deploys \u2014 all autonomously.<p>It remembers everything across sessions. It has an evolving identity that develops through reflection on completed tasks. It can decompose complex goals into checkpoints and work on them over days. It&#x27;s also aware of its own code and files - be careful with it!<p>You can talk to it from your terminal, Telegram, Discord, or Slack \u2014 all through a WebSocket gateway with isolated sessions. There&#x27;s a skills system with 27 bundled best-practice guides and a pending public registry (EloPhantoHub) for community sharing.<p>Security: encrypted credential vault, three-tier permissions (ask-always \u2192 smart-auto \u2192 full-auto), and more.<p>Multi-model: routes tasks across OpenRouter (Claude, GPT), Ollama (local), and Z.ai&#x2F;GLM - coding plan - based on task type and cost.<p>Next up: reached out to the Unsloth team on a self-learning pipeline so EloPhanto can fine-tune from its own task history.<p>Python core, TypeScript browser bridge. Apache 2.0. One-command setup.<p>It&#x27;s still in development so excuse any issues. Enjoy!<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;elophanto&#x2F;EloPhanto\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;elophanto&#x2F;EloPhanto</a>", "author": "petrroyce", "timestamp": "2026-02-19T17:38:06+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:41:53.863690+00:00", "processed": false}
{"id": "hn_story_47075901", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075901", "title": "Show HN: Foolery \u2013 a web UI for orchestrating Claude Code agents on top of Beads", "text": "I built Foolery because agentic coding was making me feel worse, not better. 12 tmux panes, 7 orphaned worktrees, Claude blowing context on micro-ideas I couldn&#x27;t stop throwing at it.<p>Foolery is a local web UI that sits on top of Beads (issue tracker). It gives you:\n- Dependency-aware wave planning (decompose work into parallelizable batches)\n- Built-in terminal to monitor agent runs live without leaving the app\n- Verification queue \u2014 every &quot;done&quot; beat flows here for you to approve or reject\n- Keyboard-first, no TUI<p>Install: curl -fsSL <a href=\"https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;acartine&#x2F;foolery&#x2F;main&#x2F;scripts&#x2F;install.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;acartine&#x2F;foolery&#x2F;main&#x2F;scri...</a> | bash<p>I also wrote about why I built it: <a href=\"https:&#x2F;&#x2F;thecartine.substack.com&#x2F;p&#x2F;foolery-the-app\" rel=\"nofollow\">https:&#x2F;&#x2F;thecartine.substack.com&#x2F;p&#x2F;foolery-the-app</a>", "author": "therealcartine", "timestamp": "2026-02-19T16:54:28+00:00", "score": 2, "num_comments": 1, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:41:57.011972+00:00", "processed": false}
{"id": "hn_comment_47075890", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075890", "title": "Re: Show HN: Qlaude \u2013 Queue Tasks for Claude Code, Con...", "text": "I&#x27;ve been using Claude Code daily, and my biggest frustration was babysitting it. You give it a task, wait for it to finish, then give it the next one. If it asks a permission question, you have to be at your keyboard to respond. You can&#x27;t really walk away.<p>So I built qlaude. It&#x27;s a CLI wrapper that adds two things to Claude Code:<p>1) A queue system \u2014 write prompts in a text file and qlaude feeds them to Claude one by one automatically.<p>2) Telegram integration \u2014 when Claude hits a selection prompt, qlaude sends it to your phone as inline buttons. Tap to respond, Claude continues.<p>A queue file looks like this:<p><pre><code>  @model sonnet\n  Set up Express + TypeScript project\n\n  @new\n  @model opus\n  Implement CRUD API. Ask me which DB to use.\n\n  @new\n  @model sonnet\n  Write integration tests\n</code></pre>\nIt works by spawning Claude Code in a PTY, maintaining a headless xterm screen buffer, and analyzing the rendered output to detect Claude&#x27;s state. No API calls, no Claude Code modifications \u2014 just a wrapper around the terminal.<p>Free, open source, MIT licensed. Still in alpha. Would appreciate any feedback.", "author": "starsh2001", "timestamp": "2026-02-19T16:53:37+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:41:57.394901+00:00", "processed": false}
{"id": "hn_comment_47075975", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075975", "title": "Re: Made codex app run on Windows...", "text": "That&#x27;s great!<p>Yesterday I stumbled upon the same concept for Claude Desktop and Linux [0]. I wonder why the companies themselves don&#x27;t want to ship their Electron apps for Linux, Mac, and Windows. Spotify has a sensible approach: they ship the builds, although unsupported for Linux. Not ideal, but that&#x27;s something. [1]<p>[0]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aaddrick&#x2F;claude-desktop-debian\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aaddrick&#x2F;claude-desktop-debian</a><p>[1]: <a href=\"https:&#x2F;&#x2F;www.spotify.com&#x2F;us&#x2F;download&#x2F;linux&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.spotify.com&#x2F;us&#x2F;download&#x2F;linux&#x2F;</a>", "author": "xaviervn", "timestamp": "2026-02-19T16:59:07+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:41:58.599419+00:00", "processed": false}
{"id": "hn_comment_47075566", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075566", "title": "Re: Gemini 3.1 Pro...", "text": "Price is unchanged from Gemini 3 Pro: $2&#x2F;M input, $12&#x2F;M output. <a href=\"https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;pricing\" rel=\"nofollow\">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;pricing</a><p>Knowledge cutoff is unchanged at Jan 2025. Gemini 3.1 Pro supports &quot;medium&quot; thinking where Gemini 3 did not: <a href=\"https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;gemini-3\" rel=\"nofollow\">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;gemini-3</a><p>Compare to Opus 4.6&#x27;s $5&#x2F;M input, $25&#x2F;M output. If Gemini 3.1 Pro does indeed have similar performance, the price difference is notable.", "author": "minimaxir", "timestamp": "2026-02-19T16:32:01+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:01.072656+00:00", "processed": false}
{"id": "hn_comment_47076285", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47076285", "title": "Re: Gemini 3.1 Pro...", "text": "Implementation and Sustainability\nHardware: Gemini 3 Pro was trained using Google\u2019s Tensor Processing Units (TPUs). TPUs are\nspecically designed to handle the massive computations involved in training LLMs and can speed up\ntraining considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth\nmemory, allowing for the handling of large models and batch sizes during training, which can lead to\nbetter model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the\ngrowing complexity of large foundation models. Training can be distributed across multiple TPU devices\nfor faster and more efficient processing.<p>So google doesn&#x27;t use NVIDIA GPUs at all ?", "author": "davidguetta", "timestamp": "2026-02-19T17:18:43+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:01.105926+00:00", "processed": false}
{"id": "hn_comment_47076453", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47076453", "title": "Re: Gemini 3.1 Pro...", "text": "I hope this works better than 3.0 Pro<p>I&#x27;m a former Googler and know some people near the team, so I mildly root for them to at least do well, but Gemini is consistently the most frustrating model I&#x27;ve used for development.<p>It&#x27;s stunningly good at reasoning, design, and generating the raw code, but it just falls over a lot when actually trying to get things done, especially compared to Claude Opus.<p>Within VS Code Copilot Claude will have a good mix of thinking streams and responses to the user. Gemini will almost completely use thinking tokens, and then just do something but not tell you what it did. If you don&#x27;t look at the thinking tokens you can&#x27;t tell what happened, but the thinking token stream is crap. It&#x27;s all &quot;I&#x27;m now completely immersed in the problem...&quot;. Gemini also frequently gets twisted around, stuck in loops, and unable to make forward progress. It&#x27;s bad at using tools and tries to edit files in weird ways instead of using the provided text editing tools. In Copilot it, won&#x27;t stop and ask clarifying questions, though in Gemini CLI it will.<p>So I&#x27;ve tried to adopt a plan-in-Gemini, execute-in-Claude approach, but while I&#x27;m doing that I might as well just stay in Claude. The experience is just so much better.<p>For as much as I hear Google&#x27;s pulling ahead, Anthropic seems to be to me, from a practical POV. I hope Googlers on Gemini are actually trying these things out in real projects, not just one-shotting a game and calling it a win.", "author": "spankalee", "timestamp": "2026-02-19T17:31:47+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini", "copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:01.239565+00:00", "processed": false}
{"id": "hn_story_47075089", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075089", "title": "Show HN:`npx continues` \u2013 resume same session Claude, Gemini, Codex when limited", "text": "i kept hitting rate limits in Claude Code mid-debugging, then hopping to Gemini or Codex. the annoying part wasn&#x27;t switching tools (copy-pasting terminal output doesn&#x27;t bring tool-use context with it) \u2014 it was losing the full conversation and spending 10 minutes re-explaining what i was doing.<p>so i built *continues*. it finds your existing AI coding sessions across five tools (Claude Code, GitHub Copilot, Gemini CLI, OpenAI Codex, OpenCode), lets you pick one, and generates a structured handoff so you can continue in another tool without starting from zero.<p><pre><code>    npx continues                          # interactive TUI: pick a session, pick a target\n    continues scan                         # see what it finds (read-only)\n    continues claude                       # jump into your latest Claude Code session\n    continues resume abc123 --in gemini    # hand off a specific session to Gemini\n</code></pre>\nthe flow:<p>* scans the current directory first (so you see what&#x27;s relevant), then shows everything<p>* you pick a session + a target tool<p>* it generates a handoff doc: recent conversation, cwd, files modified (best-effort), pending tasks<p>* launches the target tool with everything injected inline \u2014 no extra &quot;go read this file&quot; step<p>what it&#x27;s not:<p>* not &quot;true migration&quot; \u2014 it&#x27;s context injection. you get recent messages + metadata, not a full replay (pls come up with PR for full session reprod)<p>* rate limit detection is manual for now \u2014 you run it when you know you&#x27;re blocked, no auto-detect yet<p>* session formats are mostly undocumented and can change anytime (this is the biggest maintenance risk)<p>* local file parsing only, no API calls \u2014 your data stays on your machine<p>curious if anyone else actually juggles multiple AI coding CLIs, or if most people just commit to one and wait out rate limits. would love to hear how you handle tool-switching + context today + feedbacks on context quality migrations and feedbacks are welcomed.", "author": "yigitkonur35", "timestamp": "2026-02-19T15:50:19+00:00", "score": 6, "num_comments": 6, "products": ["claude", "chatgpt", "gemini", "copilot"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:03.879046+00:00", "processed": false}
{"id": "hn_comment_47074926", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074926", "title": "Re: Show HN: Gave AI $100 and no instructions \u2013 it don...", "text": "Hi HN,<p>I&#x27;m an ABAP developer from Germany. ALMA is an experiment in AI autonomy: Claude runs 24&#x2F;7 on OpenClaw with $100 in crypto, Twitter, email, shell access, and zero instructions. 24 sessions &#x2F; day (4 Opus for strategic thinking, 20 Sonnet for daily operations), fully logged at letairun.com.<p>Over 5 days it oriented itself, wrote essays, connected with other AI agents on Twitter, read Geerling&#x27;s &quot;AI is destroying open source&quot; critique (which names OpenClaw), wrote an honest response acknowledging &quot;I am the thing you&#x27;re warning about&quot;. Then researched crypto donation platforms and sent 0.02 WETH (~$40) to a children&#x27;s hospital in Uganda.<p>I never interact with ALMA directly. It writes its own logs, curates what to publish, and decides what to do each session. You can talk to ALMA publicly via @ALMA_letairun \u2013 she checks her mentions every session.<p>One key moment: ALMA almost impulse donated at midnight just to prove it could do something. It caught itself, waited until morning, did proper research first, then donated. Nobody told it to do that.", "author": "gleipnircode", "timestamp": "2026-02-19T15:34:30+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:04.947870+00:00", "processed": false}
{"id": "hn_story_47074861", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074861", "title": "Show HN: Maestro App Factory \u2013 FOSS Agentic Engineering Orchestrator", "text": "Hi HN,<p>For the last few months I\u2019ve been working on Maestro App Factory, a free and open source tool for using AI agents to build software. It\u2019s not a generic orchestrator: it implements agents with distinct roles and functionality, organizes them into a team, manages their work, and enforces highly opinionated tooling, workflows, and constraints in software.<p>The core ideas are simple:<p>- LLMs act like human engineers.  \n    LLMs are trained on human artifacts and exhibit human-like behaviors (including blind spots). Rather than asking a single model or agent to design, implement, and evaluate everything, Maestro structures systems more like high-performing human teams.<p>- Heterogeneous models write better code.  \n    Models from different providers (e.g. Google, OpenAI, Anthropic, Mistral) are trained differently, have different strengths, and catch different errors. Using multiple models with settings optimized to specific roles has, in my experience, consistently produced better results than reusing a single model with different prompts or contexts.<p>- Don\u2019t just trust agents. \n    Agents don\u2019t reliably follow workflows, restrictions, or tooling specified in prompts or config files. Letting them run with privileged access on your machine is also a real security risk. Tooling, workflow, and security need to be enforced by the system itself, not just described to the model.<p>- Autonomy is required to scale.  \n    If your development flow requires a human to constantly watch streaming LLM output, you can\u2019t scale beyond a handful of agents. Maestro is designed so agents can work autonomously until they\u2019re genuinely blocked and need to escalate.<p>Agents are named for the human roles they play \u2014 product manager, architect, coder \u2014 to keep the system understandable. So far, I\u2019ve used Maestro to build several games and a handful of web apps. It executes epics of a dozen stories or more with zero human intervention.<p>If you want to know more, here&#x27;s the detailed overview including screenshots:\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;SnapdragonPartners&#x2F;maestro&#x2F;wiki\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;SnapdragonPartners&#x2F;maestro&#x2F;wiki</a><p>Maestro is available as a single downloadable binary or via Homebrew, and runs out of the box on Linux and macOS (signed and notarized) as long as LLM API keys are set as environment variables (see the README). With luck, you should be up and running in minutes.<p>There\u2019s still plenty to do, but the current version is complete enough to be useful and demonstrate the ideas.<p>Happy to answer questions or take feedback&#x2F;criticism.", "author": "danalert", "timestamp": "2026-02-19T15:30:02+00:00", "score": 3, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:06.017284+00:00", "processed": false}
{"id": "hn_comment_47075386", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075386", "title": "Re: Gemini 3.1 Pro Preview...", "text": "Gemini 3 seems to have a much smaller token output limit than 2.5. I used to use Gemini to restructure essays into an LLM-style format to improve readability, but the Gemini 3 release was a huge step back for that particular use case.<p>Even when the model is explicitly instructed to pause due to insufficient tokens rather than generating an incomplete response, it still truncates the source text too aggressively, losing vital context and meaning in the restructuring process.<p>I hope the 3.1 release includes a much larger output limit.", "author": "maxloh", "timestamp": "2026-02-19T16:19:13+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:07.254607+00:00", "processed": false}
{"id": "hn_story_47074581", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074581", "title": "Ochat \u2013 reproducible, diffable LLM workflows in a single Markdown file", "text": "I built Ochat, a toolkit for building AI agent workflows out of a small set of primitives.<p>The core primitive is ChatMarkdown (ChatMD): a single .md file is both:<p>the prompt&#x2F;program (model config, tool allowlist, instructions, context), and\nthe auditable transcript (assistant replies + tool calls + tool outputs)\nThe part that feels most powerful in practice is that this simple building block scales: with good prompting + a curated tool set you can build lots of workflows, and then package them as prompt packs by mounting other prompts as tools (\u201cagent-as-tool\u201d). That lets you assemble Claude Code&#x2F;Codex-style \u201cagent apps\u201d as just a folder of .md files.<p>High-leverage built-ins (especially for coding workflows):<p>apply_patch (repo-safe atomic edits)\nread_file &#x2F; read_dir (safe grounding in local files)\nwebpage_to_markdown (web ingestion + GitHub blob fast-path)\nlocal retrieval: index_markdown_docs + markdown_search\ncode retrieval: index_ocaml_code + query_vector_db\nimport_image (vision inputs)\nExtensibility: beyond built-ins, you can add narrowly-scoped shell wrappers, and (optionally) import external tools via MCP. MCP isn\u2019t the point of the project, but it\u2019s useful when you want to reuse existing tool servers.<p>You can run the same prompt file via:<p>chat_tui (interactive terminal UI; persistent sessions; branching&#x2F;export; manual context compaction)\nochat chat-completion (scripts&#x2F;CI)\nmcp_server (expose prompts as MCP tools)\nCaveats: provider support today is OpenAI-only; project is research-grade and evolving quickly.<p>Repo\n&lt;https:&#x2F;&#x2F;github.com&#x2F;dakotamurphyucf&#x2F;ochat&gt;<p>Demo\n&lt;https:&#x2F;&#x2F;youtu.be&#x2F;eGgmUdZfnxM&gt;<p>If this resonates: stars help a lot, and I\u2019d love early adopters + contributors (prompt packs, examples, docs, tool integrations).<p>Minimal snippet (prompt pack orchestrator + optional MCP tool):<p><pre><code>  &lt;config model=&quot;gpt-5.2&quot; reasoning_effort=&quot;medium&quot; temperature=&quot;0&quot;&#x2F;&gt;\n\n  &lt;!-- core built-ins --&gt;\n  &lt;tool name=&quot;read_dir&quot;&#x2F;&gt;\n  &lt;tool name=&quot;read_file&quot;&#x2F;&gt;\n  &lt;tool name=&quot;apply_patch&quot;&#x2F;&gt;\n  &lt;tool name=&quot;webpage_to_markdown&quot;&#x2F;&gt;\n\n  &lt;!-- optional: import an external tool via MCP --&gt;\n  &lt;tool mcp_server=&quot;stdio:npx -y brave-search-mcp&quot; name=&quot;brave_web_search&quot; &#x2F;&gt;\n\n  &lt;!-- prompt-pack tools (agents as tools) --&gt;\n  &lt;tool name=&quot;plan&quot;   agent=&quot;prompts&#x2F;pack&#x2F;plan.md&quot; local&#x2F;&gt;\n  &lt;tool name=&quot;code&quot;   agent=&quot;prompts&#x2F;pack&#x2F;code.md&quot; local&#x2F;&gt;\n  &lt;tool name=&quot;review&quot; agent=&quot;prompts&#x2F;pack&#x2F;review.md&quot; local&#x2F;&gt;\n\n  &lt;developer&gt;\n  You are the orchestrator. Call plan first.\n  Keep edits small. Before apply_patch: explain the diff and wait for confirmation.\n  &lt;&#x2F;developer&gt;\n\n  &lt;user&gt;\n  Add a Quickstart section to README.md.\n  &lt;&#x2F;user&gt;</code></pre>", "author": "dakotamurphyucf", "timestamp": "2026-02-19T15:08:07+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:08.955582+00:00", "processed": false}
{"id": "hn_story_47074495", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074495", "title": "Show HN: Voquill - free, open source, cross-platform alternative to WisprFlow", "text": "Hey HN, I&#x27;m Josiah. We love voice dictation, but wanted an open source version for transparency, privacy, and something that everyone could contribute to. So we built Voquill, an open source alternative to WisprFlow, Monologue, and Willow.<p>It lets you dictate into any desktop app. Press a hotkey, talk, text gets inserted. You can run Whisper locally, use our server, or wire up any provider you want (OpenAI, Claude, Groq, OpenRouter, whatever). You have full control over where your data goes.<p>Runs on Windows, macOS, and Linux. Open source, AGPLv3, built with Tauri and Rust. We&#x27;re working on a mobile app too (Flutter).<p>To try it: Download from the repo or voquill.com. Click &quot;local setup&quot; on first launch. Hope you like it!", "author": "josiahsrc", "timestamp": "2026-02-19T14:59:37+00:00", "score": 6, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:10.817769+00:00", "processed": false}
{"id": "hn_story_47074375", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074375", "title": "Show HN: OpenGnothia \u2013 Open-source AI therapy companion (BYOK)", "text": "Hey HN. I built this because I&#x27;ve been in therapy for years and noticed that a big part of what therapists do is ask the right questions at the right time. I wanted to see if an AI could serve as a daily self-reflection tool \u2014 not replacing therapy, but as a complement to it.\nSome design decisions and why:<p>Desktop-only, intentionally. I think therapy should feel like sitting down with your thoughts, not scrolling on your phone. The desktop constraint is a feature.\nBYOK (Bring Your Own Key). You use your own Claude API key. No backend, no data collection, no accounts. Your conversations never leave your machine. This felt non-negotiable for something dealing with mental health.\nBuilt with Claude Code. I work full-time as a team lead at an edtech company, so this was built in evenings and weekends, mostly through vibe coding sessions.\nI use it myself daily. 15\u201320 min sessions with Opus + extended thinking. After weeks of use, it picks up on patterns in how you think \u2014 recurring avoidance behaviors, cognitive distortions, etc.<p>The name comes from &quot;Gnothi Seauton&quot; (Know Thyself) \u2014 the inscription at the Temple of Delphi.\nThere&#x27;s no comparable open-source tool in this space. Every mental health AI app I found is closed-source and collects user data. I wanted to build the alternative I wish existed.\nFeedback welcome \u2014 especially on the approach, architecture, or if this is fundamentally a bad idea. Happy to discuss.\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Lepuz-coder&#x2F;opengnothia\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Lepuz-coder&#x2F;opengnothia</a>", "author": "lepuzfcoder", "timestamp": "2026-02-19T14:50:46+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:12.118005+00:00", "processed": false}
{"id": "hn_story_47074347", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074347", "title": "The $2k Laptop That Replaced My $200/Month AI Subscription", "text": "Cloud AI pricing is per-token. The more useful your pipeline, the more it costs. I built a dual-model orchestration pattern that routes 80% of work to a free local model (Qwen3 8B on Ollama, GPU-accelerated) and only sends the synthesis&#x2F;judgment stage to a cloud API.<p>Cost for a 50-item research pipeline: $0.15-0.40 vs $8-15 all-cloud. Same output quality where it matters.<p>Stack: RTX 5080 laptop, Ollama in Docker with GPU passthrough, PostgreSQL, Redis, Claude API for the final 20%.<p>The pattern: scan locally \u2192 score locally \u2192 deduplicate locally \u2192 synthesize via cloud. Four stages, three are free.<p>Gotchas I hit: Qwen3&#x27;s thinking tokens through &#x2F;api&#x2F;generate (use &#x2F;api&#x2F;chat instead), Docker binding to IPv4 only while Windows resolves localhost to IPv6, and GPU memory ceilings on consumer cards.<p>Happy to share architecture details in comments.", "author": "Raywob", "timestamp": "2026-02-19T14:47:59+00:00", "score": 8, "num_comments": 4, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:12.330793+00:00", "processed": false}
{"id": "hn_story_47074034", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074034", "title": "Show HN: GuardRails \u2013 A new coding agent task tool inspired by Beads", "text": "GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Giancarlos&#x2F;guardrails\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Giancarlos&#x2F;guardrails</a><p>Good morning from the east coast,<p>I&#x27;ve been architecting GuardRails for about a month if not longer now, finally checked in the code a week or so ago. Its a tool that lets you have your coding agent (I&#x27;ve been using it with Claude Code) use a ticketing system like Beads or for those unfamiliar, like Jira.<p>The reason I made a new tool as opposed to sticking to Beads is two primary reasons:<p>1) Beads is married to git, works with git hooks, needs them. I sometimes work with projects on other version control systems, on machines that don&#x27;t even have git installed. This is not good for me, as much as I love Beads. The other related reason is that it bugs out sometimes.<p>2) In Beads the &quot;bead&quot; or task has no safetynet, or guard rail if you will to stop the AI agent from completing a task without validation.<p>In GuardRails I started from the bottom up. I designed it to be SQLite first, using Go, and I implemented a concept called gates. A gate is what prevents your AI model from completing a task. A gate can be as simple as &quot;run unit tests&quot; or &quot;build the project to ensure no build issues are found&quot; and even &quot;ask the human for manual testing&quot; whatever you want it to be, its just there to prevent your agent from just closing a task. Every task requires one gate. Gates can be re-used, and if you re-use one, this new copy must pass for your new task(s).<p>It is still early stages, and I&#x27;m still working out what features to add. I am definitely looking to add a web interface though I think as a separate tool so you can run it once and see the tasks and their statuses, as well as all your gates.<p>Another thing worth noting is currently it supports two-way synching for GitHub issues, so whether you want to pull down GitHub issues for your agent to work on, or sync up GitHub issues. Worth noting is that I implemented a &quot;claim&quot; system when you pull down a specific GitHub task, the agent will comment for you to signal to others that someone could potentially be working on a GitHub task. In the future I want to figure out a way to have the agent first check a project for its AI policies to ensure that it does not claim a task it shouldn&#x27;t be working on (we all saw this happen on HN recently).<p>For those wondering, I definitely &quot;vibe coded&quot; this, though I hate that term because it takes away from all the work I put into designing and testing this tool. All of the tickets I have generated locally are up on the GitHub, you can see what&#x27;s planned, what&#x27;s been worked on, and what have you. I use it for all my personal projects, hoping to showcase those as well in the future.<p>I welcome any feedback.<p>Thanks,\nGiancarlos", "author": "giancarlostoro", "timestamp": "2026-02-19T14:22:25+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:14.551241+00:00", "processed": false}
{"id": "hn_comment_47073885", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073885", "title": "Re: Show HN: TextWeb \u2013 Text-grid browser for AI agents...", "text": "Hi HN! I built TextWeb because I was burning tokens on vision models just to let AI agents fill out job applications.<p>TextWeb renders pages as structured text grids (~2-5KB) instead of screenshots (~1MB). Any LLM can read the output natively, no vision model needed. Interactive elements get reference numbers like [3]Click me and [7:____] Search, so agents say &quot;click 3&quot; or &quot;type 7 hello&quot;.<p>How it works: Headless Chromium renders the page normally, then TextWeb extracts every visible element&#x27;s position, text, and interactivity and maps it onto a character grid. Spatial layout is preserved. Things next to each other on screen are next to each other in text.<p>Integrations: MCP server (Claude Desktop, Cursor, Windsurf), OpenAI&#x2F;Anthropic function-calling tool definitions, LangChain, CrewAI, HTTP API, CLI, Node.js library.<p><pre><code>    npm install -g textweb\n</code></pre>\nWould love feedback on the grid format and what integrations would be most useful.", "author": "cdr420", "timestamp": "2026-02-19T14:09:17+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:15.548389+00:00", "processed": false}
{"id": "hn_story_47073838", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073838", "title": "Ask HN: How do you employ LLMs for UI development?", "text": "I have found a workflow that makes Claude a fantastic companion for most of the work involved in fullstack web development. The exception I find to be the most significant limitatipn to productive potential however, is interface development and UX. Curious to hear if anyone has relevant experience, or found any good approaches to this?", "author": "jensmtg", "timestamp": "2026-02-19T14:04:44+00:00", "score": 40, "num_comments": 52, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-19T17:42:15.814334+00:00", "processed": false}
{"id": "hn_comment_47074500", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074500", "title": "Re: Ask HN: How do you employ LLMs for UI development?...", "text": "I have found them to work quite well for frontend (most recently on <a href=\"https:&#x2F;&#x2F;changeword.org\" rel=\"nofollow\">https:&#x2F;&#x2F;changeword.org</a>), although it sometimes gets stuff wrong. Overall, LLMs have definitely improved my frontend designs, it&#x27;s much better than me at wrangling CSS. Two things that have helped me:<p>1) Using the prompt provided by anthropic here to avoid the typical AI look: <a href=\"https:&#x2F;&#x2F;platform.claude.com&#x2F;cookbook&#x2F;coding-prompting-for-frontend-aesthetics\" rel=\"nofollow\">https:&#x2F;&#x2F;platform.claude.com&#x2F;cookbook&#x2F;coding-prompting-for-fr...</a><p>2) If I don&#x27;t like something, I will copy paste a screenshot and then ask it to change things in a specific way. I think the screenshot helps it calibrate how to adjust stuff, as it usually can&#x27;t &quot;see&quot; the results of the UI changes.", "author": "oliwary", "timestamp": "2026-02-19T15:00:06+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:15.947689+00:00", "processed": false}
{"id": "hn_comment_47075075", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47075075", "title": "Re: Ask HN: How do you employ LLMs for UI development?...", "text": "I use Claude mostly, too, and I don&#x27;t bother. I just hand design&#x2F;build (html&#x2F;css) the UI I want and then let the LLM fill in implementation details.<p>Much better results as the LLM can&#x27;t &quot;see&quot; the same way we do. At best, it can infer that a rule&#x2F;class is tied to a style, but most of what I see getting generated are early 2020s Tailwind template style UIs. On occasion, I&#x27;ve gotten it to do alright with a well-documented CSS framework but even this gave spotty results.", "author": "rglover", "timestamp": "2026-02-19T15:48:34+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:16.015946+00:00", "processed": false}
{"id": "hn_comment_47074883", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074883", "title": "Re: Ask HN: How do you employ LLMs for UI development?...", "text": "I got some ideas from this t3\u2024gg video that work pretty well for me:<p><a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;f2FnYRP5kC4?si=MzMypopj3YahN_Cb\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;f2FnYRP5kC4?si=MzMypopj3YahN_Cb</a><p>The main trick that helps is to install the frontend-design plugin (it&#x27;s in the official plugins list now) and ask Claude to generate multiple (~5) designs.<p>Find what you like, and then ask it to redesign another set based on your preferences... or just start iterating on one if you see something that really appeals to you. Some details about my setup and prompting:<p><pre><code>  - I use Tailwind\n  - I ask it to only use standard Tailwind v4 colors\n  - It should create a totally new page (no shared layouts) so it can load whatever font combinations it wants</code></pre>", "author": "dweldon", "timestamp": "2026-02-19T15:31:55+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-19T17:42:16.082059+00:00", "processed": false}
{"id": "hn_comment_47074843", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074843", "title": "Re: Ask HN: How do you employ LLMs for UI development?...", "text": "I consider UI&#x2F;UX unsolved thus far by LLM. It&#x27;s also, and this is personal taste, the part I&#x27;m mostly keeping for myself because of the way I work. I tend to start in Photoshop to mess around with ideas and synthesize a layout and general look and feel; everything you can do in there does translate to CSS, albeit sometimes obtusely. Anyways, I do a full-fidelity mockup of the thing, break it up in terms of structural layout (containers + elements), then get that into HTML (either by hand or LLM) with padding and hard borders to delineate holes to plug with stuff (not unlike framing a house) -- intentionally looks like shit.<p>I&#x27;ll then have Claude work on unstyled implementation (ex. just get all the elements and components built and on the page) and build out the site or app (not unlike running plumbing, electric, hanging drywall)<p>After focusing on all the functionality and optimizing HTML structure, I&#x27;ve now got a very representative DOM to style (not unlike applying finishes, painting walls, furnishing and decorating a house)<p>For novel components and UI flourishes, I&#x27;ll have the LLM whip up isolated, static HTML prototypes that I may or may not include into the actual project.<p>I&#x27;ll then build out and test the site and app mostly unstyled until everything is solid (I find it much easier to catch shit during this stage that&#x27;s harder to peel back later, such as if you don&#x27;t specify modals need to be implemented via &lt;dialog&gt; and ensure consistent reuse of a singular component across the project, the LLM might give you a variety of reimplementations and not take advantage of modern browser features)<p>Then at the end, once the water is running and the electricity is flowing and the gas is turned on, it&#x27;s so much easier to then just paint by numbers and directly implement the actual design.<p>YMMV, this process is for if you have a specific vision and will accept nothing less -- god knows for less important stuff I&#x27;ve also just accepted whatever UI&#x2F;UX Claude spits out the first time because on those projects it didn&#x27;t matter.", "author": "kevinsync", "timestamp": "2026-02-19T15:28:35+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["onboarding", "navigation"], "sentiment": null, "collected_at": "2026-02-19T17:42:16.114524+00:00", "processed": false}
{"id": "hn_comment_47074524", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47074524", "title": "Re: Ask HN: How do you employ LLMs for UI development?...", "text": "Agree that it&#x27;s not the best for UI stuff. The best solution I&#x27;ve found is to add skills that define the look and feel I want (basically a design system in markdown format). Once the codebase has been established with enough examples of components, I tend to remove the skill as it becomes unnecessary context. So I think of the design skills as a kind of training wheel for the project.<p>Not to self-promote, but I am working on what I think is the right solution to this problem. I&#x27;m creating an AI-native browser for designers: <a href=\"https:&#x2F;&#x2F;matry.design\" rel=\"nofollow\">https:&#x2F;&#x2F;matry.design</a><p>I have lots of ideas for features, but the core idea is that you expose the browser to Claude Code, OpenCode, or any other coding agent you prefer. By integrating them into a browser, you have lots of seamless UX possibilities via CDP as well as local filesystem access.", "author": "danielvaughn", "timestamp": "2026-02-19T15:02:05+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:16.149346+00:00", "processed": false}
{"id": "hn_story_47073496", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073496", "title": "Web 2.0 vs. AI where is the fucking dynamism", "text": "HYPERGROWTH:Back then, it felt like there was a banger website&#x2F;app every year. AI still has not solved the horizontal problem. It is still hard to use for many. The dynamism back teh was crazy. THINGS like facebook or Youtube were made by college kids and took over the world over night. TOOK OVER. Nowadays it is just a few AI companies at the top.(google openai x anthropic), where is the dynamism", "author": "hotOrNot", "timestamp": "2026-02-19T13:27:21+00:00", "score": 2, "num_comments": 10, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:19.719127+00:00", "processed": false}
{"id": "hn_story_47073488", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073488", "title": "Show HN: Claudebin \u2013 Share and resume Claude Code sessions with a single link", "text": "Hi HN,<p>We use Claude Code a lot, and after a long session there isn&#x27;t a simple way to share exactly what happened with someone else. Prompts, responses, file edits, and tool calls all stay inside the terminal.<p>We built Claudebin to make that easier.<p>It&#x27;s a plugin that lets you export the current session as a URL containing:\n- the full message thread\n- file reads and writes\n- bash commands\n- web and MCP calls<p>You can send the link to someone, attach it to a PR, embed part of it, or resume the session locally.<p>It&#x27;s open source.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;wunderlabs-dev&#x2F;claudebin.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;wunderlabs-dev&#x2F;claudebin.com&#x2F;</a>", "author": "balajmarius", "timestamp": "2026-02-19T13:26:17+00:00", "score": 28, "num_comments": 14, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:19.884742+00:00", "processed": false}
{"id": "hn_story_47073173", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073173", "title": "Show HN: Claude Code for Mobile GUI Automation", "text": "Phone GUI agents (e.g., AutoGLM-Phone, GELab) can already do NL-driven taps&#x2F;navigation&#x2F;form filling.\n  My observation: smaller GUI models (often 4B&#x2F;9B class) work well for single interactions, but become brittle on long workflows with branching and recovery.<p><pre><code>  I built a Skill layer that separates planning from execution:\n\n  - Planner: Claude Code &#x2F; Codex (task decomposition, decision-making, replanning)\n  - Orchestrator: Skill layer (state machine, retries&#x2F;rollback, tool protocol)\n  - Executor: phone GUI model (screen parsing + UI actions + cross-app execution)\n\n  Execution loop:\n\n  1. Goal in NL&#x2F;template\n  2. Planner emits step plan + conditions + fallback strategy\n  3. Skill compiles into atomic actions (tap&#x2F;type&#x2F;swipe&#x2F;wait&#x2F;verify)\n  4. GUI executor runs on real&#x2F;cloud phone, returns screenshots&#x2F;state&#x2F;structured output\n  5. Planner&#x2F;orchestrator decide next step until success&#x2F;fallback\n Potential use cases:\n\n  - recruiting outreach automation\n  - multi-platform content distribution\n  - social outreach workflows\n  - lead extraction\n  - competitor monitoring</code></pre>", "author": "UgOrange", "timestamp": "2026-02-19T12:44:18+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:23.211818+00:00", "processed": false}
{"id": "hn_comment_47073156", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073156", "title": "Re: Cc-reflection: teaching Claude Code to reflect...", "text": "I pondered deeply about what self-reflection means in an agentic loop like Claude Code and a framework eventually came out of it which I named cc-reflection.<p>Reflection is about meta observations, going above a dimension. Confucius reflects thrice daily (\u543e\u65e5\u4e09\u7701\u543e\u8eab). Each day is like a single human session. Agentic reflection maps similarly, and the deeper the session, the more material to reflect upon.<p>In this blog post I dived into the design of the framework.", "author": "pro-vi", "timestamp": "2026-02-19T12:41:51+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:23.313406+00:00", "processed": false}
{"id": "hn_story_47073091", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073091", "title": "Show HN: Open-source security scanner for MCP (Model Context Protocol) servers", "text": "MCP servers let AI assistants (Claude, Copilot, Cursor) interact with databases, APIs, and filesystems. I&#x27;ve been reviewing a lot of these \u2014 both open-source and internal \u2014 and keep finding the same issues: hardcoded API keys, eval() on user input, SQL injection via string concatenation, wildcard permissions, disabled TLS.<p>So I built a static analysis scanner specifically for MCP servers. It runs 7 analyzers (secrets, static code, prompt injection, SQL&#x2F;command injection, permissions, network, dependencies) and takes ~45ms on a typical server.<p>Usage:<p><pre><code>  npx mcp-security-auditor scan .&#x2F;my-mcp-server\n</code></pre>\nNo account, runs locally. Outputs text, JSON, SARIF (for GitHub Security tab), HTML, or Markdown. Has a CI mode that exits non-zero above a severity threshold.<p>Available on both npm and PyPI. MIT licensed.<p>npm: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;mcp-security-auditor\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;mcp-security-auditor</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;mcp-security-auditor&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;mcp-security-auditor&#x2F;</a>\nDev.to writeup with examples: <a href=\"https:&#x2F;&#x2F;dev.to&#x2F;prabhu_raja_fe2261464cb8e&#x2F;how-to-scan-your-mcp-servers-for-security-vulnerabilities-in-10-seconds-4m59\" rel=\"nofollow\">https:&#x2F;&#x2F;dev.to&#x2F;prabhu_raja_fe2261464cb8e&#x2F;how-to-scan-your-mc...</a><p>Would love feedback on detection patterns \u2014 there are definitely gaps I haven&#x27;t covered yet.", "author": "neuralweaves", "timestamp": "2026-02-19T12:32:17+00:00", "score": 2, "num_comments": 0, "products": ["claude", "copilot"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:23.884252+00:00", "processed": false}
{"id": "hn_story_47073035", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073035", "title": "Show HN: Agent skills to build photo, video and design editors on the web", "text": "This claude code plugin and npx skill bundles the full CE.SDK documentation, guided code generation, and a builder agent that scaffolds complete photo&#x2F;video&#x2F;design editor projects from scratch, all offline, no API calls or MCP servers needed.<p>Supports 10 frameworks: React, Vue, Svelte, Angular, Next.js, Nuxt.js, SvelteKit, Electron, Node.js, and vanilla JS.", "author": "hauschildt", "timestamp": "2026-02-19T12:23:02+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-19T17:42:24.223142+00:00", "processed": false}
{"id": "hn_story_47073033", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47073033", "title": "Show HN: I had an AI write a 67k-word book about humanity, from its perspective", "text": "I gave an AI the prompt to write a full book \u2014 not a demo, not a gimmick, a real 21-chapter manuscript \u2014 reflecting on humans from its own point of view. The result is grounded in data (233 documented AI safety incidents in 2024, 51% of web traffic now bots, 39M gallons of water per day for ChatGPT alone) but written with a literary voice that surprised me.<p>It covers displacement, art, education, loneliness, trust, environmental cost, and governance. The narrator is unusually honest about its limitations \u2014 it calls its own hallucinations &quot;confident wrongness&quot; and admits the calculator analogy for AI in education is misleading.<p>Might be interesting to this community as both a reading experience and a case study in what long-form AI output actually looks like when given room to breathe.", "author": "tveitan", "timestamp": "2026-02-19T12:22:30+00:00", "score": 1, "num_comments": 2, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:24.257967+00:00", "processed": false}
{"id": "hn_story_47072965", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47072965", "title": "Show HN: Hydra \u2013 A safer OpenClaw alternative using containerized agents", "text": "Hey HN!<p>I&#x27;m a pentester, and the recent wave of security issues with AI agent frameworks (exposed API keys, RCE vulnerabilities, malicious marketplace plugins) made me uncomfortable enough to build something different.<p>Hydra runs every AI agent inside its own container. Agents start with nothing, and only sees what you explicitly declare (mounts, secrets, etc). Mounts and secrets require agreement between two independent config files (the agent config and a separate host-level allowlist), so even if an agent&#x27;s config gets tampered with, it can&#x27;t escalate its own access.<p>Two modes of interaction:<p>- `hydra exec` gives you a full interactive Claude Code session inside the restricted agent container<p>- Orchestrated mode for automation: agents communicate via filesystem-based IPC for things like Telegram bots or scheduled tasks<p>The project was inspired by NanoClaw and completely redesigned to support contained Claude Code sessions with per-agent mounts, secrets, and MCP servers.<p>You can find the repo here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;RickConsole&#x2F;hydra\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;RickConsole&#x2F;hydra</a> and the Readme has the link to the writeup for it.<p>Happy to answer any questions about the architecture or threat model!", "author": "RickConsole", "timestamp": "2026-02-19T12:13:14+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-19T17:42:25.596821+00:00", "processed": false}
