{"id": "hn_story_46194828", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46194828", "title": "Launch HN: Nia (YC S25) \u2013 Give better context to coding agents", "text": "Hi HN, I am Arlan and I am building Nia (<a href=\"https:&#x2F;&#x2F;trynia.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;trynia.ai</a>), a SOTA context layer for AI coding agents. Nia lets tools like Cursor, Claude Code, and other MCP clients index and query real codebases and documentation so they stop hallucinating against outdated or wrong sources, with applications beyond coding agents to any AI system that requires grounded context across domains.<p>Coding agents are only as good as the context you give them. General models are trained on public code and documentation that is often old, and they usually have no idea what is inside your actual repo, internal wiki, or the exact version of a third party SDK you use. The result is very familiar: you paste URLs and code snippets into the prompt, the agent confidently uses an outdated API or the wrong framework version, and you spend more time verifying and correcting it than if you had written the code yourself. Once models are good enough at generating code, feeding them precise, up-to-date context becomes the bottleneck.<p>I ran into this pattern first on my own projects when (a few months ago) I was still in high school in Kazakhstan, obsessed with codegen tools and trying every coding agent I could find. I saw it again when I got into YC and talked to other teams who were also trying to use agents on real work.<p>The first version of Nia was basically \u201cmy personal MCP server that knows my repos and favorite doc sites so I do not have to paste URLs into Cursor anymore.\u201d Once I saw how much smoother my own workflow became, it felt obvious that this should be a product other people could use too.<p>Under the hood, Nia is an indexing and retrieval service with an MCP interface and an API. You point it at sources like GitHub repositories, framework or provider docs, SDK pages, PDF manuals, etc. We fetch and parse those with some simple heuristics for code structures, headings, and tables, then normalize them into chunks and build several indexes: a semantic index with embeddings for natural language queries; a symbol and usage index for functions, classes, types, and endpoints; a basic reference graph between files, symbols, and external docs; regex and file tree search for cases where you want deterministic matches over raw text.<p>When an agent calls Nia, it sends a natural language query plus optional hints like the current file path, stack trace, or repository. Nia runs a mix of BM25 style search, embedding similarity, and graph walks to rank relevant snippets, and can also return precise locations like \u201cthis function definition in this file and the three places it is used\u201d instead of just a fuzzy paragraph. The calling agent then decides how to use those snippets in its own prompt.\nOne Nia deployment can serve multiple agents and multiple projects at once. For example, you can have Cursor, Claude Code, and a browser based agent all pointed at the same Nia instance that knows about your monorepo, your internal wiki, and the provider docs you care about. We keep an agent agnostic session record that tracks which sources were used and which snippets the user accepted. Any MCP client can attach to that session id, fetch the current context, and extend it, so switching tools does not mean losing what has already been discovered.<p>A lot of work goes into keeping indexes fresh without reprocessing everything. Background workers periodically refetch configured sources, detect which files or pages changed, and reindex those incrementally. This matters because many of the worst \u201challucinations\u201d I have seen are actually the model quoting valid documentation for the wrong version. Fixing that is more about version and change tracking than about model quality.<p>We ship Nia with a growing set of pre-indexed public sources. Today this includes around 6k packages from common frameworks and provider docs, plus package search over thousands of libraries from ecosystems like PyPI, npm, and RubyGems, as well as pre indexed &#x2F;explore page where everyone can contribute their sources! The idea is that a new user can install Nia, connect nothing, and still get useful answers for common libraries. Then, as soon as you add your own repos and internal docs, those private sources are merged into the same index.\nSome examples of how people use Nia so far: - migrating from one payments provider or API version to another by indexing the provider docs plus example repos and letting the agent propose and iterate on patches; - answering \u201chow do I do X in this framework\u201d by indexing the framework source directly instead of relying only on official docs that might be stale; - turning an unfamiliar public codebase into a temporary wiki to self onboard, where you can ask structural questions and jump to specific files, functions, or commits; - building a browser agent that answers questions using up to date code and docs even when the public documentation lags behind.<p>Nia is a paid product (<a href=\"https:&#x2F;&#x2F;www.trynia.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.trynia.ai&#x2F;</a>) but we have a free tier that should be enough for individuals to try it on real projects. Above that there is a self-serve paid plan for heavier individual use, and organization plans with higher limits, SOC 2, seat based billing, and options for teams that want to keep indexing inside their own environment. For private GitHub repos we can clone and index locally so code does not leave your infrastructure.<p>We store account details and basic telemetry like query counts and errors to operate the service, and we store processed representations of content you explicitly connect (chunks, metadata, embeddings, and small graphs) so we can answer queries. We do not train foundation models on customer content and we do not sell user data.\nMoreover, I can see Nia play out in the larger context of the agents space due to the global problem of providing reliable context to those systems. Early signals show that people are already using Nia for healthcare data, cloning Paul Graham by indexing all of his essays and turning him into an AI agent, using Naval\u2019s archive to build a personalized agent, and more.<p>I would love to get Nia into the hands of more engineers who are already pushing coding agents hard and see where it breaks. I am especially interested in hearing about failure modes, annoying onboarding steps, places where the retrieval logic is obviously wrong or incomplete, or any security concerns I should address. I will be in the thread to answer questions, share more technical details, and collect any brutal feedback you are willing to give!", "author": "jellyotsiro", "timestamp": "2025-12-08T17:10:14+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["onboarding", "navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-08T17:12:56.487009+00:00", "processed": false}
{"id": "hn_story_46194548", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46194548", "title": "Show HN: Tampermonkey/Stylus but with prompts instead of code (open source)", "text": "I\u2019ve built a browser extension that allows you to personalize websites just by prompting. It takes your request and uses openai\u2019s codex-mini to generate the JS and CSS needed to apply the change.<p>It can do all sorts of things: stop autoplaying videos, replace links with archive.is on newspapers, dim sidebars, or add small QOL features like editing the responses in chatgpt so it\u2019s easier to copy&#x2F;paste. Earlier today I asked it to add a \u201ccost per 100 requests\u201d column on OpenRouter\u2019s activity page\u2014decimals makes it hard for my ADHD brain to process.<p>Technically, you can do this with developer tools and user styles but i\u2019ve been impressed with codex\u2019 ability to take my vague requests and turn it into  working styles with just 10% of the source page for context.<p>With an Apple dev account you can use it on mobile via Safari extensions.", "author": "alentodorov", "timestamp": "2025-12-08T16:50:03+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-08T17:12:57.548631+00:00", "processed": false}
{"id": "hn_comment_46193195", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46193195", "title": "Re: Built in 30 days by someone who had never coded be...", "text": "The Backstory\nExactly 30 days ago, I was frustrated. I kept switching between different AI apps \u2014 ChatGPT for reasoning, Gemini for speed, Grok for real-time news. Each had strengths, but none offered a single interface that could handle it all intelligently.\nSo, I decided to build it myself. The problem? I had zero coding experience.<p>The Learning Curve\nWhen I started this journey last month, I had never touched a line of code or opened a terminal. I spent sleepless nights teaching myself everything from scratch. To give you an idea of the mountain I had to climb, 30 days ago I had zero experience with:\n* Languages &amp; IDEs: JavaScript, Android Studio, Visual Studio<p>* Infrastructure: GitHub (version control was a nightmare at first), Firebase<p>* AI Integrations: OpenAI API, Grok API, Gemini API, Tavily (for search)<p>* Business Logic: Stripe integration for payments<p>What I Built\nThe result is Ask AI \u2014 available now on Web &amp; Android. It\u2019s an all-in-one AI assistant designed to stop the \u201capp fatigue.\u201d\nKey Features:\n*  Auto-Model Routing: An engine that analyzes your prompt and automatically routes it to the best model (Gemini Flash for speed, GPT-5&#x2F;Grok for complex tasks). Saves you money and time.<p>*  Visuals: Live 4K wallpapers and JavaScript-generated themes (Matrix, Terminal, Frosted Glass).<p>*  Real-Time Data: Integrated Tavily and Grok to fetch live web data.<p>*  Fair Pricing: A free tier that actually works (Nano models), plus a premium tier for heavy lifters.<p>Why This Matters\nAsk AI isn\u2019t just another assistant. It\u2019s proof that anyone \u2014 even with zero coding background \u2014 can learn, build, and launch something meaningful in just 30 days. My hope is that this inspires other beginners to take the leap and create.\nI Need Your Feedback\nSince this is my very first project, I\u2019m sure there are bugs I haven\u2019t found and UI quirks I\u2019ve missed. I\u2019d love for this community to test it out and give me brutally honest feedback.\n Try Ask AI here:\n* Play Store: <a href=\"https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=ask_ai.info.twa\">https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=ask_ai.info.tw...</a><p>Thanks for checking it out!", "author": "sarymismail", "timestamp": "2025-12-08T15:18:27+00:00", "score": null, "num_comments": null, "products": ["chatgpt", "gemini", "grok"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2025-12-08T17:13:03.460167+00:00", "processed": false}
{"id": "hn_story_46192266", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46192266", "title": "Show HN: I built an AI tool to evaluate my AngelList deal flow", "text": "I&#x27;m Kyle, a software engineer who started angel investing through AngelList syndicates (~25 deals, $1-10k each). I&#x27;d see interesting ideas and clever founders but wasn&#x27;t sure what to look for or how to compare them. I wanted a system to think through deals more systematically. A second opinion to challenge my initial read.<p>What it does: - Paste a deal memo \u2192 get scoring on 8 criteria (founder, market, traction, etc.) - Every score cites specific evidence. &quot;Strong retention&quot; without numbers = lower score - Compare deals side-by-side, ask follow-up questions<p>Demo:<p><a href=\"https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;a360b329f9e849c38c1ea70ba510d178\" rel=\"nofollow\">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;a360b329f9e849c38c1ea70ba510d178</a><p>Tech: - Claude Sonnet 4.5 for analysis (Anthropic for nuanced judgment) - Local anonymization\u2014company&#x2F;founder names scrubbed client-side before API calls - Multi-layer QA: accuracy checker catches hallucinations, auto-retry on errors, final polish<p>What I learned: AI coding tools make it too easy to tinker. I&#x27;d have 3 fixes going at once, creating more bugs than I solved. Had to force myself to slow down and work methodically. Bigger lesson: I spent months tweaking in isolation instead of getting external feedback. This post is me breaking that habit.<p>Try it: Free tier has 20 triages + 3 deep analyses&#x2F;month. I&#x27;d love feedback on whether scoring feels calibrated and happy to talk about any elements of my development here.<p><a href=\"https:&#x2F;&#x2F;angelcheck.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;angelcheck.ai</a>", "author": "stiline06", "timestamp": "2025-12-08T13:56:44+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-08T17:13:10.093316+00:00", "processed": false}
{"id": "hn_comment_46192188", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46192188", "title": "Re: I built an API testing tool that generates tests f...", "text": "The idea came from a simple problem: most teams have lots of API endpoints, but almost no one has realistic coverage. Writing and maintaining test collections takes forever, and scripts always fall out of sync.<p>Rentgen takes one cURL request and generates: \n\u2022 boundary tests (min&#x2F;max, out-of-range)\n\u2022 enum variation tests\n\u2022 invalid&#x2F;negative input cases\n\u2022 trimming&#x2F;whitespace cases\n\u2022 structure&#x2F;mapping validation\n\u2022 reflection safety checks\n\u2022 missing&#x2F;incorrect security headers\n\u2022 basic latency&#x2F;load insights\n\u2022 automatic bug-report templates\n\u2022 and many other.<p>The goal is to give engineers a rough but honest API health picture in ~2 minutes \u2014 without maintaining test files or writing code.<p>A fun surprise: I pointed Rentgen at ChatGPT\u2019s API and found a few issues we genuinely didn\u2019t expect to see in production. They were fixed immediately after reporting.<p>I would really appreciate feedback from the community:\n\u2022 What categories of tests are missing?\n\u2022 Which edge cases do you usually find manually?\n\u2022 What would make this useful in your workflow?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;LiudasJan&#x2F;Rentgen\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;LiudasJan&#x2F;Rentgen</a><p>Happy to answer anything about the engine design, how the generator works, reflection detection, or upcoming performance modules.", "author": "liudasjank", "timestamp": "2025-12-08T13:49:43+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-08T17:13:10.365354+00:00", "processed": false}
{"id": "hn_comment_46192597", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46192597", "title": "Re: Alignment Is Capability...", "text": "I&#x27;ve only been using it a couple of weeks, but in my opinion, Opus 4.5 is the biggest jump in tech we&#x27;ve seen since ChatGPT 3.5.<p>The difference between juggling Sonnet 4.5 &#x2F; Haiku 4.5 and just using Opus 4.5 for everything is night &amp; day.<p>Unlike Sonnet 4.5 which merely had promise at being able to go off and complete complex tasks, Opus 4.5 seems genuinely capable of doing so.<p>Sonnet needed hand-holding and correction at almost every step. Opus just needs correction and steering at an early stage, and sometimes will push back and correct my understanding of what&#x27;s happening.<p>It&#x27;s astonished me with it&#x27;s capability to produce easy to read PDFs via Typst, and has produced large documents outlining how to approach very tricky tech migration tasks.<p>Sonnet would get there eventually, but not without a few rounds of dealing with compilation errors or hallucinated data. Opus seems to like to do &quot;And let me just check my assumptions&quot; searches which makes all the difference.", "author": "xnorswap", "timestamp": "2025-12-08T14:28:42+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2025-12-08T17:13:12.787526+00:00", "processed": false}
{"id": "hn_story_46191564", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46191564", "title": "Show HN: Chorus \u2013 Multi-agent debate through epistemological framework collision", "text": "Hey HN,<p>I&#x27;ve been building Chorus, a multi-agent system with a different approach than the typical role-based agents (AutoGen, CrewAI, etc.).<p>The core idea: instead of giving agents &quot;roles&quot; (researcher, critic, writer), each agent reasons through an epistemological framework \u2013 a set of rules about what counts as valid knowledge, what questions to ask, and what reasoning moves are allowed&#x2F;forbidden.<p>When you run a debate, frameworks with incompatible validity tests are forced to collide. A &quot;Metric&quot; agent (everything must be quantifiable) arguing with a &quot;Storyteller&quot; agent (context and lived experience matter) creates productive tension that surfaces trade-offs a single perspective would miss.<p>The interesting part: the system can detect when agents synthesize something that doesn&#x27;t fit any existing framework \u2013 and extract it as a new &quot;emergent framework.&quot; I&#x27;ve got 33 of these now, discovered through debates, not designed by me. Whether these are genuinely novel epistemologies or sophisticated pattern matching is an open question I&#x27;m still investigating.<p>What it&#x27;s not: consensus-seeking, voting, or &quot;let&#x27;s all agree.&quot; The goal is structured disagreement that produces insights.<p>Built with: Node.js backend, vanilla JS frontend, multiple LLM providers (Claude, GPT-4, Gemini, Mistral).<p>Live for waitlist signup at: <a href=\"https:&#x2F;&#x2F;chorusai.replit.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;chorusai.replit.app&#x2F;</a><p>-I&#x27;ll send a beta code to the email used for sign up<p>Feedback wanted: Is &quot;epistemological frameworks&quot; meaningfully different from good prompt engineering? Would love HN&#x27;s honest take on whether this is genuine innovation or dressed-up multi-agent chat.", "author": "efoobz", "timestamp": "2025-12-08T12:41:16+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-08T17:13:16.131502+00:00", "processed": false}
{"id": "hn_comment_46191027", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46191027", "title": "Re: Show HN: Sornic \u2013 Turn any URL into social media p...", "text": "I built this because writing the same content differently for LinkedIn, Twitter, Instagram, etc. was taking too long.<p>Paste a URL \u2192 AI reads the page \u2192 Generates platform-specific posts.<p>Stack: Next.js, Claude API, Upstash Redis, Vercel.<p>Free to try (3 generations). Would love feedback on output quality.", "author": "digi_wares", "timestamp": "2025-12-08T11:21:25+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-08T17:13:19.436415+00:00", "processed": false}
{"id": "hn_story_46190576", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46190576", "title": "CLI coding agents browsing ncdu/gdu directly instead of parsing JSON", "text": "Idea:<p>Tools like `ncdu` and `gdu` already present a compressed, human-friendly view of the filesystem tree. A human just looks at the TUI and presses \u2191 \u2193 \u2192 \u2190 to drill down, decide what matters, and ignore the rest.<p>Most current AI workflows instead export giant JSON dumps, then build an entire parsing + chunking + filtering layer on top of them. This burns context, adds complexity, and doesn\u2019t match the real usage model.<p>So the question is: Why not let an LLM simply *operate the TUI* the same way a human does?<p>Just read the visible screen text, choose an action, and repeat.\nThe TUI <i>is already</i> an optimized abstraction layer.<p>This fits the same class of interaction as modern CLI-AI agents like *Claude Code* or *OpenCode CLI*, but instead of orchestrating commands, the model would literally navigate an interactive interface (ncdu&#x2F;gdu) step-by-step.<p>Questions:<p>* Does this interaction model make sense, or is there some fundamental flaw I&#x27;m missing?\n* Is anyone aware of existing OSS (beyond general agents like Claude Code &#x2F; OpenCode CLI) that specifically lets an LLM \u201cdrive\u201d ncdu&#x2F;gdu or similar TUIs directly?", "author": "shou_arisaka", "timestamp": "2025-12-08T10:13:03+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-08T17:13:21.443000+00:00", "processed": false}
