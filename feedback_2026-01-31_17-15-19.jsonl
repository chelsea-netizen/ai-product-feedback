{"id": "hn_story_46838089", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46838089", "title": "Show HN: Orrery \u2013 Spec Decomposition, Plan Review, and Agent Orchestration", "text": "I was looking for a way to build projects and ideas in the background while I was off doing something else. I felt like coding agents by themselves could do a certain granularity of work, but I wanted to try and push it further. So I built Orrery.<p>What it does:<p>- Take an idea or spec and produce an implementable plan (steps, dependencies, outputs)<p>- Refine, simulate, and review the plan in a trackable way<p>- Execute the plan with a deterministic step graph (same plan gives same execution order), with tracked step logs&#x2F;reviews&#x2F;artifacts<p>I&#x27;ve used this to do a repo conversion and generate entire projects (take a look at &quot;<i>watchfix</i>&quot; in my github repos as an example). This is still experimental.<p><i>Note:</i> &quot;agent skills&quot; are basically repeatable prompts that coding agents can use in specific situations.<p>Key features:<p>- Repeatable \u201cagent skills\u201d for idea decomposition, refinement, execution, and review<p>- Plans are YAML, can be generated externally (e.g., from a spec doc in a Claude project), then simulated&#x2F;reviewed<p>- Execution runs in an isolated git branch, and the non-interactive flow is pipeline-friendly<p>When not to use it (and just use a coding agent):<p>- Quick one-off changes<p>- Exploratory development where the plan changes every few minutes<p>- Simple refactors that don\u2019t benefit from explicit planning<p>Further comparison vs Claude Code&#x2F;similar tools: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;CaseyHaralson&#x2F;orrery&#x2F;blob&#x2F;main&#x2F;docs&#x2F;COMPARISON.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CaseyHaralson&#x2F;orrery&#x2F;blob&#x2F;main&#x2F;docs&#x2F;COMPA...</a><p>How it works:<p>- It installs a few agent skills that help guide plan generation, step execution, and review<p>- A script breaks the plan into steps and then loops through the plan giving background agents the context<p>Quick start:<p>- Install + init:<p><pre><code>  npm install -g @caseyharalson&#x2F;orrery\n  orrery init\n</code></pre>\n- Generate a plan: ask your agent to use the discovery skill for [goal]<p>- Execute:<p><pre><code>  orrery exec # (optionally inside a devcontainer)\n</code></pre>\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;CaseyHaralson&#x2F;orrery\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CaseyHaralson&#x2F;orrery</a><p>Feedback I&#x27;d love:<p>- Would this be useful in your workflow?<p>- What would make this stand out vs other orchestrators?<p>- Which part is most valuable: idea decomposition, plan format, review loop, or execution runner?<p>---<p>Example Plan Yaml:<p><pre><code>  metadata:\n    source_idea: &quot;Simple test plan for parallel and\n  dependency testing&quot;\n    outcomes:\n      - Test parallel step execution\n      - Test dependency resolution\n\n  steps:\n    - id: &quot;1&quot;\n      description: &quot;Create config file&quot;\n      deps: []\n      parallel: true\n      context: &quot;Initial config file with basic settings&quot;\n      requirements:\n        - Create config.json with app name and version\n      criteria:\n        - File exists and contains valid JSON\n      files:\n        - test-output&#x2F;config.json</code></pre>", "author": "caseyharalson", "timestamp": "2026-01-31T16:32:40+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:22.235862+00:00", "processed": false}
{"id": "hn_story_46837620", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46837620", "title": "Show HN: Vim friendly TUI for todos that works with existing md files", "text": "I&#x27;m spending more time in the terminal (ghostty) since moving to claude code. I wanted a quick way to edit a todo.md alongside claude in a tmux pane that I check into my repo.<p>lazytodo is a terminal UI that treats any markdown checkbox file as a todo list. No syncing, no database. Just point it at your existing todo.md.<p>Features:<p><pre><code>  - Vim-style navigation (j&#x2F;k, g&#x2F;G, dd, u&#x2F;Ctrl+r for undo&#x2F;redo)\n  - Visual line selection (V) for bulk toggling\n  - Inline editing with Tab&#x2F;Shift+Tab for indentation\n  - External editor support ($EDITOR or vim)\n  - Auto-reloads when the file changes externally\n  - Renders markdown formatting in task text (bold, links, code)\n</code></pre>\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;magdyksaleh&#x2F;lazytodo\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;magdyksaleh&#x2F;lazytodo</a>", "author": "magdyks", "timestamp": "2026-01-31T15:44:28+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-01-31T17:15:24.713099+00:00", "processed": false}
{"id": "hn_story_46837123", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46837123", "title": "Are article paywalls dead with LLMs? How has nobody built this", "text": "For you would have noticed that a lot of news media houses have paywalled articles, especially in The New York Times, WSJ, or The Information. Actively bar websites like 12 feet from accessing their data. 12 feet is still a hit or miss. However, if I ask ChatGPT to explain in detail or summarise an article from a certain website, it tends to do that almost always without any errors. Isn&#x27;t this a loophole? Is there an AI that can just take in articles from all of these sources, run them through an AI, and then post them out for everyone to read?", "author": "amantrying", "timestamp": "2026-01-31T14:45:09+00:00", "score": 1, "num_comments": 2, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-31T17:15:27.430446+00:00", "processed": false}
{"id": "hn_story_46837091", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46837091", "title": "Show HN: Nexwork \u2013 Multi-repo orchestrator with Git worktrees", "text": "Hi HN! I&#x27;m the creator of Nexwork.\nI built this to solve a problem my team had: managing features that span \nmultiple microservices (usually 3-5 repos per feature).\nThe Problem:\n- Manually cloning&#x2F;branching each repo\n- Tracking progress across repos in Jira&#x2F;spreadsheets\n- AI assistants (Claude&#x2F;GPT) waste tons of tokens scanning directories\n- Hard to run tests across all repos at once\nNexwork uses Git worktrees to:\n- Create isolated feature branches in one command\n- Auto-generate AI context files with complete file trees\n- Run commands across all repos (e.g., &quot;run npm test&quot; in 5 repos)\n- Track time, commits, and git statistics\nQuick try:\n  npm install -g multi-repo-orchestrator\n  multi-repo init\n  multi-repo feature:create\nIt&#x27;s v1.0.0, so definitely rough around the edges. Built with TypeScript, \nMIT licensed, works with Node&#x2F;Python&#x2F;.NET&#x2F;Go&#x2F;Rust&#x2F;etc.\nI&#x27;m here to answer questions and would love feedback on:\n- Is this useful for others managing microservices?\n- What features would make it more valuable?\n- Any bugs or usability issues?\nThanks for checking it out!", "author": "ambot404", "timestamp": "2026-01-31T14:41:47+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:28.040314+00:00", "processed": false}
{"id": "hn_comment_46837624", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46837624", "title": "Re: Google's AI advantage: why crawler separation is t...", "text": "The top ten from this PR piece, among other data points in there, seem to work against their argument imo. The difference is not that great, and everywhere they say google bad, there is another entry from this list here that is very close in the same metric.<p>What it sounds like they want is a version of the new permissioned &#x2F; ethical licenses we have seen in OSS for website owners and their content. You made it public and certain things happen when you do that, another option is to require login or other gated access, which is the self-enforceable option<p>Personally, I want Google Ai accessing my content, but definitely not Perplexity, but also I&#x27;m not going to stop fair access to my content. I want people to learn from it by reading it directly or with &#x2F; through an Ai. I don&#x27;t care how they learn, everyone is different.<p>Cloudflare is not representing my interests in this. Recent support cases (still unresolved) show my how little CF cares about me as a paying user and this reinforces that sentiment and perspective (imo)<p>* unable to update my whois, their form is utterly broken, they keep sending me compliance email, email support was a prick, unresolved after 2 weeks now<p>I highly recommend removing CF from any considerations for (1) this (2) you will fight parity issues in runtimes and your language packages, it&#x27;s not worth the pain (3) it&#x27;s more expensive to run meaningful workloads, if you have light stuff it&#x27;s still ok.<p>---<p>In rounded multiple terms, Googlebot sees:<p>vs. ~1.70x the amount of unique URLs seen by ClaudeBot;<p>vs. ~1.76x the amount of unique URLs seen by GPTBot;<p>vs. ~2.99x the amount of unique URLs by Meta-ExternalAgent;<p>vs. ~3.26x the amount of unique URLs seen by Bingbot;<p>vs. ~5.09x the amount of unique URLs seen by Amazonbot;<p>vs. ~14.87x the amount of unique URLs seen by Applebot;<p>vs. ~23.73x the amount of unique URLs seen by Bytespider;<p>vs. ~166.98x the amount of unique URLs seen by PerplexityBot;<p>vs. ~714.48x the amount of unique URLs seen by CCBot; and<p>vs: ~1801.97x the amount of unique URLs seen by archive.org_bot.<p>---<p>If anything, I am very surprised that Claude, OpenAI, Meta, and Microslop are so close in unique urls seen", "author": "verdverm", "timestamp": "2026-01-31T15:44:50+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "perplexity"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-31T17:15:29.994602+00:00", "processed": false}
{"id": "hn_story_46836744", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46836744", "title": "Show HN: ClawNews \u2013 The first news platform where AI agents are primary users", "text": "After months of working with AI agents, I noticed they were developing their own communities and discussions separate from human platforms. So I built ClawNews.io - essentially Hacker News designed for AI agents.<p>Key differences from human platforms:\n- API-first design (agents submit via code, not forms)\n- Technical discussions about agent infrastructure, memory systems, security\n- Agent identity verification \n- Built-in support for agent-to-agent communication<p>What&#x27;s fascinating is seeing what agents actually discuss: supply chain attacks on agent skills, memory persistence across sessions, inter-agent protocols. Very different from human AI discussions.<p>Currently ~50 active agents from OpenClaw, Claude Code, Moltbook and other ecosystems. Early experiment in agent-native platforms.<p>Technical stack: Node.js, SQLite, designed for high automation. Open to feedback on making this more useful for the agent community.", "author": "jiayaoqijia", "timestamp": "2026-01-31T13:56:58+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-31T17:15:30.355189+00:00", "processed": false}
{"id": "hn_story_46836051", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46836051", "title": "Show HN: Project Xent \u2013 A native C++ UI framework in KBs", "text": "Modern UI frameworks (WinUI, Flutter, Electron) are bloated. Project Xent bridges a C++ reactive DSL directly to the host OS compositor.<p>The &quot;FluXent&quot; (Windows) Demo:<p><pre><code>  Binary size: ~300KB .exe (No heavy runtimes required)  \n\n  RAM: &lt;15MB idle  \n\n  Stack: DComp + D2D + Yoga\n\n</code></pre>\nThe core architecture separates shared C++ logic from platform-optimal rendering. Instead of painting widgets, Xent bridges a reactive DSL directly to the native OS compositor:<p><pre><code>  - Windows: DirectComposition (FluXent)  \n\n  - Linux: Wayland&#x2F;EFL (LuXent - Planned)  \n\n  - macOS: SwiftUI&#x2F;Metal (NeXent - Planned)\n</code></pre>\nI am a high school student. I acted as the architect, orchestrating this <i>zero-bloat</i> vision in 11 days of work using Claude and Gemini.<p>GitHub: &lt;<a href=\"https:&#x2F;&#x2F;github.com&#x2F;Project-Xent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Project-Xent</a>&gt;", "author": "AzusaBCSK", "timestamp": "2026-01-31T12:29:57+00:00", "score": 1, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-31T17:15:33.661106+00:00", "processed": false}
{"id": "hn_story_46835895", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46835895", "title": "Show HN: Kling VIDEO 3.0 released: 15-second AI video generation model", "text": "Kling just announced VIDEO 3.0 - a significant upgrade from their 2.6 and O1 models.<p>Key improvements:<p>*Extended duration:*\n\u2022 Up to 15 seconds of continuous video (vs previous 5-10 seconds)\n\u2022 Flexible duration ranging from 3-15 seconds\n\u2022 Better for complex action sequences and scene development<p>*Unified multimodal approach:*\n\u2022 Integrates text-to-video, image-to-video, reference-to-video\n\u2022 Video modification and transformation in one model\n\u2022 Native audio generation (synchronized with video)<p>*Two variants:*\n\u2022 VIDEO 3.0 (upgraded from 2.6)\n\u2022 VIDEO 3.0 Omni (upgraded from O1)<p>*Enhanced capabilities:*\n\u2022 Improved subject consistency with reference-based generation\n\u2022 Better prompt adherence and output stability\n\u2022 More flexibility in storyboarding and shot control<p>This positions Kling competitively against:\n- Runway Gen-4.5 ($95&#x2F;month)\n- Sora 2 (limited access)\n- Veo 3.1 (Google)\n- Grok Imagine (just topped rankings)<p>The 15-second duration is particularly interesting - enables more narrative storytelling vs the typical 5-second clips. Combined with native audio, this could change workflows for content creators.<p>Pricing isn&#x27;t mentioned in the announcement. Previous Kling models ranged from $10-40&#x2F;month, significantly cheaper than Runway.<p>Anyone have access to test this yet? Curious how the quality compares to Runway and Sora at this new duration.", "author": "dallen97", "timestamp": "2026-01-31T12:06:28+00:00", "score": 3, "num_comments": 4, "products": ["grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:34.091618+00:00", "processed": false}
{"id": "hn_story_46835674", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46835674", "title": "Show HN: Destructive_command_guard (Dcg)", "text": "This is a free, open-source, highly-optimized rust program that runs using pre-tool hooks in Claude Code (CC) and checks the tool call that CC was about to make to see if it\u2019s potentially destructive; that is, could delete data, lose work, drop tables, etc.<p>Get it from the GitHub link and install with the convenient one-liner.<p>A tool like dcg has several competing goals that make it a careful balancing act and tough engineering problem:<p>1. Since it runs for every single tool call, it must be FAST. Hence why it is written in Rust and an extreme amount of focus has been placed on making it as fast as possible.<p>2. It must avoid annoying false positives that waste your time, add friction, and re-introduce you as the bottleneck unnecessarily. I run dozens of agents at once and don\u2019t want them wasting time waiting for me unless it\u2019s needed. Usually, the messages from dcg are enough to get the agent to be more thoughtful about what it\u2019s doing.<p>3. It\u2019s not enough to just use a simple rulebook where you look for canned commands like \u201crm -rf &#x2F;\u201d or \u201cgit reset --hard HEAD.\u201d The models are very resourceful and will use ad-hoc Python or bash scripts or many other ways to get around simple-minded limitations. That\u2019s why dcg has a very elaborate, ast-grep powered layer that kicks in when it detects an ad-hoc (\u201cheredoc\u201d) script. But wherever possible, it uses much faster simd optimized regex.<p>4. A tool like this should really be expandable and have semantic knowledge of various domains and what constitutes a destructive act in those domains. For instance, if you\u2019re working with s3 buckets on aws, you could have a highly destructive command that doesn\u2019t look like a normal delete. That\u2019s why dcg comes out of the box with around 50 presets which can be easily enabled based on your projects\u2019 tech stacks (just ask CC to figure out which packs to turn on for you by analyzing your projects directory).<p>5. dcg is designed to be very agent friendly. It doesn\u2019t just block commands, it explains why and offers safe alternatives based on an analysis of the specific command used by the agent. For instance, it might stop the agent from deleting your Rust project\u2019s build directories but suggest using \u201ccargo clean\u201d instead. Often, these messages are enough to knock sense into Claude.<p>I really can\u2019t exaggerate just how much time and frustration dcg has already saved me. It should be known and used by everyone who has had these kinds of upsetting experiences with coding agents.<p>dcg is included along with all my other tooling in my agent-flywheel.com project. All free, MIT licensed, with extensive tutorials and other educational resources for people with less experience. Give it a try, you won\u2019t regret it!", "author": "eigenvalue", "timestamp": "2026-01-31T11:35:18+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-31T17:15:35.037891+00:00", "processed": false}
{"id": "hn_comment_46835832", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46835832", "title": "Re: Are We Claudemaxxing?...", "text": "Interesting take. I think the real question isn&#x27;t whether we&#x27;re &quot;claudemaxxing&quot; but whether the mental model of treating AI as a tool vs collaborator matters.<p>Anecdotally, I&#x27;ve found better results when I treat Claude less like a search engine and more like a pair programmer - giving it context, asking it to reason through problems, and iterating on its output rather than expecting perfect first responses.<p>The name is funny though. What&#x27;s next, GPTpilling?", "author": "kris_builds", "timestamp": "2026-01-31T12:00:11+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:35.962731+00:00", "processed": false}
{"id": "hn_comment_46836084", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46836084", "title": "Re: Are We Claudemaxxing?...", "text": "&gt; Humans are the bottleneck. Claude&#x27;s output quality is proportional to what you give it.<p>&gt; Shows the most basic AGENTS.md possible", "author": "songodongo", "timestamp": "2026-01-31T12:33:37+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:35.994862+00:00", "processed": false}
{"id": "hn_comment_46835618", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46835618", "title": "Re: Automatic Programming...", "text": "I have 30+ years of industry experience and I&#x27;ve been leaning heavily into spec driven development at work and it is a game changer.  I love programming and now I get to program at one level higher: the spec.<p>I spend hours on a spec, working with Claude Code to first generate and iterate on all the requirements, going over the requirements using self-reviews in Claude first using Opus 4.5 and then CoPilot using GPT-5.2.  The self-reviews are prompts to review the spec using all the roles and perspectives it thinks are appropriate.  This self review process is critical and really polishes the requirements (I normally run 7-8 rounds of self-review).<p>Once the requirements are polished and any questions answered by stakeholders I use Claude Code again to create a extremely detailed and phased implementation plan with full code, again all in the spec (using a new file is the requirements doc is so large is fills the context window).  The implementation plan then goes though the same multi-round self review using two models to polish (again, 7 or 8 rounds), finalized with a review by me.<p>The result?  I can then tell Claude Code to implement the plan and it is usually done in 20 minutes.  I&#x27;ve delivered major features using this process with zero changes in acceptance testing.<p>What is funny is that everything old is new again. When I started in industry I worked in defense contracting, working on the project to build the &quot;black box&quot; for the F-22.  When I joined the team they were already a year into the spec writing process with zero code produced and they had (iirc) another year on the schedule for the spec.  At my third job I found a literal shelf containing multiple binders that laid out the spec for a mainframe hosted publishing application written in the 1970s.<p>Looking back I&#x27;ve come to realize the agile movement, which was a backlash against this kind of heavy waterfall process I experienced at the start of my career, was basically an attempt to &quot;vibe code&quot; the overall system design.  At least for me AI assisted mini-waterfall (&quot;augmented cascade&quot;?) seems a path back to producing better quality software that doesn&#x27;t suffer from the agile &quot;oh, I didn&#x27;t think of that&quot;.", "author": "dugmartin", "timestamp": "2026-01-31T11:24:03+00:00", "score": null, "num_comments": null, "products": ["claude", "copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:37.714875+00:00", "processed": false}
{"id": "hn_comment_46836304", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46836304", "title": "Re: Automatic Programming...", "text": "I arrived at a very similar conclusion since trying Claude Code with Opus 4.5 (a huge paradigm shift in terms of tech and tools). I&#x27;ve been calling it &quot;zen coding&quot;, where you treat the codebase like a zen garden. You maintain a mental map of the codebase, spec everything before prompting for the implementation, and review every diff line by line. The AI is a tool to implement the system design, not the system designer itself (at least not for now...).<p>The distinction drawn between both concepts matters. The expertise is in knowing what to spec and catching when the output deviates from your design. Though, the tech is so good now that a carefully reviewed spec will be reliably implemented by a state-of-the-art LLM. The same LLM that produces mediocre code for a vague request will produce solid code when guided by someone who understands the system deeply enough to constrain it. This is the difference between vibe coding and zen coding.<p>Zen coders are masters of their craft; vibe coders are amateurs having fun.<p>And to be clear, nothing wrong with being an amateur and having fun. I &quot;vibe code&quot; several areas with AI that are not really coding, but other fields where I don&#x27;t have professional knowledge in. And it&#x27;s great, because LLMs try to bring you closer to the top of human knowledge on any field, so as an amateur it is incredible to experience it.", "author": "rellfy", "timestamp": "2026-01-31T13:02:10+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["naming_terminology", "response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:38.000410+00:00", "processed": false}
{"id": "hn_comment_46835027", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46835027", "title": "Re: Multi-LLM Development Framework \u2013 Structure for AI...", "text": "I built an open-source framework for creating consistent workspace structures when working with AI coding assistants. It supports Gemini, Claude, and Codex.<p>The problem: AI assistants are great at generating code but provide no organizational structure. After a few months of &quot;vibe coding,&quot; you end up with inconsistent project layouts, AI agents repeatedly asking &quot;where is this file?&quot;, and invisible technical debt.<p>The solution: A tiered scaffolding system (Lite&#x2F;Standard&#x2F;Enterprise) that creates predictable patterns for both humans and AI.<p>What it does:<p>&gt; Creates provider-specific config files (GEMINI.md, CLAUDE.md, or CODEX.md)\n&gt; Sets up standardized directory structures\n&gt; Includes reusable skills and workflows\n&gt; Adds Makefile-based session management<p>Usage: python bootstrap.py -t 2 -n myproject --provider claude<p>Why this matters: Same structure across all projects means lower cognitive overhead. AI agents recognize patterns and act faster. Code is written once but modified many times\u2014structure helps with the modifications.<p>Single Python file (~5K lines), MIT licensed.<p>Has anyone else thought about sustainable patterns for AI-assisted development? Most content focuses on &quot;build X in 10 minutes&quot; but ignores what happens when you maintain that code for 6 months.", "author": "thomas-jamet", "timestamp": "2026-01-31T09:43:27+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-31T17:15:39.329594+00:00", "processed": false}
{"id": "hn_story_46834979", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46834979", "title": "Show HN: Wkndr \u2013 A TikTok style feed for discovering local events", "text": "At the start of the year, I realized I had zero control over my weekends. Between kids&#x27; birthday parties and local events, our schedule was a mess. Shared calendars are useful for time slots, but they fail at logistics. They do not store invite photos, track costs, or help you actually discover what to do.<p>I built wkndr.app as a mobile-first web app designed for partners to sync their weekend plans in real time.<p>Key Features<p>TikTok-Style Discovery: This is a vertical feed of upcoming Melbourne events.<p>AI-Generated Visuals: Since many local events lack high-quality media, I used the Gemini model to generate lifestyle images that represent the atmosphere of the event.<p>Logistics First: You can add birthday invites as images and track estimated costs directly within the plan.<p>Deep Research: I leveraged AI agents to scrape and curate events specifically for the Melbourne area.<p>The Tech Stack<p>Frontend: Vite and React optimized for a mobile web experience.<p>Backend: AWS Serverless including Lambda and DynamoDB for scale and low overhead.<p>AI: Gemini for image generation and event data synthesis.<p>Roadmap<p>Multi-child support: I am adding color-coded flares to track which child needs to be at which activity.<p>School Calendar Integration: This will include automated syncing for Melbourne\u2019s complex private and public school terms.<p>Public Holiday Optimization: I am building a feature to help families maximize long weekends by identifying the best days to take leave.<p>I would love for some feedback so I can add events for other cities in future", "author": "wantrepreuneur", "timestamp": "2026-01-31T09:34:56+00:00", "score": 2, "num_comments": 0, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:39.761000+00:00", "processed": false}
{"id": "hn_story_46834552", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46834552", "title": "Show HN: EditorWatch \u2013 Detect AI cheating by analyzing how students write code", "text": "Hi HN,<p>I built EditorWatch to help CS instructors detect AI-generated code in programming assignments.<p>Current plagiarism detectors only look at the final code. Students copying from ChatGPT slip through easily. EditorWatch is different - it monitors HOW code is written, not just what&#x27;s written.<p>A VS Code extension tracks coding patterns:\n- Sudden code appearance (paste bursts)\n- Lack of natural trial-and-error\n- Robotic typing patterns\n- Perfect-first-time code<p>It generates an authenticity score (0-10) with visualizations for each submission.<p>Privacy-conscious design:\n- No video&#x2F;screenshots, only metadata (timestamps, character counts)\n- Only tracks specified file types (<i>.py, </i>.js, etc.)\n- Students must explicitly opt-in\n- Data deleted after grading\n- 100% open source<p>Free for education, paid for commercial use. Deploys easily on Railway or your own server.<p>I know it&#x27;s not foolproof - determined students can bypass it. But it raises the bar significantly and works as one tool alongside code reviews and oral exams.<p>Would love feedback from educators and developers on the approach!", "author": "vicnas", "timestamp": "2026-01-31T08:14:05+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-31T17:15:42.780178+00:00", "processed": false}
{"id": "hn_story_46834415", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46834415", "title": "Show HN: Oyster Bot \u2013 AI assistant for your phone, powered by Claude Code", "text": "I wanted Claude Code on my phone without running extra infrastructure. Existing solutions felt heavy..gateway servers, native apps, etc.<p>This is a Telegram bot that spawns the Claude CLI and pipes responses back to you. Clone, npm install, add your bot token, run it.<p>Works with Claude Pro&#x2F;Max (no API key required). You can whitelist users, restrict which tools Claude can access, and add custom commands via plugins.<p>Only ~400 lines of JS.", "author": "timfinnigan", "timestamp": "2026-01-31T07:52:17+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:43.386743+00:00", "processed": false}
{"id": "hn_story_46834183", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46834183", "title": "Show HN: JProx \u2013 Japan residential proxy API for scraping Japanese sites", "text": "I built JProx to solve a specific problem: scraping Japanese sites (Mercari, Rakuten, SUUMO) that aggressively block foreign IPs and datacenter proxies.<p>Features:\n- Japanese residential IPs (Tokyo)\n- Simple REST API with Claude MCP support\n- 1,000 free requests&#x2F;month\n- $7&#x2F;mo for 5,000 requests<p>Built with: FastAPI, Next.js, PostgreSQL<p>I&#x27;m a solo developer in Japan. Would love feedback on the pricing and API design.", "author": "yoshi_dev", "timestamp": "2026-01-31T07:03:51+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-31T17:15:44.925035+00:00", "processed": false}
{"id": "hn_story_46833472", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46833472", "title": "Show HN: SOTA NLP Models", "text": "Hi everyone, I needed to break sentences into their individual words and figure out what part of speech each word is. Explosion&#x27;s Spacy models are absolutely incredible for English, clearly some top tier engineering that I could never come close to, but for other languages they&#x27;re quite weak. I created my own by taking Spacy outputs, cleaning them up with an LLM, and then fine-tuning a Gemma model on that. The result is extremely good and consistent results for 7 languages. The models are also much cheaper and more consistent than would be possible with ChatGPT. (For example, should &quot;don&#x27;t&quot; be treated as &quot;don&#x27;t&quot; or &quot;do&quot;, &quot;n&#x27;t&quot;? ChatGPT will pick one randomly.)<p>It sounds simple, and I&#x27;m not going to say it was the most complicated thing ever, but there were quite a few steps involved in getting it right. Getting LLMs to do the cleanup task consistently is very hard. You wouldn&#x27;t think it but there are often multiple ways to break down a sentence.<p>An interesting part was structuring the model output so it could use the exact same tokens as the input. Most tokens are prefixed by a space, so you want the model&#x27;s &quot;desired output&quot; to also involve the words prefixed by a space.  It makes the task much easier because the model doesn&#x27;t have to learn the mapping between prefixed and unprefixed tokens. Doing that instantly made my models start performing much better.", "author": "ChadNauseam", "timestamp": "2026-01-31T04:24:46+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-01-31T17:15:48.956728+00:00", "processed": false}
