{"id": "hn_story_46489730", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46489730", "title": "Tell HN: Perplexity Has Unspecified Chr Limits for Session Export", "text": "Hello all,<p>I discovered, the hard way, that exporting Perplexity sessions to PDF results in substantial content loss when the page is ~90 pages.<p>After opening a ticket on the matter, a brief dialogue with a rep proved unhelpful and confusing. It was stated that the Export as PDF feature only exports individual &quot;threads&quot;, and that to export an entire session, each so-called thread must be individually selected and exported. This is simply wrong.<p>In practice, there is no method to select threads through the Top-Right&#x2F; 3-dot menu&#x2F;Export as PDF option. Testing this with various sessions from 1 to 170 page exports showed no indication that threads were relevant.<p>Exports under 90 pages tend* to retain all content, while a 93-page didn&#x27;t, but a 95 and 170 page export did. This indicates that the character limit (if that&#x27;s the cause) is variable, as 170 pages is almost guaranteed to contain more characters than 90.<p>The fundamental point here, whatever the cause, is that data loss is inevitable under the present UI with its absence of documentation, notices etc.<p>*I observed changes after submitting the ticket and modifications have already been made. The situation was worse before, and now less worse, but still applies.", "author": "eth0up", "timestamp": "2026-01-04T16:53:14+00:00", "score": 2, "num_comments": 1, "products": ["perplexity"], "categories": ["content_clarity", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-04T17:09:13.469176+00:00", "processed": false}
{"id": "hn_story_46489710", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46489710", "title": "Show HN: Moo.md \u2013 Mental Models for Claude Code", "text": "Claude Code is fast. But fast at what?<p>moo.md makes it a thinking partner, not just a task executor. Mental models. Confidence gates. Learnings that persist.<p>When you&#x27;re stuck on a decision, it runs a pre-mortem. When debugging, Ishikawa. When you need perspective, it channels Rich Hickey or Paul Graham.<p>Plugins for decisions, writing, and design.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;saadshahd&#x2F;moo.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;saadshahd&#x2F;moo.md</a>", "author": "saadshahd", "timestamp": "2026-01-04T16:51:22+00:00", "score": 1, "num_comments": 1, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-04T17:09:13.599709+00:00", "processed": false}
{"id": "hn_comment_46489722", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46489722", "title": "Re: Show HN: Moo.md \u2013 Mental Models for Claude Code...", "text": "What I actually get from this:<p>1. No more &quot;good work&quot; fluff. Claude challenges decisions instead of agreeing. The expert simulations come with confidence ratings and citations \u2014 if it&#x27;s channeling Hickey at 7&#x2F;10, it tells you why.<p>2. Compaction anxiety is gone. I used to dread long sessions because insights would disappear when context got too long. Now learnings persist in ~&#x2F;.claude&#x2F;learnings&#x2F; \u2014 patterns from last month are still there.<p>3. Grounded opinions. When I ask &quot;what would Rich Hickey think?&quot;, the response cites his actual talks and documented philosophy. Not hallucinated advice.<p>The mental models aren&#x27;t the point. The point is Claude stops being agreeable and starts being useful.", "author": "saadshahd", "timestamp": "2026-01-04T16:52:30+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-04T17:09:13.632410+00:00", "processed": false}
{"id": "hn_comment_46489604", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46489604", "title": "Re: I Was Wrong About Claude Code Skills...", "text": "&gt; But now, for the first time, I tried Claude Code&#x27;s Skills, and they blew me away, because I wasn&#x27;t aware that those skills are injected automatically<p>They work so well because they&#x27;re also &quot;baked in&quot; the training run of the model. The concept is simple, but training it to actually use it unlocks the &quot;wow&quot; factor. (using cc with other models, not trained specifically for this, reveals how big training is)<p>They also highlight this in their docs, you don&#x27;t need to teach claude about skills. Or to write new skills. It &quot;knows&quot; what skills are, how to use, edit them etc.", "author": "NitpickLawyer", "timestamp": "2026-01-04T16:40:12+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-04T17:09:15.120081+00:00", "processed": false}
{"id": "hn_comment_46489560", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46489560", "title": "Re: AI Sycophancy Panic...", "text": "I still suspect what happened was when the midwits all got access to ChatGPT etc and started participating in the A&#x2F;B tests, they strongly selected for responses that agreed with them regardless of whether they were actually correct.<p>Some of us want to be told when and why we\u2019re wrong, and somewhere along the way AI models were either intentionally or unintentionally guided away from doing it because it improved satisfaction or engagement metrics.<p>We already know from decades of studies that people prefer information that confirms their existing beliefs, so when you present 2 options with a \u201cWhich answer do you prefer?\u201d selection, it\u2019s not hard to see how the one that begins with \u201cYou\u2019re absolutely right!\u201d wins out.", "author": "transcriptase", "timestamp": "2026-01-04T16:37:00+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-04T17:09:20.640316+00:00", "processed": false}
{"id": "hn_comment_46487587", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46487587", "title": "Re: Show HN: I replaced beads with a faster, simpler M...", "text": "I&#x27;ve been running long duration coding agents with Claude Code for about 6 months now. Steve Yegge released Beads back in October and I found that giving Claude tools for proper task tracking was a massive unlock. But Beads grew massively in a short time and every release made it slower and more frustrating to use. I started battling it several times a week as its background daemon took to syncing the wrong things at the wrong times.<p>Over the holidays I finally ripped it out and wrote ticket as a replacement. It keeps the core concept I actually cared about (graph-based task dependencies) but drops everything else.<p>ticket a single file bash script built on coreutils managing flat files. You don&#x27;t need to index everything with SQLite when you have awk. It&#x27;s just a small plumbing utility that gets out of your way so you can get to work.<p>Would love feedback on gaps. I built this for my own agent workflows so there are probably use cases I haven&#x27;t thought about.", "author": "wild_egg", "timestamp": "2026-01-04T13:09:08+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-04T17:09:25.270805+00:00", "processed": false}
{"id": "hn_comment_46486692", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46486692", "title": "Re: Reelsy \u2013 Multi-Agent AI System for Short Video Cre...", "text": "Hey HN,<p>We&#x27;ve been working on Reelsy for the past few months and wanted to share what we&#x27;ve learned building a multi-agent AI system for video content creation.<p>The Problem<p>Creating short-form video content (YouTube Shorts, TikTok, Reels) at scale is brutal. A single 60-second video costs $500+ with freelancers and takes 3-5 hours. For creators who need to post daily, this is unsustainable.<p>The bigger technical challenge: AI-generated images have a consistency problem. Ask any image model to generate &quot;the same character&quot; across 15 scenes, and you&#x27;ll get 15 different-looking people.<p>Our Approach: Multi-Agent Architecture<p>Instead of throwing one LLM at the problem, we built a 5-agent pipeline inspired by actual film production:<p>Director Agent - Breaks down the story concept into a shot list\nScriptwriter Agent - Writes dialogue and narration for each scene\nCharacter Designer Agent - Creates reference images and locks character identity\nCinematographer Agent - Determines camera angles, lighting, composition\nHook Generator Agent - Optimizes the first 3 seconds for each platform\nThe agents communicate through structured outputs and can iterate on each other&#x27;s work. We found this beats a single mega-prompt approach by ~40% on our internal quality benchmarks.<p>Character Consistency Solution<p>We use Gemini 2.5 Flash (&quot;Nano Banana&quot; on LMArena) with reference image anchoring. The Character Designer creates a canonical reference image, then every subsequent scene generation includes this reference with specific instructions to maintain identity.<p>Current results: 85%+ consistency across 15-20 scene generations. Not perfect, but usable for most content types.<p>Multi-agent coordination is harder than it looks. Race conditions, agent disagreements, and context window limits are real problems.<p>Character consistency is still the hardest unsolved problem in AI video. Reference anchoring helps but isn&#x27;t bulletproof.<p>Platform-specific optimization matters more than raw quality. A slightly lower quality video with proper hooks outperforms beautiful content with weak openings.<p>Current Pricing<p>~$0.70 per video (15-20 scenes, voiceover, music). We&#x27;re not trying to undercut human editors on quality, but for high-volume content needs, this makes previously impossible workflows viable.<p>Would love feedback from the HN community, especially on the multi-agent architecture. Happy to answer questions about our implementation choices.", "author": "smallmartial", "timestamp": "2026-01-04T10:29:54+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-04T17:09:32.086089+00:00", "processed": false}
{"id": "hn_comment_46485869", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46485869", "title": "Re: Pocket Brain \u2013 offline AI chat that runs in the br...", "text": "I travel a lot and got tired of ChatGPT being unusable on flights (no Wi\u2011Fi), so I built a browser-based AI chat that runs locally. It downloads a small open model once (cached in IndexedDB), then runs inference on-device via WebGPU in a Web Worker, so it works offline and nothing leaves your machine. Trade-offs: smaller models (not GPT\u20114), first load is a big download, and older hardware&#x2F;mobile can struggle. However it works, and it goes into the direction of open-source, AI, and smaller, smarter models. Curious for feedback on the UX for model downloads, and whether people think browser-local AI is a viable direction.", "author": "mihailyonchev", "timestamp": "2026-01-04T07:46:55+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-04T17:09:38.429664+00:00", "processed": false}
{"id": "hn_story_46485367", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46485367", "title": "Show HN: I built an AI optimized for venting, not working", "text": "Hi HN,<p>I built AnnaAi.App because I was tired of AI &quot;copilots&quot; always trying to make me more productive or efficient.<p>Sometimes, you don&#x27;t need a solution, a to-do list, or a lecture on emotional management. You just need to vent.<p>Most current LLMs are guardrailed to be overly objective or polite. If you complain about a bad boss or a terrible day, they tend to say &quot;I understand, but have you tried looking at it from their perspective?&quot; which is often the last thing you want to hear at 2 AM.<p>I designed Anna to be strictly non-judgmental and supportive. Think of it as a digital &quot;Tree Hole&quot; (a safe space to shout into the void). She is prompted to take your side, validate your feelings, and even &quot;roast&quot; the things that annoy you, rather than trying to fix them.<p>It&#x27;s an experiment in &quot;Anti-productivity&quot; AI. Would love to hear your thoughts on this approach to emotional alignment.", "author": "fengyiqicoder", "timestamp": "2026-01-04T05:56:28+00:00", "score": 2, "num_comments": 3, "products": ["copilot"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-04T17:09:39.964536+00:00", "processed": false}
{"id": "hn_comment_46485843", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46485843", "title": "Re: Show HN: Claude Reflect \u2013 Auto-turn Claude correct...", "text": "I won&#x27;t lie, this sounds like a recipe for context rot.<p>LLMs degrade as the context &#x2F; prompt size grow. For that reason I don&#x27;t even use a CLAUDE.md at all.<p>There are very few bits that I do need to routinely repeat, because those are captured by linters&#x2F;tests, or prevented by subdividing the tasks in small-enough chunks.<p>Maybe at times I wish I could quickly add some frequently used text to prompts (e.g. &quot;iterate using `make test TEST=foo`&quot;), but otherwise I don&#x27;t want to delegate context&#x2F;prompt building to an AI - it would quickly snowball.", "author": "vemv", "timestamp": "2026-01-04T07:40:24+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-04T17:09:42.460679+00:00", "processed": false}
{"id": "hn_comment_46484809", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46484809", "title": "Re: I built a tool to create AI agents that live in iM...", "text": "Hey everyone, I made this thing:\n<a href=\"https:&#x2F;&#x2F;tryflux.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;tryflux.ai&#x2F;</a><p>Context: I&#x27;ve tried probably 15 different AI apps over the past year. ChatGPT, note-taking apps, productivity apps, all of it. But most of them are just clutter on my iphone.<p>They live in some app I have to deliberately open. And I just... don&#x27;t.<p>But you know what I open 50 times a day without thinking? iMessage.<p>So out of mild frustration with the &quot;AI app graveyard&quot; on my phone, I built Flux.<p>What it does:<p>You describe a personality and what you want the agent to do<p>In about 2 minutes, you have a live AI agent in iMessage<p>Blue bars. Native. No app download for whoever texts it<p>No code required<p>The thesis that got us here: AI is already smart enough. The bottleneck is interaction. Dashboards get forgotten. Texts get answered.<p>This was also my first time hitting #1 on Product Hunt, which was surreal.<p>We&#x27;re very early and probably broke something. If you try it, feedback is super welcome, weird edge cases, &quot;this doesn&#x27;t work,&quot; or &quot;why would anyone use this&quot; comments all help.<p>That&#x27;s all. Happy to answer questions.", "author": "danielsdk", "timestamp": "2026-01-04T04:10:06+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2026-01-04T17:09:43.253931+00:00", "processed": false}
