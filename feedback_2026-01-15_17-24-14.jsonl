{"id": "hn_story_46635859", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46635859", "title": "Show HN: I'm building an open-source AI agent runtime using Firecracker microVMs", "text": "Hello Hacker News! I&#x27;m Mark. I&#x27;m building Moru, an open-source runtime for AI agents that runs each session in an isolated Firecracker microVM. It started as a fork of E2B, and most of the low-level Firecracker runtime is still from upstream.<p>It lets you run agent harnesses like Claude Code or Codex in the cloud, giving each session its own isolated microVM with filesystem and shell access.<p>The repo is: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;moru-ai&#x2F;moru\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;moru-ai&#x2F;moru</a><p>Each VM is a snapshot of a Docker build. You define a Dockerfile, CPU, memory limits, and Moru runs the build inside a Firecracker VM, then pauses and saves the exact state: CPU, dirty memory pages, and changed filesystem blocks.<p>When you spawn a new VM, it resumes from that template snapshot. Memory snapshot is lazy-loaded via userfaultfd, which helps sandboxes start within a second.<p>Each VM runs on Firecracker with KVM isolation and a dedicated kernel. Network uses namespaces for isolation and iptables for access control.<p>From outside, you talk to the VM through the Moru CLI or TypeScript&#x2F;Python SDK. Inside, it&#x27;s just Linux. Run commands, read&#x2F;write files, anything you&#x27;d do on a normal machine.<p>I&#x27;ve been building AI apps since the ChatGPT launch. These days, when an agent needs to solve complex problems, I just give it filesystem + shell access. This works well because it (1) handles large data without pushing everything into the model context window, and (2) reuses tools that already work (Python, Bash, etc.). This has become much more practical as frontier models have gotten good at tool use and multi-step workflows.<p>Now models run for hours on real tasks. As models get smarter, the harness should give models more autonomy, but with safe guardrails. I want Moru to help developers focus on building agents, not the underlying runtime and infra.<p>You can try the cloud version without setting up your own infra. It&#x27;s fully self-hostable including the infra and the dashboard. I&#x27;m planning to keep this open like the upstream repo (Apache 2.0).<p>Give it a spin: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;moru-ai&#x2F;moru\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;moru-ai&#x2F;moru</a> \nLet me know what you think!<p>Next features I&#x27;m working toward:<p>- Richer streaming: today it&#x27;s mostly stdin&#x2F;stdout. That pushes me to overload print&#x2F;console.log for control-plane communication, which gets messy fast. I want a separate streaming channel for structured events and coordination with the control plane (often an app server), while keeping stdout&#x2F;stderr for debugging.<p>- Seamless deployment: a deploy experience closer to Vercel&#x2F;Fly.io.<p>- A storage primitive: save and resume sessions without always having to manually sync workspace and session state.<p>Open to your feature requests or suggestions.<p>I&#x27;m focusing on making it easy to deploy and run local-first agent harnesses (e.g., Claude Agent SDK) inside isolated VMs. If you&#x27;ve built or are building those, I&#x27;d appreciate any notes on what&#x27;s missing, or what you&#x27;d prioritize first.", "author": "markoh49", "timestamp": "2026-01-15T17:18:30+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-15T17:24:15.608124+00:00", "processed": false}
{"id": "hn_story_46634915", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46634915", "title": "Ask HN: How to overcome the limit of roles in LLM's", "text": "Our use case is not uncommon, we are developing tools so that people can install LLM&#x27;s on their e-commerces.<p>But there are some interesting challenges that I feel can&#x27;t be solved unless inference providers allow us to include the concept additional entities in a conversation.<p>As far as I know the three most basic ones shared alongside all providers are:<p>- System<p>- Assistant<p>- User<p>That&#x27;s fine and it allows for simple conversational-based approaches (ChatGPT, Claude, Gemini, etc). However in our use case we allow our customers (not the final user who is talking with the AI) to configure the AI in different ways (personality, RAG, etc), which poses a problem.<p>If we inject those customer settings in the System prompt then that&#x27;s a risk because there might be conflicting prompts with our internal rules. So the easiest option is to &quot;clean&quot; the customer prompts before injecting them, but that feels hacky and just adds one more level of indirection. Cleaning the prompt and injecting it with common patterns like XML tags seems to help a bit but still feels extremely risky for some reason.<p>Injecting it in the assistant or user also seems flaky and prone to prompt injection.<p>Creating a fake tool call and result like &quot;getPersonalityConfiguration&quot; seems to work the best, from our testing it is treated as something between the System and Assistant roles. And our top system prompt rules are still respected while allowing the customer some freedom to configure the AI.<p>The problem comes when you need to add more parties to what essentially is a 2 entity conversation. Sometimes we want external agents to chime in a conversation (via subagents or other methods) and there is no good way to do that AFAIK. It gets the occasional confusion and starts mixing up who is who.<p>One of our typical scenarios that we need to model:<p>System: Your rules are: You will never use foul language...<p>Store owner: You are John the customer agent for store Foo...<p>User: Do you have snowboards in stock?<p>Assistant-&gt;User: Let me check with the team. I&#x27;ll get back to you soon.<p>System-&gt;Team: User is asking if we have snowboards in stock. Do we?<p>Team: We do have snowboards in stock.<p>Team-&gt;User: Yes we do have snowboards in stock!<p>User: Perfect, if I buy them will the courier send it to my country? [country name].<p>Assistant-&gt;User: Let me check, I need to see if our courier can ship a snowboard to your country.<p>Assistant-&gt;Third party logistics: I have a user from [country] interested in buying a snowboard. The dimensions are X by Y and the weight is Z. We would send it from our logistics center located at [address].<p>Third party logistics -&gt; Assistant: Yes we can do it, it will be 29.99 for the shipping.<p>Assistant-&gt;User: Yes they can ship it to [country] but it does incur in 29.99 extra charge...<p>I obviated tool calls and responses, but that&#x27;s basically the gist of it. Spawning sub-agents that have the context of the main conversation works but at some point it is limiting (we need to copy all personality traits and relevant information via summarization or injecting the conversation in a manner that the sub-agent won&#x27;t get confused). It feels like an anti-pattern and trying to fight the intended use case of LLM&#x27;s, which seems to be focused in conversation between two entities with the occasional external information going in through System or tool calling.<p>It would be amazing if we could add custom roles to model messages, still with special cases like agent or assistant.<p>Has anyone worked with similar problems? How did you solve it? Is this solved in the model lab or at the inference provider level (post-training)?", "author": "weli", "timestamp": "2026-01-15T16:23:35+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-15T17:24:21.309579+00:00", "processed": false}
{"id": "hn_story_46634848", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46634848", "title": "Apple's new Google Gemini deal sounds bigger, better than expected", "text": "", "author": "gmays", "timestamp": "2026-01-15T16:20:19+00:00", "score": 3, "num_comments": 0, "products": ["gemini"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-15T17:24:21.474493+00:00", "processed": false}
{"id": "hn_story_46634773", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46634773", "title": "How do you pick a Coding Agent HN?", "text": "There&#x27;s lots of models benchmark out there, but how do you evaluate coding agents?<p>I&#x27;ve been seeing a lot of OpenCode fuzz on HN lately, because of Anthropic disabling their access to the private subscription endpoints, and I confess it made me feel like I could be missing out on something though I can&#x27;t tell for sure.<p>There&#x27;s also Amp Code who seems to be picking up traction, and, although more on the IDE side, I have tried Kiro through AWS Credits and it surprisingly outperforms Claude Code for me in some cases but didn&#x27;t fully bait me into the switch.<p>Codex works as good as Claude Code for me but I like Claude&#x27;s UX and Opus 4.5 better.<p>Are there any reliable Coding Agents benchmark out there? What is your take?", "author": "hmokiguess", "timestamp": "2026-01-15T16:15:41+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-15T17:24:22.047843+00:00", "processed": false}
{"id": "hn_story_46634178", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46634178", "title": "Show HN: I lost \u20ac50K to non-paying clients, so I built an AI contract platform", "text": "Hey HN,<p>I&#x27;m Roma, 21, from Bucharest, Romania. At 20 I was running a 12-person design studio doing \u20ac250K&#x2F;year.<p>Then I lost \u20ac50K+ to clients who never paid. No contracts, just trust. Studio collapsed, I took \u20ac40K in debt.<p>That experience led me to build Accordio \u2013 an AI-powered contract and payment platform for freelancers.<p>The core idea: paste your meeting notes, AI extracts scope&#x2F;deliverables&#x2F;payment terms and generates a proposal.<p>Client accepts \u2192 contract auto-generates. Contract signed \u2192 invoice auto-generates.<p>Everything interlinked, no re-entering data across 4 different tools.<p>Tech stack: Next.js, Supabase, Claude for AI, Stripe Connect for payments, custom e-signature system (built from scratch \u2013 DocuSign API was too expensive).<p>The AI has memory...it learns your rates, terms, clients over time. Not just one-off generation.<p>Built it for freelancers like me.<p>Been &quot;vibe coding&quot; this for 2 years... before the term even existed.<p>Would love feedback from HN.", "author": "deduxer", "timestamp": "2026-01-15T15:45:26+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-15T17:24:25.738351+00:00", "processed": false}
{"id": "hn_comment_46634084", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46634084", "title": "Re: SaaS Is Not Dead...", "text": "while it makes sense that companies are unlikely to want to maintain a bunch of auxiliary saas tools just because Claude Code exists, it might be the case that Claude Code massively reduces the barrier to entry for software companies, and in theory the maintenance costs as well. So while companies will still outsource a lot, their options for outsourcing could go up a tonne, so even though companies are still spending money on external options, those options see margin pressure that dilute SaaS from 70% margin in the era where building great software was hard, to ~0%, if building great software suddenly becomes trivial.", "author": "r_thambapillai", "timestamp": "2026-01-15T15:40:09+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-15T17:24:27.721587+00:00", "processed": false}
{"id": "hn_story_46633863", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46633863", "title": "Show HN: VerityNgn\u2013Open-source AI that fact-checks YouTube videos", "text": "I built an open-source system that generates truthfulness reports for YouTube videos using multimodal AI and a counter-intelligence approach.<p>*Live demo:* <a href=\"https:&#x2F;&#x2F;verityngn.streamlit.app\" rel=\"nofollow\">https:&#x2F;&#x2F;verityngn.streamlit.app</a><p>*Documentation:* <a href=\"https:&#x2F;&#x2F;hotchilianalyticsllc.mintlify.app\" rel=\"nofollow\">https:&#x2F;&#x2F;hotchilianalyticsllc.mintlify.app</a><p>*Repo:* <a href=\"https:&#x2F;&#x2F;github.com&#x2F;hotchilianalytics&#x2F;verityngn-oss\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;hotchilianalytics&#x2F;verityngn-oss</a><p>*Substack Article:* <a href=\"https:&#x2F;&#x2F;ajjcop.substack.com&#x2F;p&#x2F;i-built-an-ai-that-fact-checks-youtube\" rel=\"nofollow\">https:&#x2F;&#x2F;ajjcop.substack.com&#x2F;p&#x2F;i-built-an-ai-that-fact-checks...</a><p>### The Problem<p>Existing fact-checking tools only analyze text transcripts. They miss on-screen graphics, visual demonstrations, and the multimodal nature of video. Worse, when you search for evidence, you often get promotional press releases that <i>confirm</i> false claims because the misinformation ecosystem is SEO-optimized.<p>### How VerityNgn Works<p>1. *Multimodal analysis*: Uses Gemini 2.5 Flash (1M token context) to analyze video frames at 1 FPS \u2014 audio, OCR, visuals, and transcript together.\n2. *Intelligent Segmentation*: Automatically calculates optimal segment sizes based on model context windows (reduces API calls by 86% for typical 30-min videos).\n3. *Enhanced claim extraction*: Multi-pass extraction with specificity scoring (0-100) and &quot;absence claim&quot; generation (identifying what&#x27;s NOT mentioned, like missing FDA disclaimers).\n4. *Counter-intelligence*: Actively hunts for contradiction \u2014 searching for YouTube review&#x2F;debunking videos and detecting self-referential press releases (94% precision).\n5. *Probabilistic output*: A calibrated THREE-state distribution (TRUE&#x2F;FALSE&#x2F;UNCERTAIN) based on Bayesian aggregation of source validation power.<p>### Results (200-claim test set)<p>- 75% accuracy vs. ground truth (95% CI: 61-85%)\n- +18% improvement from counter-intel on misleading content\n- Well-calibrated (Brier score = 0.12, ECE = 0.04)\n- Cost: $0.50\u2013$2.00 per video<p>### Tech Stack<p>- Python 3.12, Gemini 2.5 Flash via Vertex AI\n- LangChain&#x2F;LangGraph for orchestration\n- Streamlit UI, Cloud Run backend\n- yt-dlp for video download\n- Google Custom Search + YouTube Data API<p>### Honest Limitations<p>- English only\n- YouTube only (no TikTok&#x2F;Instagram yet)\n- ~25% error rate (75% accuracy means 25% wrong)\n- Susceptible to coordinated fake review campaigns\n- No human-in-the-loop<p>### Why Open Source<p>Misinformation is too important to solve behind closed doors. The methodology needs to be transparent and auditable. Full research papers with step-by-step calculations are in the `papers&#x2F;` directory.<p>Looking for feedback on the approach and contributions (especially: multi-language support, additional platforms, expanded evidence sources).", "author": "ajjcoppola", "timestamp": "2026-01-15T15:28:19+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-15T17:24:28.013311+00:00", "processed": false}
{"id": "hn_comment_46633419", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46633419", "title": "Re: Show HN: Ctrl \u2013 Open-source AI OS where each app h...", "text": "Hey HN,<p>Ctrl is an open-source AI desktop where AI builds React apps with SQLite databases - and here&#x27;s the key part: each app comes with an AI assistant that can read and reason about your actual data.<p>The difference:<p>ChatGPT generates code snippets. Cursor helps you write code. Ctrl creates complete apps where the AI understands your specific data:<p>- &quot;Build me an invoice tracker&quot; \u2192 Creates React app + SQLite database\n- &quot;Show me unpaid invoices over 30 days&quot; \u2192 AI queries YOUR invoice data and shows results\n- &quot;Add client health scoring based on payment speed&quot; \u2192 AI analyzes your historical data, proposes scoring formula, updates schema<p>Technical approach:<p>- Real TypeScript&#x2F;React apps compiled in-browser (esbuild-wasm)\n- Each app = folder with TSX files + SQLite database + .ctrl&#x2F;context.md\n- The context.md documents data structure and purpose - AI reads this before any modifications\n- Built on Claude&#x2F;GPT via your API key, runs entirely local, MIT licensed<p>Current state:<p>Beta on macOS, Windows, Linux. We&#x27;re a small team in Prague, going fully open source because we believe the innovation is in execution, not hiding code.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;CtrlAIcom&#x2F;ctrl\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CtrlAIcom&#x2F;ctrl</a>\nDemo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;6yWZpNCK8mw\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;6yWZpNCK8mw</a>\nWebsite: <a href=\"https:&#x2F;&#x2F;ctrlai.com\" rel=\"nofollow\">https:&#x2F;&#x2F;ctrlai.com</a><p>Would love feedback, especially on the data-aware AI architecture.", "author": "rado12", "timestamp": "2026-01-15T14:57:27+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-15T17:24:31.706303+00:00", "processed": false}
{"id": "hn_comment_46633116", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46633116", "title": "Re: Vibe \u2013 Claude Skill to let Claude Code read screen...", "text": "Hey HN,<p>I&#x27;m the creator of vibe. I built this because I was tired of describing UI bugs to Claude Code when I could just show them.<p>The problem: When debugging with AI assistants, visual context gets lost. You end up typing &quot;the button is misaligned by about 3 pixels&quot; when you could just show a screenshot.<p>What it does:\n- Captures screen regions using macOS&#x27;s native screencapture\n- Bundles the screenshot with git status, diffs, and terminal logs\n- Feeds everything to Claude Code for analysis and fixes<p>How I built it:<p>The core insight was that Claude Code supports custom commands via markdown files in ~&#x2F;.claude&#x2F;commands&#x2F;. So I created two commands:<p>1. &#x2F;vibe-select - Calls screencapture -i, saves to .vibedbg&#x2F;region.png, records metadata\n2. &#x2F;vibe-ask - Constructs a prompt with:\n   - Screenshot file path (Claude Code can read images)\n   - git status and git diff output\n   - Last N lines of terminal logs\n   - User&#x27;s debug instruction<p>The whole thing is ~200 lines of JS + shell scripts. I deliberately kept it minimal\u2014no frameworks, no dependencies beyond Node.js.<p>Technical details:\n- Uses Node.js for the CLI\n- Integrates with Claude Code via custom commands\n- Stores debug sessions in .vibedbg&#x2F; directory\n- Works from subdirectories (finds nearest .vibedbg&#x2F; or git root)\n- MIT licensed<p>Demo video (2:43): <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;tCvOZ0IUxm0\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;tCvOZ0IUxm0</a><p>It&#x27;s made my debugging workflow much faster, especially for frontend issues. The workflow is now:\n1. See bug \u2192 &#x2F;vibe-select\n2. Capture screen \u2192 &#x2F;vibe-ask &quot;fix this&quot;\n3. Claude analyzes, proposes fix, applies it\n4. Done<p>I&#x27;d love feedback on the approach and any ideas for improvements. Happy to answer questions about the implementation!<p>Some questions I&#x27;m thinking about:\n- Should I add Linux&#x2F;Windows support? (Would need different screenshot tools )\n- Is there value in video capture instead of just screenshots?\n- Would this work better as a VSCode extension?<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Blurjp&#x2F;vibe\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Blurjp&#x2F;vibe</a>", "author": "blurjp", "timestamp": "2026-01-15T14:37:53+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-15T17:24:33.341876+00:00", "processed": false}
{"id": "hn_story_46633058", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46633058", "title": "Show HN: Azurite \u2013 Triage Linear and GitHub issues using MCP (Deck of Cards UI)", "text": "Hello HN,<p>I\u2019m the builder behind Azurite. I built this because I found that my bottleneck wasn&#x27;t the volume of notifications (Linear, Slack, GitHub), but the context switching cost required to process them.<p>The Problem: Every time I opened a Linear ticket, I had to:<p>Read the ticket.<p>Search Slack to find the thread where the decision was made.<p>Check GitHub to see if the PR was actually merged. Result: Context switching 3 times just to make one decision.<p>What it is: Azurite is a unified triage interface that treats notifications as a deck of cards. The &quot;magic&quot; is that it pre-fetches the Context Graph. Instead of just showing you &quot;Fix Login Bug,&quot; it scans the ticket description, finds the linked Slack thread URL, fetches the summary via the Slack API, and pins it to the card. You see the answer, not just the task.<p>The Architecture:<p>Frontend: React + Framer Motion (Optimistic UI with spring physics for 60fps interactions).<p>Backend: Python (FastAPI) handling Async Webhooks (no polling lag).<p>Orchestration: Built on Anthropic&#x27;s Model Context Protocol (MCP). We treat SaaS tools as standardized &quot;Servers&quot; and the backend as the &quot;Client.&quot;<p>Intelligence: Hybrid Chain. Llama 3.3 (via Groq) for sub-second classification, GPT-4o-mini for deep context summarization.<p>Data: SQLite (Edge-Cached). We map the relationships between external tickets and conversations server-side for speed.<p>A Note on Access (The Google 100-User Limit): Because Azurite requires sensitive scopes (Gmail&#x2F;Calendar) to function, I am currently restricted by Google&#x27;s unverified app limit to 100 active tokens. This is a hard technical ceiling until my security audit is complete.<p>I am manually cycling users in batches of 10 every day as I rotate inactive users out. I am prioritizing the HN queue for the next 24 hours.<p>If you want to help break the beta (or critique my implementation of MCP), get in line here: <a href=\"https:&#x2F;&#x2F;azurite-labs.vercel.app&#x2F;?ref=AZ-PJQEY5\" rel=\"nofollow\">https:&#x2F;&#x2F;azurite-labs.vercel.app&#x2F;?ref=AZ-PJQEY5</a>", "author": "QaysHaji", "timestamp": "2026-01-15T14:33:18+00:00", "score": 3, "num_comments": 1, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-15T17:24:33.686149+00:00", "processed": false}
{"id": "hn_comment_46634648", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46634648", "title": "Re: I spent a year on Linux and forgot to miss Windows...", "text": "I love Linux and use it daily, but this paragraph gave me pause:<p>&quot;I\u2019ve spent dozens of hours combing through Reddit threads, analyzing old Stack Overflow solutions, and, in times of true desperation, asking AI chatbots like Mistral\u2019s Le Chat and Anthropic\u2019s Claude for help deciphering error messages. Luckily, the Linux community is also very supportive. If you\u2019re willing to ask for help, or at least do a little troubleshooting, you\u2019ll be able to work out any problems that come your way.&quot;<p>There are many people -- like my Mom or Dad, for example -- who will never find this appealing and are likely to dig themselves into deeper holes trying to fix system issues on the command line. That&#x27;s why Steve Jobs was on the money when he talked about a computer that was as intuitive as an appliance -- it has to &quot;just work&quot; for most normies. While I&#x27;m as frustrated with Windows as the next person, I&#x27;d probably just hand the average person a Mac mini instead of popping a linux distro on their machine if they needed a new computer (though if all they are doing is just browsing the web and reading emails, a ubuntu install is probably fine).", "author": "lordleft", "timestamp": "2026-01-15T16:09:05+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["error_messages", "navigation"], "sentiment": null, "collected_at": "2026-01-15T17:24:35.321399+00:00", "processed": false}
{"id": "hn_story_46632472", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46632472", "title": "Show HN: Webhook Debugger \u2013 OS Alternative to RequestBin with Replay,SSRF Checks", "text": "Hi HN,<p>I built *Webhook Debugger* because existing tools were either ephemeral (RequestBin) or required persistent tunnels (ngrok) that I couldn&#x27;t leave running on a staging server.<p>*Repo*: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ar27111994&#x2F;webhook-debugger-logger\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ar27111994&#x2F;webhook-debugger-logger</a><p>*The Stack*:\nNode.js, Express, Apify SDK (for storage&#x2F;dataset abstraction), Docker.<p>*Key Technical Decisions*:<p>1.  *Global SSE Heartbeat*: Instead of `setInterval` per connection (O(N) timers), we use a single global timer to flush heartbeats to all `res` objects.\n2.  *SSRF Protection*: Custom validator (`src&#x2F;utils&#x2F;ssrf.js`) that resolves DNS and checks against a blocklist (including AWS Metadata IP `169.254.169.254`) before forwarding&#x2F;replaying requests.\n3.  *Resilience*: The Replay engine implements exponential backoff for `ECONNABORTED` errors, allowing it to handle &quot;blips&quot; when targeting local dev servers.\n4.  *Hot-Reloading*: A background poller reads the input JSON every 5s and dynamically updates middleware, rate limits, auth keys, and webhook counts without restarting the process.\n5.  *Platform Integration*: We encountered (and fixed in v2.8.7) a specific schema validation bug that only occurred in the platform UI, teaching us to lean on native storage exports rather than wrapping them in custom views.<p>It&#x27;s open source (ISC). I&#x27;d love feedback on the *SSRF implementation* \u2013 ensuring users can replay to `localhost` (safe in dev) but not internal subnets (unsafe in prod) was a tricky balance.", "author": "ar27111994", "timestamp": "2026-01-15T13:47:53+00:00", "score": 1, "num_comments": 0, "products": ["grok"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-15T17:24:38.011785+00:00", "processed": false}
{"id": "hn_story_46630869", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46630869", "title": "Show HN: I built a game on my old phone without knowing what I was building", "text": "I&#x27;m calling this &quot;Vibe Discovery&quot; \u2014 distinct from vibe coding because I didn&#x27;t know the requirements upfront. Started with &quot;make something with the accelerometer&quot; and discovered through 6 iterations that I wanted a WebGL marble game.\nThe interesting part was the dev setup: Claude Code running in Termux on a Redmi Note 9 (4GB RAM). The same-device feedback loop \u2014 code, test accelerometer, react, iterate \u2014 made rapid discovery possible in a way that laptop-to-phone deployment wouldn&#x27;t.", "author": "kikkupico", "timestamp": "2026-01-15T11:08:54+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-15T17:24:53.681546+00:00", "processed": false}
{"id": "hn_story_46630810", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46630810", "title": "Show HN: Bazinga \u2013 Enforced engineering practices for AI coding", "text": "Hi HN,\nI&#x27;m sharing BAZINGA, a framework that applies professional software engineering practices to AI development.\nThe observation: AI coding tools generate code without the safeguards we require from human developers. No mandatory code review. No security scanning. No test coverage requirements.\nBAZINGA addresses this by coordinating multiple AI agents that follow a professional workflow:\n## The Workflow\n1. PM analyzes requirements\n2. Developer implements + writes tests\n3. Security scan runs (mandatory)\n4. Lint check runs (mandatory)\n5. Tech Lead reviews code (independent reviewer)\n6. Only approved code completes\n## Key Principles\n*Separation of concerns:* Writers don&#x27;t review their own code. Developer agent writes, Tech Lead agent reviews. Same principle as human teams.\n*Mandatory quality gates:* Security scanning, lint checking, and coverage analysis run on every change. Not optional.\n*Structured problem-solving:* Complex issues get formal analysis:\n- Root Cause Analysis (5 Whys)\n- Architectural Decision Records\n- Security Issue Triage\n- Performance Investigation\n*Audit trail:* Every decision logged with reasoning. Full traceability for compliance.\n## What It Catches\n- SQL injection, XSS, auth vulnerabilities (via bandit, npm audit, gosec, etc.)\n- Code style violations (via ruff, eslint, golangci-lint, etc.)\n- Missing test coverage (via pytest-cov, jest, etc.)\n- Architectural concerns (via Tech Lead review)\n## Quick Start\nuvx --from git+<a href=\"https:&#x2F;&#x2F;github.com&#x2F;mehdic&#x2F;bazinga.git\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mehdic&#x2F;bazinga.git</a> bazinga init my-project\nMIT licensed. Works with Claude Code.\n## Technical Approach\nBuilt on research from Google&#x27;s ADK and Anthropic&#x27;s context engineering principles. Uses role-based separation with 6-layer drift prevention to ensure agents stay in their designated roles.\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mehdic&#x2F;bazinga\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mehdic&#x2F;bazinga</a>\nHappy to discuss the engineering approach or answer questions about multi-agent coordination.\n```", "author": "mehditch", "timestamp": "2026-01-15T11:00:51+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-15T17:24:54.087830+00:00", "processed": false}
