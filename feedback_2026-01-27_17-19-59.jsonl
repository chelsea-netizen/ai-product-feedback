{"id": "hn_story_46782835", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46782835", "title": "Show HN: Dexicon \u2013 Capture AI coding sessions so your team never loses context", "text": "We built Dexicon because there&#x27;s invaluable context in AI coding sessions that disappears the moment you close the tab. Architectural decisions, debugging rabbit holes, the &quot;why we did it this way&quot; - gone.<p>Dexicon captures sessions from Claude Code, Cursor, Codex, and others, then makes it all searchable via MCP. You can also upload sessions manually along with relevant docs. It extracts atomic pieces of context into a knowledge graph - for V1, that means completed tasks and debugging&#x2F;root-cause analyses, the non-trivial stuff that helps when someone hits the same issue a few weeks later.<p>It&#x27;s designed to be useful for solo devs who want searchable insights into their own sessions, but scales to teams as a way to solve the tribal knowledge problem.<p>We&#x27;re pre-seed with a handful of paying customers. The developers we&#x27;ve been working with have surprised us with use cases we didn&#x27;t anticipate: encoding team best practices, speeding up onboarding for new teammates, and generating optimized agent instructions from their own session history.<p>Now we&#x27;re opening up access to more users and would love feedback from HN community.", "author": "kpam", "timestamp": "2026-01-27T17:05:17+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-27T17:20:01.405677+00:00", "processed": false}
{"id": "hn_story_46782579", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46782579", "title": "Show HN: Kalibr \u2013 Autonomous Routing for AI Agents", "text": "Hey HN, we\u2019re Devon and Alex from Kalibr (<a href=\"https:&#x2F;&#x2F;kalibr.systems\" rel=\"nofollow\">https:&#x2F;&#x2F;kalibr.systems</a>).<p>Kalibr is an autonomous routing system for AI agents. It replaces human debugging with an outcome-driven learning loop. On every agent run, it decides which execution path to use based on what is actually working in production.<p>An execution path is a full strategy, not just a model: model + tools + parameters.<p>Most agents hardcode one path. When that path degrades or fails, a human has to notice, debug, change configs, and redeploy. Even then, the fix often doesn\u2019t stick because models and tools keep changing.<p>I got tired of being the reliability layer for my own agents. Kalibr replaces that.<p>With Kalibr, you register multiple paths for a task. You define what success means. After each run, your code reports the outcome. Kalibr captures telemetry on every run, learns from outcomes, and routes traffic to the path that\u2019s working best while continuously canarying your alternative paths. When one path degrades or fails, traffic shifts immediately. No alerts, no dashboards and no incident response.<p>How is this different from other routers or observability tools?<p>Most routers choose between models using static rules or offline benchmarks. Observability tools show traces and metrics but still require humans to act. Kalibr is outcome-aware and autonomous. It learns directly from production success and changes runtime behavior automatically. It answers not \u201cwhat happened?\u201d but \u201cwhat should my agent do next?\u201d<p>We\u2019re not a proxy. Calls go directly to OpenAI, Anthropic, or Google. We\u2019re not a retry loop. Failed paths are routed away from, not retried blindly. Success rate always dominates; cost and latency only matter when success rates are close.<p>Python and TypeScript SDKs. Works with LangChain, CrewAI, and the OpenAI Agents SDK. Decision latency is ~50ms. If Kalibr is unavailable, the Router falls back to your first path.<p>Think of it as if&#x2F;else logic for agents that rewrites itself based on real production outcomes.<p>We\u2019ve been running this with design partners and would love feedback. Always curious how others are handling agent reliability in production.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;kalibr-ai&#x2F;kalibr-sdk-python\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;kalibr-ai&#x2F;kalibr-sdk-python</a><p>Docs &amp; benchmarks: <a href=\"https:&#x2F;&#x2F;kalibr.systems&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;kalibr.systems&#x2F;docs</a>", "author": "devonkelley", "timestamp": "2026-01-27T16:51:09+00:00", "score": 2, "num_comments": 5, "products": ["claude", "chatgpt"], "categories": ["naming_terminology", "error_messages", "response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:03.020540+00:00", "processed": false}
{"id": "hn_comment_46782392", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46782392", "title": "Re: CDC File Xfer Google Stadia's rsync replacement no...", "text": "I&#x27;ve been working on extending CDC File Transfer to support Linux\u2192Linux and macOS builds. This is an open-source project born from Google Stadia \u2013 fast file sync tools (cdc_rsync) and a streaming filesystem (cdc_stream) that use Content Defined Chunking (FastCDC) to transfer only the changed portions of files.<p>Originally Windows\u2192Linux only, designed for game developers to iterate on 40+ GB builds. The CDC-based diffing is up to 30x faster than rsync (1500 MB&#x2F;s vs 50 MB&#x2F;s).<p>What I added (40 commits):<p>Linux Client Support:<p>Ported cdc_rsync client to run on Linux (was Windows-only)\nAdded process_linux.cc (~700 lines): Process management using fork&#x2F;execve\nAdded port_manager_linux.cc: Port detection using POSIX shared memory\nNow you can sync Linux\u2192Linux, not just Windows\u2192Linux\nmacOS Support:<p>Full build support for macOS (ARM64 and x86_64)\nFixed 17+ platform-specific issues:\n&#x2F;tmp, &#x2F;var, &#x2F;etc are symlinks to &#x2F;private&#x2F;* \u2013 breaks path comparisons\nPOSIX shared memory rounds ftruncate() up to 16KB, rejects shrinking\nBSD fts_* API reports Bazel runfiles symlinks as FTS_SL not FTS_F\nstrerror(0) returns different strings per platform\nstat64 doesn&#x27;t exist on macOS (just use stat)\nAll 41 tests now pass on macOS\nInfrastructure:<p>GitHub Actions CI for Linux and macOS (self-hosted runner for macOS)\nAdded comprehensive CLAUDE.md for AI-assisted development\nThe project uses Bazel with gRPC, protobuf, and FUSE. Dependencies via git submodules or http_archive.<p>GitHub (original): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;cdc-file-transfer\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;cdc-file-transfer</a>\nMy fork: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Vadiml1024&#x2F;cdc-file-transfer\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Vadiml1024&#x2F;cdc-file-transfer</a><p>I&#x27;ve been using Claude Code (Anthropic&#x27;s AI coding assistant) for most of this work. Happy to discuss the experience or the technical challenges.", "author": "vadiml", "timestamp": "2026-01-27T16:41:15+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-27T17:20:04.134568+00:00", "processed": false}
{"id": "hn_comment_46782212", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46782212", "title": "Re: I Stopped Reading Code. My Code Reviews Got Better...", "text": "&gt; A user noticed that their email signature formatting was off in Cora, our AI-powered email assistant. I asked Claude Code to investigate and fix it. By morning, the fix had touched 27 files, and more than 1,000 lines of code had changed. I didn\u2019t write any of them.<p>Email signature formatting, 27 files, more than 1000 lines of code changes? I would not read that code either, that&#x27;s automatically rejected. I can&#x27;t possibly imagine a problem that would result in a subtle formatting bug that would require that level of change.<p>&gt; migration that moved email_signature from one database table to another<p>Migration? For a formatting bug? Are you sure?<p>Stop self-snitching people, this kind of behavior is beyond unprofessional, its negligent. Anyway have fun with your unreadable spaghetti code base.", "author": "throwawayffffas", "timestamp": "2026-01-27T16:30:46+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:06.659216+00:00", "processed": false}
{"id": "hn_story_46781784", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46781784", "title": "Show HN: Claude Threads \u2013 Collaborate on Claude Code via Slack (Or Mattermost)", "text": "I wanted my team to start using Claude Code but didn&#x27;t want to set everyone up before they were convinced. Started piping output to Mattermost (and later Slack) so people could watch and learn how to work with Claude Code.\nEnded up building more: multiple sessions in parallel (each in a thread, hence the name), approve messages from other users with emojis, approve file writes, attach images&#x2F;files, worktrees per thread.<p>It runs on your machine.<p>I built most of it using itself. Teammates watching live caught stuff I missed.<p><a href=\"https:&#x2F;&#x2F;claude-threads.run\" rel=\"nofollow\">https:&#x2F;&#x2F;claude-threads.run</a> \n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;anneschuth&#x2F;claude-threads\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anneschuth&#x2F;claude-threads</a>", "author": "aschuth", "timestamp": "2026-01-27T16:04:12+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:07.100594+00:00", "processed": false}
{"id": "hn_story_46781731", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46781731", "title": "Show HN: Lumina \u2013 Open-source observability for AI systems(OpenTelemetry-native)", "text": "Hey HN! I built Lumina \u2013 an open-source observability platform for AI&#x2F;LLM applications. Self-host it in 5 minutes with Docker Compose, all features included.<p>The Problem:<p>I&#x27;ve been building LLM apps for the past year, and I kept running into the same issues:\n- LLM responses would randomly change after prompt tweaks, breaking things.\n- Costs would spike unexpectedly (turns out a bug was hitting GPT-4 instead of 3.5).\n- No easy way to compare &quot;before vs after&quot; when testing prompt changes.\n- Existing tools were either too expensive or missing features in free tiers.<p>What I Built:<p>Lumina is OpenTelemetry-native, meaning:\n- Works with your existing OTEL stack (Datadog, Grafana, etc.).\n- No vendor lock-in, standard trace format.\n- Integrates in 3 lines of code.<p>Key features:\n - Cost &amp; quality monitoring \u2013 Automatic alerts when costs spike, or responses degrade.\n - Replay testing \u2013 Capture production traces, replay them after changes, see diffs.\n - Semantic comparison \u2013 Not just string matching \u2013 uses Claude to judge if responses are &quot;better&quot; or &quot;worse.&quot;\n - Self-hosted tier \u2013 50k traces&#x2F;day, 7-day retention, ALL features included (alerts, replay, semantic scoring)<p>How it works:<p>```bash\n# Start Lumina\ngit clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\ncd Lumina&#x2F;infra&#x2F;docker\ndocker-compose up -d\n```<p>```typescript\n&#x2F;&#x2F; Add to your app (no API key needed for self-hosted!)\nimport { Lumina } from &#x27;@uselumina&#x2F;sdk&#x27;;<p>const lumina = new Lumina({\n  endpoint: &#x27;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;traces&#x27;,\n});<p>&#x2F;&#x2F; Wrap your LLM call\nconst response = await lumina.traceLLM(\n  async () =&gt; await openai.chat.completions.create({...}),\n  { provider: &#x27;openai&#x27;, model: &#x27;gpt-4&#x27;, prompt: &#x27;...&#x27; }\n);\n```<p>That&#x27;s it. Every LLM call is now tracked with cost, latency, tokens, and quality scores.<p>What makes it different:<p>1. Free self-hosted with limits that work \u2013 50k traces&#x2F;day and 7-day retention (resets daily at midnight UTC). All features included: alerts, replay testing, and semantic scoring. Perfect for most development and small production workloads. Need more? Upgrade to managed cloud.<p>2. OpenTelemetry-native \u2013 Not another proprietary format. Use standard OTEL exporters, works with existing infra. Can send traces to both Lumina AND Datadog simultaneously.<p>3. Replay testing \u2013 The killer feature. Capture 100 production traces, change your prompt, replay them all, and get a semantic diff report. Like snapshot testing for LLMs.<p>4. Fast \u2013 Built with Bun, Postgres, Redis, NATS. Sub-500ms from trace to alert. Handles 10k+ traces&#x2F;min on a single machine.<p>What I&#x27;m looking for:<p>- Feedback on the approach (is OTEL the right foundation?)\n- Bug reports (tested on Mac&#x2F;Linux&#x2F;WSL2, but I&#x27;m sure there are issues)\n- Ideas for what features matter most (alerts? replay? cost tracking?)\n- Help with the semantic scorer (currently uses Claude, want to make it pluggable)<p>Why open source:<p>I want this to be the standard for LLM observability. That only works if it&#x27;s:\n- Free to use and modify (Apache 2.0)\n- Easy to self-host (Docker Compose, no cloud dependencies)\n- Open to contributions (good first issues tagged)<p>The business model is managed hosting for teams that don&#x27;t want to run infrastructure. But the core product is and always will be free.<p>Try it:\n- GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;use-lumina&#x2F;Lumina</a>\n- Docs: <a href=\"https:&#x2F;&#x2F;docs.uselumina.io\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.uselumina.io</a>\n- Quick start: 5 minutes from `git clone` to dashboard<p>I&#x27;d love to hear what you think! Especially interested in:\n- What observability problems are you hitting with LLMs\n- Missing features that would make this useful for you\n- Any similar tools you&#x27;re using (and what they do better)<p>Thanks for reading!", "author": "Evanson", "timestamp": "2026-01-27T16:00:06+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:07.337782+00:00", "processed": false}
{"id": "hn_story_46781155", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46781155", "title": "Show HN: Kimi K2.5 (Agent Swarm, beats GPT-5) now on RouterLab (Swiss hosting)", "text": "Hi HN!<p>Moonshot AI released Kimi K2.5 today, and we integrated it on RouterLab within hours.<p>Why this matters:<p>*Open source beats proprietary:*\n\u2022 Kimi K2.5: 50.2% on HLE (Humanity&#x27;s Last Exam)\n\u2022 GPT-5: 41.7%\n\u2022 Claude 4.5: 32.0%<p>First time an open-source model beats GPT-5 on expert-level reasoning.<p>*Agent Swarm architecture:*\n\u2022 Orchestrates up to 100 parallel agents\n\u2022 1,500 simultaneous tool calls\n\u2022 4.5x faster than sequential execution\n\u2022 Autonomous task decomposition<p>Example: &quot;Analyze 50 competitors and create a report&quot;\n\u2192 Creates 50 research agents\n\u2192 Parallel execution\n\u2192 Compiled report in minutes<p>*Technical specs:*\n\u2022 1T parameters (32B activated)\n\u2022 384 experts MoE\n\u2022 INT4 quantization native\n\u2022 256k context window\n\u2022 Open weights on Hugging Face<p>*Benchmarks:*\n\u2022 HLE (reasoning): 50.2% (GPT-5: 41.7%)\n\u2022 BrowseComp (web nav): 60.2% (GPT-5: 54.9%)\n\u2022 SWE-Bench (coding): 71.3%\n\u2022 VideoMMMU: 86.6%<p>*Pricing:*\n\u2022 $0.60&#x2F;$3.00 per 1M tokens\n\u2022 5x cheaper than GPT-5<p>We&#x27;re a Swiss company (Eyelo SA, founded 1983) and integrated Kimi K2.5 on RouterLab with Swiss&#x2F;German hosting for GDPR compliance.<p>OpenAI-compatible API, migration is 2 lines of code:<p>```python\nclient = OpenAI(\n    base_url=&quot;<a href=\"https:&#x2F;&#x2F;routerlab.ch&#x2F;v1\" rel=\"nofollow\">https:&#x2F;&#x2F;routerlab.ch&#x2F;v1</a>&quot;,\n    api_key=&quot;your-key&quot;\n)\nI wrote a technical analysis:\n<a href=\"https:&#x2F;&#x2F;medium.com&#x2F;@comeback01&#x2F;kimi-k2-5-the-agent-swarm-revolution-that-just-beat-gpt-5-7b06360f9735\" rel=\"nofollow\">https:&#x2F;&#x2F;medium.com&#x2F;@comeback01&#x2F;kimi-k2-5-the-agent-swarm-rev...</a><p>Try it: <a href=\"https:&#x2F;&#x2F;routerlab.ch&#x2F;blog&#x2F;kimi-k2-5\" rel=\"nofollow\">https:&#x2F;&#x2F;routerlab.ch&#x2F;blog&#x2F;kimi-k2-5</a> (14-day free trial)<p>Happy to answer technical questions about Agent Swarm, MoE architecture, or deployment!", "author": "ScioNos", "timestamp": "2026-01-27T15:21:01+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:11.876453+00:00", "processed": false}
{"id": "hn_story_46780918", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46780918", "title": "Show HN: PenPeeper\u2013An Open-Source Pentesting Engagement Manager (Optional AI)", "text": "PenPeeper \u2013 An Open-Source Pentesting Engagement Manager (with Optional AI)<p>Most pentesting tools I\u2019ve used fall into one of two buckets:<p>absurdly expensive enterprise SaaS<p>open-source tools that don\u2019t help once scanning is done<p>PenPeeper is my attempt to fix that.<p>What it is<p>A free, open-source, self-hosted pentesting engagement manager that focuses on the boring but critical parts:<p>scoping &amp; engagement tracking<p>vulnerability management<p>reporting<p>tying everything together in one workflow<p>The AI part (optional, not magic)<p>PenPeeper can integrate with local or external LLMs (Ollama, LM Studio, ChatGPT, Claude, Gemini, OpenRouter).<p>Runs on Windows (via WSL integration), MacOS, Linux<p>The goal isn\u2019t \u201cAI replaces pentesters.\u201d\nIt\u2019s:<p>faster vuln analysis<p>better first-draft reports<p>less copy-pasting between tools<p>You can run it fully local. You can turn AI off entirely.<p>Why I built it<p>Commercial tools are overpriced and locked down.\nMost open-source tools stop at scanning.\nReporting is still manual, repetitive, and error-prone.<p>That gap is what PenPeeper is trying to cover.<p>Status<p>Early but stable<p>Actively developed<p>Looking for real pentester feedback (not hype)<p>Links<p>Site: <a href=\"https:&#x2F;&#x2F;penpeeper.com\" rel=\"nofollow\">https:&#x2F;&#x2F;penpeeper.com</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;chetstriker&#x2F;PenPeeper\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;chetstriker&#x2F;PenPeeper</a><p>Feedback I want<p>What part of your pentest workflow is still the most painful?<p>Where does AI actually help vs get in the way?<p>What would make this worth using on a real engagement?<p>Happy to answer technical questions or take criticism.", "author": "chetstriker", "timestamp": "2026-01-27T15:04:02+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:13.775432+00:00", "processed": false}
{"id": "hn_comment_46781139", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46781139", "title": "Re: Show HN: Local-first AI workspaces with tiling tab...", "text": "Ah and you can download it from here: <a href=\"https:&#x2F;&#x2F;www.silain.com&#x2F;download\" rel=\"nofollow\">https:&#x2F;&#x2F;www.silain.com&#x2F;download</a> (Mac, Linux, Windows)<p>No need to host a server or anything but need keys from any of the AI providers, e.g OpenAI, Anthropic or Openrouter.", "author": "etoonoptima", "timestamp": "2026-01-27T15:20:27+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-27T17:20:14.014614+00:00", "processed": false}
{"id": "hn_story_46780862", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46780862", "title": "Show HN: Magpie \u2013 I built a CLI where AIs argue about my code", "text": "Hi HN,<p>I built Magpie because I was tired of AI code reviewers being too &quot;nice.&quot;<p>Most AI tools just say &quot;LGTM&quot; or nitpick formatting. To fix this, Magpie uses an adversarial approach: it spawns two different AI agents (e.g., a Security Expert and a Performance Critic) and forces them to debate your changes.<p>They don&#x27;t just list bugs; they attack each other&#x27;s arguments until they reach a consensus. This cuts down on hallucinations and lazy approvals.<p>Features:<p>Adversarial Debate: Watch Claude and GPT-4o fight over your code.<p>Local &amp; CI: Works on local files or GitHub PRs.<p>Model Agnostic: Supports OpenAI, Anthropic, and Gemini.<p>The Experiment: This is also an experiment in &quot;coding without coding.&quot; I didn&#x27;t write a single line of TypeScript for this project manually. The entire repo was built using Claude Code.<p>I&#x27;d love to hear your feedback\u2014especially if you manage to make the models get into an infinite argument.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;liliu-z&#x2F;magpie\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;liliu-z&#x2F;magpie</a>", "author": "leo_e", "timestamp": "2026-01-27T14:59:57+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-27T17:20:14.348428+00:00", "processed": false}
{"id": "hn_story_46780767", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46780767", "title": "Show HN: I query multiple LLMs in parallel because I don't trust any single one", "text": "I have a mass of AI subscriptions. ChatGPT, Claude, Perplexity, Gemini. My workflow became: ask Claude, then paste the same question into ChatGPT to sanity-check, then maybe ask Perplexity if I need sources. Five tabs, constant copy-pasting.<p>Council just runs your prompt against multiple models at once and shows responses side-by-side. That&#x27;s it.<p>A few things I noticed while building this:<p>1. Models disagree with each other way more than I expected. Ask anything slightly subjective or recent, and you&#x27;ll get meaningfully different answers. It&#x27;s made me much more skeptical of treating any single response as &quot;the answer.&quot;<p>2. Different models have different failure modes. Claude tends to be cautious and hedge. GPT is confident even when wrong. Perplexity gives sources but sometimes misreads them. Seeing them together makes these patterns obvious.<p>3. For code, I actually like getting 2-3 different approaches. Even if one is clearly better, seeing alternatives helps me understand the tradeoffs.<p>Tech: Next.js, OpenRouter for model access, streaming responses in parallel. The annoying part was handling the UI when models respond at different speeds \u2013 you don&#x27;t want the layout jumping around.<p>No login required to try it. Feedback welcome, especially on what&#x27;s broken or annoying.<p><a href=\"https:&#x2F;&#x2F;usecouncil.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;usecouncil.app&#x2F;</a>", "author": "jonnyhere", "timestamp": "2026-01-27T14:53:53+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini", "perplexity"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:14.735072+00:00", "processed": false}
{"id": "hn_story_46780766", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46780766", "title": "Claude Code skill for building ChatGPT Apps", "text": "The ChatGPT Apps SDK has a steep learning curve, specially OAuth, where you&#x27;re the provider and ChatGPT is the client (not the other way around). This can trip you up easily.<p>This skill teaches Claude Code how to build ChatGPT apps correctly:<p><pre><code>  - MCP server setup (Node.js&#x2F;Python)\n  - OAuth with PKCE and Dynamic Client Registration\n  - Widget development with window.openai API\n  - 20+ gotchas with fixes\n</code></pre>\nHow to install it:<p>npx skills add https:&#x2F;&#x2F;github.com&#x2F;vdel26&#x2F;skills``<p>GitHub:<p>https:&#x2F;&#x2F;github.com&#x2F;vdel26&#x2F;skills<p>Would love feedback on missing gotchas people have hit while building ChatGPT &#x2F; MCP Apps.", "author": "victordg", "timestamp": "2026-01-27T14:53:49+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-27T17:20:14.768893+00:00", "processed": false}
{"id": "hn_comment_46780642", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46780642", "title": "Re: Show HN\uff1aAgentHub - Unified, Stateful SDK for All S...", "text": "Hi HN,\nWe &#x27; ve been frustrated with the fragmentation in the LLM ecosystem. Switching between OpenAI, Anthropic, and Google often means rewriting state management logic or losing model-specific reasoning features.\nSo we built AgentHub to solve this. It\u2019s a small, open-source SDK that provides a unified Python&#x2F;TypeScript interface for all SOTA models.\nWhy I built this: Existing tools like LangChain felt too heavy, and routers like OpenRouter are closed-source and lack deep execution auditing. It includes a lightweight board for auditing LLM executions. You can permanently trace every run by passing just one parameter.\nKey Focus: We simplify the engineering process with an asynchronous, stateful, and streaming API specifically designed for multi-turn agentic executions, which significantly flattens the learning curve with zero code changes. And we ensure that model-specific capabilities, such as interleaved thinking and caching, are rigorously validated and aligned across providers with no loss of performance. \nI\u2019d love to hear your thoughts.", "author": "PrismShadow", "timestamp": "2026-01-27T14:44:38+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2026-01-27T17:20:15.421591+00:00", "processed": false}
{"id": "hn_comment_46779914", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46779914", "title": "Re: Show HN: IOPS Profiler \u2013 Jupyter magic to measure ...", "text": "Author here. Built this while working on astronomy data pipelines where we process terabyte-scale datasets.\nWe kept hitting a frustrating pattern: libraries promised great performance, benchmarks looked solid, but our pipelines were mysteriously slow. CPU and memory were fine, yet tasks taking minutes in theory took hours in practice.<p>The culprit was consistently I&#x2F;O. Either we were making millions of tiny operations, or the &quot;optimized&quot; storage layer wasn&#x27;t doing what we expected. But there was no easy way to see actual I&#x2F;O behavior without leaving Jupyter and diving into system tools.<p>So we built this. Now %%iops at the top of a cell immediately shows: &quot;Oh, 50,000 separate writes instead of buffering. That&#x27;s why.&quot;<p>It&#x27;s been invaluable for debugging performance gaps between expectations and real-world behavior in our workloads.<p>Interesting sidenote: I&#x27;m a pretty extreme AI skeptic, but wanted to see how far current tools could be taken. With minor edits, all the code, documentation, and even this HN submission were generated by Claude&#x2F;Copilot. The results surprised me.\nHappy to answer questions or hear if others have hit similar performance mysteries.", "author": "mtauraso", "timestamp": "2026-01-27T13:53:43+00:00", "score": null, "num_comments": null, "products": ["claude", "copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:21.356857+00:00", "processed": false}
{"id": "hn_story_46779716", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46779716", "title": "Show HN: I built a voice-only AI language tutor using OpenAI's Realtime API", "text": "I&#x27;m an expat in Bangkok who can read Thai menus but freezes the moment someone speaks to me. Duolingo didn&#x27;t help, I needed actual speaking practice, but tutors are expensive and scheduling is a pain.<p>So I built speaklanguageonline.com - a voice call with an AI that speaks Thai (or Vietnamese), listens to your attempts, and gives you one gentle correction at a time. No typing, no flashcards, just talking.<p>Tech:<p>OpenAI Realtime API (WebRTC) for speech-to-speech\nNext.js 14 + Vercel\nNo transcription step - the model processes audio directly, which preserves tone (critical for Thai&#x27;s 5 tones)<p>What makes it different from ChatGPT voice:<p>- Tuned for slow, patient corrections (not conversational chat)\n- One correction per turn (anxiety-inducing to get 5 things wrong at once)\n- Corrections explained in your native language\n- 3-minute session cap to keep it focused<p>Pricing: Credits, not subscriptions. You pay only for minutes used. I hate subscription guilt as much as you do.<p>Current state: Thai and Vietnamese work well. Adding Spanish, Hindi, Mandarin soon \u2014 OpenAI&#x27;s model handles them but quality varies.<p>What I learned:<p>Realtime API latency is ~300-500ms which feels natural for conversation\nPrompting for &quot;one correction only&quot; took way more iteration than expected\nTonal languages need explicit instruction to focus on tone mistakes<p>Would love feedback, especially from anyone who&#x27;s built voice-first apps or is learning a language.", "author": "digi_wares", "timestamp": "2026-01-27T13:34:23+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["tone", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:23.357267+00:00", "processed": false}
{"id": "hn_story_46779509", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46779509", "title": "Giving Claude Code a feedback loop into pdb, GDB and TUIs with tmux", "text": "", "author": "rasca", "timestamp": "2026-01-27T13:12:13+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-27T17:20:25.243053+00:00", "processed": false}
{"id": "hn_story_46779287", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46779287", "title": "Show HN: Oauth2-Proxy-Injector", "text": "A few weeks ago I could barely code in python, and I could not code a single line of go. After seeing claude handle a work project impressively, I decided to try using it to teach me to code. I asked it to scaffold projects, write todos above the functions, and tell me a reasonable order to work in. This doesn&#x27;t teach software design, but it really helps get over the hump of learning the ecosystem and standard libraries.<p>Now, I still can&#x27;t program unassisted, but I feel confident enough to read others&#x27; code and maybe even submit a small PR to fix a bug.<p>This is my first attempt at a generally useful project. It&#x27;s a mutating admission webhook. This is still a WIP, but it&#x27;s working and I&#x27;m using it on my own k3s cluster. It adds oauth2-proxy to pods that need authentication. On the cluster, I replaced Authentik with Zitadel, and I needed something to fill in the role of Authentik&#x27;s proxy provider.<p>Since so many people are using and becoming frustrated with AI now, I hope this can be some inspiration to use AI as a tool to learn something new instead of as an assistant. Hopefully the more experienced programmers here can let me know what about this looks like slop (I assume it does). I&#x27;m afraid if I keep this up, I&#x27;ll learn to program too much like claude.", "author": "spacemule", "timestamp": "2026-01-27T12:48:53+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-27T17:20:26.205118+00:00", "processed": false}
{"id": "hn_story_46779001", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46779001", "title": "Show HN: Beyond Open Source: Why AI-Assisted Projects Need 'Open Method'", "text": "Disclosure: Ferrite is built using AI-assisted development (Claude). I&#x27;m sharing this openly because I think transparency is exactly what this post is about.<p>Someone on Hacker News called my project &quot;open weights&quot;, arguing that without sharing the prompts and process that created the code, I was essentially doing the AI equivalent of releasing model weights without the training data. The code was visible, but the inputs weren&#x27;t.<p>That comment led me down a rabbit hole about what &quot;open source&quot; actually means in an AI-assisted world.\nThe problem: Open source was designed assuming humans wrote code. \nIf you could read the code, you could understand how it was made. AI breaks that assumption. When Claude writes a function based on my prompt, the code tells you what it does, but not why it exists in that form.<p>My proposal: &quot;Open Method&quot;, sharing not just the code, but the process. The prompts, the workflow, the PRDs, the decisions. Enough that someone else could understand not just what you built, but how you built it.<p>I wrote about this in more depth here: <a href=\"https:&#x2F;&#x2F;dev.to&#x2F;olaproeis&#x2F;beyond-open-source-why-ai-assisted-projects-need-open-method-fc9\" rel=\"nofollow\">https:&#x2F;&#x2F;dev.to&#x2F;olaproeis&#x2F;beyond-open-source-why-ai-assisted-...</a><p>Some context on Ferrite:<p>900+ GitHub stars<p>Approved for Flatpak (Flathub) release<p>Just got code signing approved<p>All development methodology is documented in docs&#x2F;ai-workflow&#x2F;<p>I&#x27;m not saying everyone must share their prompts. But I think the open source community should discuss what transparency looks like when AI is writing our code.<p>Questions for discussion:<p>If you were reviewing an AI-assisted PR, what would you want to see?<p>Should repos have an AI_METHOD.md alongside README.md?<p>Does &quot;open source&quot; need to evolve for the AI era?", "author": "OlaProis", "timestamp": "2026-01-27T12:16:33+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-27T17:20:28.565785+00:00", "processed": false}
{"id": "hn_story_46778689", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46778689", "title": "Show HN: LLM-schema-guard \u2013 Rust proxy enforcing JSON schemas on LLM outputs", "text": "Hey everyone,\nI built llm-schema-guard because LLMs are amazing at spitting out JSON... until they suddenly aren&#x27;t. Even with JSON mode or function calling, you still get missing fields, wrong types, or just plain broken syntax that kills your agents, RAG flows, or any tool-calling setup.\nThis is a lightweight Rust HTTP proxy that sits in front of any OpenAI-compatible API (think Ollama, vLLM, LocalAI, OpenAI itself, Groq, you name it). It grabs the generated output, checks it against a JSON Schema you provide, and only lets it through if it&#x27;s valid.\nIf it&#x27;s invalid, strict mode kicks back a clean 400 with details. Permissive mode tries auto-retrying a few times by tweaking the prompt with a fix instruction and exponential backoff.\nEverything else stays the same: full streaming support (it buffers the response to validate), Prometheus metrics so you can monitor validation fails, retries, latency, and more. Config is simple YAML for upstreams, schemas per model, rate limiting, caching, etc. There&#x27;s even an offline CLI if you just want to test schemas locally.\nIt&#x27;s built with Axum and Tokio for really low latency and high throughput, plus jsonschema-rs under the hood. Docker compose makes it dead simple to spin up with Ollama.<p>This grew out of my earlier schema-gateway project, and I&#x27;m happy to add stuff like Anthropic support, tool calling validation, or better streaming fixes if people find it useful.\nStars or contributions are very welcome!<p>Thanks for taking a look :)", "author": "iCeGaming", "timestamp": "2026-01-27T11:39:24+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:30.223168+00:00", "processed": false}
{"id": "hn_comment_46778445", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46778445", "title": "Re: 'Ralph Wiggum' loop prompts Claude to vibe-clone c...", "text": "i don&#x27;t see how that while statement feeds the claude response back into itself. its just catting the PROMPT.d to claude over and over.", "author": "cranberryturkey", "timestamp": "2026-01-27T11:10:48+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-27T17:20:32.153558+00:00", "processed": false}
