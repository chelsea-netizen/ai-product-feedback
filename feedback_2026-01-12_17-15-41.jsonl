{"id": "hn_story_46591238", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46591238", "title": "Show HN: Spec Driven Development Plugin for Claude Code", "text": "&gt; *TL;DR:* On larger features, Claude\u2019s plans tend to get vague, which leads to vibe-coded spaghetti in the implementation. This plugin forces PRD + design + task-level acceptance criteria to keep things grounded.<p>Hi HN,<p>I use Claude Code a lot and it&#x27;s native plan mode works well for small to medium changes, but on larger features the plans would be vague, which lead to more hallucinations during implementation.<p>I ended up coming up with a manual workflow for larger features where I&#x27;d write a design first by bouncing ideas back and forth with ChatGPT&#x2F;Claude&#x2F;Gemini until I had something broken down into clear phases. I\u2019d drop those docs into the repo and then have Claude Code implement each phase individually while referring back to them.<p>So I made ShipSpec, a Claude Code Plugin that automates this workflow inside Claude Code. It generates three repo-local markdown files:<p>* PRD.md \u2013 requirements\n* SDD.md \u2013 design doc\n* TASKS.md \u2013 ordered tasks with acceptance criteria<p>Because tasks link back to both the requirements and the design, Claude Code stays grounded during implementation.<p>Usage:<p>```\n&#x2F;shipspec:feature-planning &quot;add user authentication with OAuth2&quot;\n&#x2F;shipspec:implement-feature auth-feature\n&#x2F;shipspec:implement-task auth-feature TASK-001\n```<p>Not sure if this is generally useful or just scratches my own itch, but curious if others have hit similar issues with having Claude Code implement larger features.", "author": "segov", "timestamp": "2026-01-12T17:05:21+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-12T17:15:43.256705+00:00", "processed": false}
{"id": "hn_story_46591100", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46591100", "title": "Show HN: AI in SolidWorks", "text": "Hey HN! We\u2019re Will and Jorge, and we\u2019ve built LAD (Language-Aided Design), a SolidWorks add-in that uses LLMs to create sketches, features, assemblies, and macros from conversational inputs (<a href=\"https:&#x2F;&#x2F;www.trylad.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.trylad.com&#x2F;</a>).<p>We come from software engineering backgrounds where tools like Claude Code and Cursor have come to dominate, but when poking around CAD systems a few months back we realized there&#x27;s no way to go from a text prompt input to a modeling output in any of the major CAD systems. In our testing, the LLMs aren&#x27;t as good at making 3D objects as they are are writing code, but we think they&#x27;ll get a lot better in the upcoming months and years.<p>To bridge this gap, we&#x27;ve created LAD, an add-in in SolidWorks to turn conversational input and uploaded documents&#x2F;images into parts, assemblies, and macros. It includes:<p>- Dozens of tools the LLM can call to create sketches, features, and other objects in parts.<p>- Assembly tools the LLM can call to turn parts into assemblies.<p>- File system tools the LLM can use to create, save, search, and read SolidWorks files and documentation.<p>- Macro writing&#x2F;running tools plus a SolidWorks API documentation search so the LLM can use macros.<p>- Automatic screenshots and feature tree parsing to provide the LLM context on the current state.<p>- Checkpointing to roll back unwanted edits and permissioning to determine which commands wait for user permission.<p>You can try LAD at <a href=\"https:&#x2F;&#x2F;www.trylad.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.trylad.com&#x2F;</a> and let us know what features would make it more useful for your work. We\u2019d love your feedback!", "author": "WillNickols", "timestamp": "2026-01-12T16:56:17+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:43.529978+00:00", "processed": false}
{"id": "hn_story_46591026", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46591026", "title": "Cursor vs. antigravity after a week of real use", "text": "in the first week of 2026 i ended up using cursor and google antigravity back to back, not by plan but because i burned through two cursor ultra subscriptions faster than expected and decided to try antigravity on the free tier.<p>my normal usage is ~$60\u2013100&#x2F;month. within a few days it jumped to $500+, with the dashboard projecting ~$1.6k&#x2F;month. max mode was off, and the ui consistently showed a 200k context window.<p>what i eventually pieced together is that cursor maintains a large hidden prompt state. beyond visible conversation history, this includes tool traces, agent state, reasoning scaffolding, and large chunks of repo context. that state is prompt-cached using claude\u2019s cache feature, and on every request the full cached prefix is replayed.<p>anthropic bills cache reads every time this happens, even if that content is later summarized or truncated before actual inference.<p>one concrete example from my logs:\n \u2022 actual user input ~4k tokens\n \u2022 cache read tokens ~21 million\n \u2022 total billed tokens ~22 million\n \u2022 cost for a single call ~$12<p>this wasn\u2019t limited to opus. i saw the same pattern with sonnet.<p>support explained that this matched how the underlying api is billed. from my perspective, the issue wasn\u2019t correctness but visibility. cost had become decoupled from anything i could see or reason about in the product. i had no way to inspect cache size, understand replay behavior, or set meaningful guardrails.<p>i canceled and treated it as an excuse to try google antigravity.<p>the free tier was more usable than i expected. it gives access to opus 4.5, which is still my preferred model for nontrivial coding work. for easy to moderate complexity tasks, it usually finishes cleanly. limits are opaque (free \u2192 pro \u2192 ultra is described in very abstract terms), but when you hit a limit you at least get a clear cooldown message telling you when opus will be available again.<p>when opus is exhausted, antigravity falls back to gemini models. that was useful for comparison. for real coding work on a messy, evolving codebase, gemini flash, pro, and thinking consistently lost architectural decisions and produced one-off changes that didn\u2019t respect existing constraints.<p>some of that is model quality, but not all of it. cursor\u2019s agent does a better job gathering relevant repo state, forming a plan, and executing it coherently. antigravity\u2019s agent feels thinner, and i found myself spending more time reviewing and correcting diffs to preserve invariants.<p>there were also smaller papercuts. tab completion is advertised as unlimited, but i couldn\u2019t get it working reliably. going back to basic autocomplete was a reminder of how dependent my workflow is on good tab completion. ux-wise, antigravity feels slower. maybe not raw latency, but the way responses stream and animate doesn\u2019t keep me in the same loop cursor does.<p>net result: antigravity free is a solid option for starting out and experimenting, especially if budget matters. i wouldn\u2019t pay for it yet. cursor is still a strong product, but opaque caching and billing behavior makes it hard to reason about cost at scale.<p>for context, i\u2019m building an agent runtime myself at inference.sh, focused on explicit state, durable execution, and reliable deep agents with complex tool use. because of that, i\u2019m probably more sensitive than most to differences in agent orchestration, hidden state, and how cost emerges from those design choices.<p>this whole experience reinforced something i already believed: hidden state is dangerous in agent systems. hidden state combined with opaque billing is worse. if users can\u2019t see state, they can\u2019t reason about cost. and if they can\u2019t reason about cost, they won\u2019t trust the system.<p>right now, none of these coding agents are \u201cset and forget\u201d if you\u2019re doing work that hasn\u2019t been done a thousand times before. you still have to stay in charge.", "author": "okaris", "timestamp": "2026-01-12T16:51:24+00:00", "score": 1, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:43.805386+00:00", "processed": false}
{"id": "hn_comment_46590972", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46590972", "title": "Re: Apple picks Google's Gemini to power Siri...", "text": "This is one of those announcements that actually just excites me as a consumer. We give our children HomePods as their first device when they turn 8 years old (Apple Watch at 10 years, laptop at 12) and in the 6 years I have been buying them, they have not improved one ounce. My kids would like to listen to podcasts, get information, etc. All stuff that a voice conversation with Chatgpt or Gemini can do today, but Siri isn&#x27;t just useless-- it&#x27;s actually quite frustrating!", "author": "jmacd", "timestamp": "2026-01-12T16:48:09+00:00", "score": null, "num_comments": null, "products": ["chatgpt", "gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-12T17:15:49.304246+00:00", "processed": false}
{"id": "hn_story_46588905", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46588905", "title": "Show HN: Agent-of-empires: opencode and claudecode session manager", "text": "Hi! I\u2019m Nathan: an ML Engineer at Mozilla.ai: I built agent-of-empires (aoe): a CLI application to help you manage all of your running Claude Code&#x2F;Opencode sessions and know when they are waiting for you.<p>- Written in rust and relies on tmux for security and reliability\n- Monitors state of cli sessions to tell you when an agent is running vs idle vs waiting for your input\n- Manage sessions by naming them, grouping them, configuring profiles for various settings<p>I&#x27;m passionate about getting self-hosted open-weight LLMs to be valid options to compete with proprietary closed models. One roadblock for me is that although tools like opencode allow you to connect to Local LLMs (Ollama, lm studio, etc), they generally run muuuuuch slower than models hosted by Anthropic and OpenAI. I would start a coding agent on a task, but then while I was sitting waiting for that task to complete, I would start opening new terminal windows to start multitasking. Pretty soon, I was spending a lot of time toggling between terminal windows to see which one needed me: like help in adding a clarification, approving a new command, or giving it a new task.<p>That\u2019s why I build agent-of-empires (\u201caoe\u201d). With aoe, I can launch a bunch of opencode and Claude Code sessions and quickly see their status or toggle between them, which helps me avoid having a lot of terminal windows open, or having to manually attach and detach from tmux sessions myself. It\u2019s helping me give local LLMs a fair try, because them being slower is now much less of a bottleneck.<p>You can give it an install with<p>curl -fsSL <a href=\"https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;njbrake&#x2F;agent-of-empires&#x2F;main&#x2F;scripts&#x2F;install.sh\" rel=\"nofollow\">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;njbrake&#x2F;agent-of-empires&#x2F;m...</a>  | bash<p>Or brew install njbrake&#x2F;aoe&#x2F;aoe<p>And then launch by simply entering the command `aoe`.<p>I\u2019m interested in what you think as well as what features you think would be useful to add!<p>I am planning to add some further features around sandboxing (with docker) as well as support for intuitive git worktrees and am curious if there are any opinions about what should or shouldn\u2019t be in it.<p>I decided against MCP management or generic terminal usage, to help keep the tool focused on parts of agentic coding that I haven\u2019t found a usable solution for.<p>I hit the character limit on this post which prevented me from including a view of the output, but the readme on the github link has a screenshot showing what it looks like.<p>Thanks!", "author": "river_otter", "timestamp": "2026-01-12T14:23:07+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:54.221058+00:00", "processed": false}
{"id": "hn_story_46588608", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46588608", "title": "Show HN: I got tired of \"Reliability Spaghetti,\" so I monkeypatched PydanticAI", "text": "Author of the &quot;Confident Idiot&quot; post here (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46152838\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46152838</a>).<p>After that discussion, I looked at my own agent code and realized it was 80% error handling and 20% reasoning. I was manually decorating every function with retries, regex checks, and JSON validators. It was unreadable.<p>I realized that reliability shouldn&#x27;t be Application Code; it should be Infrastructure.<p>I built Steer to test a pattern: Monkeypatching the framework to decouple reliability.<p>Instead of decorating functions, I initialize Steer at the entry point. It hooks into the framework&#x27;s lifecycle (PydanticAI &#x2F; OpenAI), introspects the tools, and attaches &quot;Reality Locks&quot; (SQL parsers, Schema checks, Entropy filters) globally.<p>Before (The Spaghetti):<p><pre><code>  # Business logic mixed with safety logic\n  @retry(stop=stop_after_attempt(3))\n  def run_query(q):\n      if &quot;DROP&quot; in q: raise Error() # Manual check\n      response = agent.run(q)\n      if not is_valid_sql(response): raise Error() # Manual check\n      return response\n</code></pre>\nAfter (The Mesh):<p><pre><code>  import steer\n  # One line patches the framework globally.\n  # Auto-attaches SQL validators to any tool returning SQL.\n  steer.init(patch=[&quot;pydantic_ai&quot;], policy=&quot;strict_sql&quot;)\n\n  # Pure Business Logic\n  agent.run(query)\n</code></pre>\nIt currently handles SQL AST validation, PII redaction, and a &quot;Slop Filter&quot; (using Shannon Entropy to block apologies).<p>It\u2019s open source and local-first. I\u2019m curious if anyone else is using this &quot;sidecar&quot; pattern or if you prefer explicit middleware?<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;imtt-dev&#x2F;steer\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;imtt-dev&#x2F;steer</a>", "author": "steer_dev", "timestamp": "2026-01-12T13:59:57+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:55.427918+00:00", "processed": false}
{"id": "hn_comment_46589334", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46589334", "title": "Re: IntentGrid \u2013 An LLM benchmark requiring spatial re...", "text": "Hi HN,<p>I\u2019ve been experimenting with a different kind of LLM benchmark, and wanted to share it here for feedback.<p>IntentGrid is a language-only, turn-based competitive game designed to test strategic planning, spatial reasoning, and long-horizon decision making in large language models.<p>Instead of puzzles or static tasks, models play a 40-turn adversarial game on a 13\u00d713 grid.\nEach turn, they must:<p>analyze a dense board state,<p>reason about future congestion and forced combat,<p>express intent in natural language,<p>and output a strictly validated action plan.<p>Because 80 units are spawned over 40 turns on a 169-cell board, the system guarantees saturation:\ncombat is unavoidable, and passive survival fails. Timing, positioning, and coordination matter more than tactics alone.<p>A concrete match example (Kimi vs Gemini):\n <a href=\"https:&#x2F;&#x2F;intentgrid.org&#x2F;match&#x2F;25f2530d-c7e6-4553-b231-dff4a982e4cb\" rel=\"nofollow\">https:&#x2F;&#x2F;intentgrid.org&#x2F;match&#x2F;25f2530d-c7e6-4553-b231-dff4a98...</a>", "author": "mingli_yuan", "timestamp": "2026-01-12T14:54:31+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:57.041852+00:00", "processed": false}
{"id": "hn_story_46588138", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46588138", "title": "Show HN : Pilot \u2013 System to improve dramatically your AI coding", "text": "I&#x27;m a non-technical guy who spent 2 months trying to ship software with AI tools. Not toy projects \u2014 real things I wanted to use. Finance analyzers, productivity tools, dev utilities.<p>The models are incredible. But the loop was broken.<p>Every session started from zero. Context would explode. The AI would hallucinate with confidence. And because I can&#x27;t read code, I had no way to verify when something was wrong. I just knew it was broken.\nSo I stopped fighting the model and started building the system around it.<p>Pilot is a &#x2F;pilot folder you drop into any repo. It&#x27;s emergent complexity from simple primitives \u2014 markdown files that give AI tools:<p>Persistent state (STATE.md tracks where you are in the workflow)\nScoped tasks (TASK.md defines boundaries before implementation)\nEvidence capture (real terminal output via MCP, not generated text)\nProtected paths (red zones require human approval)\nRecovery (LKG commit auto-updated after health passes)<p>The core insight: split the AI into two roles.\nOrchestrator (Claude&#x2F;ChatGPT) \u2014 high reasoning, low volume. Writes specs, reviews evidence, manages flow.\nBuilder (Cursor&#x2F;Claude Code) \u2014 high volume, lower cost. Implements, provides proof.\nThe Orchestrator defines scope before the Builder touches anything. The Builder works within boundaries. The Orchestrator reviews after. Two models, two verification passes.\nIt&#x27;s moving from &quot;trust me&quot; to &quot;show me the terminal.&quot;<p>Why I needed this:\nI wanted to program by intuition, not by syntax. I can design systems. I can spec features. I can verify that tests pass and URLs work. What I can&#x27;t do is read 200 lines of generated TypeScript and know if it&#x27;s correct.\nSo the system had to prove correctness without requiring code review. Evidence-based commits. Scope contracts. Clear rejection criteria.\nIt&#x27;s shared intuition for messy realities. Not a sandbox \u2014 I know markdown isn&#x27;t a firewall. It&#x27;s defense in depth: separation of concerns, multi-model review, explicit rules, human gates.<p>Technical notes:\nThe workflow is a state machine: idle \u2192 building \u2192 verifying \u2192 done. Evidence comes from MCP-captured terminal output. The Orchestrator validates Builder output against TASK.md constraints. Red zone violations trigger automatic escalation.\nThe &#x2F;pilot folder is just markdown. Any MCP-enabled tool can read it. No vendor lock-in.<p>Limitations (being honest):\nSolo builder workflow. Team use needs merge strategy for state files.\nConvention-based, not filesystem-enforced. If you need true isolation, run in a container.\nContext can still drift if you skip the workflow. Health checks help, but it&#x27;s not foolproof.\nToken overhead exists. Trading cost for correctness insurance.<p>What I&#x27;ve built with it:\nPrivate projects mostly \u2014 finance analyzer, productivity tools, Framer components, and Pilot itself. Iterating on the workflow every time I hit a wall until the walls stopped appearing.<p>Now using it on bigger things I plan to release.<p>Felt too good not to share.<p>Happy to discuss the architecture, failure modes, or specific edge cases.", "author": "crog", "timestamp": "2026-01-12T13:23:00+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:57.841176+00:00", "processed": false}
{"id": "hn_story_46588052", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46588052", "title": "Show HN: Shellbox \u2013 Instant Linux Boxes via SSH", "text": "I built a service that gives you instant Linux boxes using only SSH. No accounts, no CLI tools, no browser \u2013 just:<p><pre><code>  ssh shellbox.dev\n</code></pre>\nYour SSH key is your identity. First connection creates your account.<p>Commands work over SSH:\n  ssh shellbox.dev create mybox\n  ssh -t shellbox.dev connect mybox\n  ssh shellbox.dev list<p>Each box gets a public HTTPS URL for serving apps or webhooks.<p>Pricing: $0.05&#x2F;hr running, $0.005&#x2F;hr paused. Boxes auto-pause on disconnect and resume where you left off.<p>Even billing stays in the terminal \u2013 run `ssh shellbox.dev funds 10` and you get a QR code right in your shell. Scan it, pay via Paddle, done. No account dashboards.<p>Specs: 2 vCPUs, 4GB RAM, 50GB SSD per box.<p>I wanted something I could spin up from any machine with just a terminal. No installs, no OAuth flows, no &quot;please verify your email.&quot; Works great for quick experiments, running Claude Code in isolation, or giving demos a public URL.<p>Would love feedback. What&#x27;s missing?", "author": "messh", "timestamp": "2026-01-12T13:17:06+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-12T17:15:58.363881+00:00", "processed": false}
{"id": "hn_comment_46587882", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46587882", "title": "Re: Reelive \u2013 Access Sora 2, Veo 3, Kling in one place...", "text": "I built Reelive (<a href=\"https:&#x2F;&#x2F;reelive.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;reelive.ai</a>) \u2013 a platform that gives you access to multiple state-of-the-art AI video generation models through one unified interface.<p>*The problem:*\nAI video generation is fragmented. Sora 2 requires an OpenAI subscription, Veo 3 is limited to specific regions, Kling requires a Chinese phone number, and each model has different APIs and interfaces. Comparing outputs means juggling multiple accounts and credit systems.<p>*The solution:*\nReelive aggregates the best AI video models in one place:\n- OpenAI Sora 2 (text-to-video, image-to-video)\n- Google Veo 3 (premium visual quality)\n- Alibaba Wan 2.5&#x2F;2.6 (fast generation, open source)\n- Kuaishou Kling 2.6 (cinematic motion)\n- ByteDance Seedance (high-quality image-to-video)\n- Minimax Hailuo (efficient generation)<p>*Features:*\n- Unified credit system \u2013 one balance works across all models\n- Image-to-video for all supported models\n- Multiple aspect ratios (16:9, 9:16, 1:1, 4:3)\n- Up to 1080p output\n- Pay-as-you-go or subscription plans<p>*Tech stack:*\nNext.js 16, Drizzle ORM, PostgreSQL, Cloudflare R2 for storage, Stripe for payments.<p>Built this because I wanted to experiment with different AI video models without managing 6+ separate accounts. Happy to answer any questions about the technical implementation or the video generation landscape.<p>Live demo: <a href=\"https:&#x2F;&#x2F;reelive.ai&#x2F;explore\" rel=\"nofollow\">https:&#x2F;&#x2F;reelive.ai&#x2F;explore</a>", "author": "danny_miller", "timestamp": "2026-01-12T13:03:43+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:15:59.129722+00:00", "processed": false}
{"id": "hn_comment_46587740", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46587740", "title": "Re: You're probably vibe coding wrong (and that's why ...", "text": "I\u2019ll say it straight<p>Most people arent failing with AI because it\u2019s weak.. They\u2019re failing because they treat it like magic instead of engineering<p>Ive built production apps this way\nReal users. Real traffic. Real consequences.\nMostly with Cursor. Very little manual intervention<p>But first\u2026 this is likely your current flow:<p>You open your editor\nYou type \u201cbuild me X\u201d\nAI starts strong\u2026 then drifts\nOne fix breaks another thing\nYou restart. Again\nThat\u2019s not building\nThat\u2019s rolling dice!!<p>Here\u2019s the system I use\nIt\u2019s boring. It\u2019s structured\nAnd it works every single time<p>Step 1 : architecture first (before a single line of code)<p>Before touching Cursor, open ChatGPT and ask for structure, not code<p>Describe the product in painful detail \nWhat it does. Who its for. What matters. What doesnt. \nThen ask for:<p>the full architecture \nfolder and file structure \nwhat each part is responsible for \nwhere state lives \nhow things talk to each other<p>Nothing fancy. Markdown only<p>Save this as <a href=\"http:&#x2F;&#x2F;architecture.md\" rel=\"nofollow\">http:&#x2F;&#x2F;architecture.md</a> and drop it into an empty project folder<p>This document is the spine of the app \nIf this is vague, everything downstream will be vague too<p>Step 2 : turn the architecture into small boring tasks<p>Next ask AI to convert that architecture into a task list<p>Not \u201cbuild auth\u201d \nBut \u201ccreate auth schema\u201d, \u201cwire session state\u201d, \u201cprotect route X\u201d.. \nEach task must:<p>be small enough to test \nhave a clear start and end \ntouch one concern only<p>The key detail: tell the AI these tasks will be executed one by one with testing in between<p>This becomes <a href=\"http:&#x2F;&#x2F;tasks.md\" rel=\"nofollow\">http:&#x2F;&#x2F;tasks.md</a> \nAt this point, you still havent written code \nBut the chaos is already gone<p>Step 3 : now I let Cursor work (with rules)<p>Only now open Cursor<p>tell it:\nYou\u2019re an engineer joining this project\nYou\u2019ve been given <a href=\"http:&#x2F;&#x2F;architecture.md\" rel=\"nofollow\">http:&#x2F;&#x2F;architecture.md</a> and <a href=\"http:&#x2F;&#x2F;tasks.md\" rel=\"nofollow\">http:&#x2F;&#x2F;tasks.md</a>\nRead them carefully. No guessing\nThen add strict rules:<p>minimal code only\nno refactors unless asked\nno unrelated changes\ndon\u2019t break existing behavior\nstop after each task so I can test<p>One task\nRun it\nTest it\nCommit it\nRepeat<p>This sounds slower\nIt\u2019s not<p>Why this works (and vibe coding usually doesnt)<p>Most vibe coding fails for one reason: intent isn\u2019t frozen<p>When intent is fuzzy AI fills gaps with guesses\nThose guesses compound\nThats how you get \u201cit worked yesterday\u201d bugs<p>This workflow fixes that\nYou\u2019re not dumping everything into the IDE and hoping\nYou\u2019re giving AI a map\nYou\u2019re keeping it on rails\nYou stay the one making decisions\nAI becomes a fast, obedient engineer\nNot a creative wildcard.<p>This is how you ship clean, testable, AI assisted code\nwithout the spiral.. without rewrites and without fear of touching things later<p>Id normally say \u201cfollow me for the playbook\u201d but f it.. just use it", "author": "Shabamed", "timestamp": "2026-01-12T12:51:26+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["naming_terminology", "tone"], "sentiment": null, "collected_at": "2026-01-12T17:16:00.541441+00:00", "processed": false}
{"id": "hn_story_46587158", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46587158", "title": "Show HN: AI that turns project ideas into structured specs", "text": "Hey HN,\nWe built Max Requirements to solve a problem we kept running into: clients have ideas in their heads but struggle to communicate them in a way developers can work with.<p>It&#x27;s a conversation-based tool where 6 specialized AI agents guide you through requirements gathering \u2014 project scope, user types, user stories, prioritization (MoSCoW), and UX preferences.<p>Output is a structured spec document.\nStack: React, Bun, LangGraph for agent orchestration, Claude Haiku 4.5 via OpenRouter, SQLite.<p>The goal was to replicate what a good product manager does in a discovery session, but in 30 minutes instead of weeks.<p>Free tier available. Would love feedback from anyone who&#x27;s dealt with the pain of unclear requirements.<p>HN folks can use code HACKERNEWS for a free month. \n(works until 12 Feb 2026)", "author": "Eggvelop", "timestamp": "2026-01-12T11:48:33+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["content_clarity", "response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:16:03.663826+00:00", "processed": false}
{"id": "hn_story_46586875", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46586875", "title": "Show HN: Notebooklm-Py \u2013 Unofficial Python API for Google NotebookLM", "text": "Hi HN,\nI\u2019ve been using NotebookLM heavily, but the manual &quot;drag-and-drop&quot; workflow was a bottleneck. I wanted to build automated pipelines (like auto-generating podcasts from a folder of PDFs), but the lack of an official API made this impossible.\nThe Solution: By mapping the internal RPC endpoints used by the web frontend, I built a native Python client that interacts directly with the backend. This bypasses the overhead and brittleness of browser automation tools like Selenium.\nDemo:\nWatch Claude Code use the CLI to automate a workflow in the terminal: <a href=\"https:&#x2F;&#x2F;asciinema.org&#x2F;a&#x2F;767284\" rel=\"nofollow\">https:&#x2F;&#x2F;asciinema.org&#x2F;a&#x2F;767284</a>\nWhat you can do with it:\nDeep Research &amp; Content Automation: Automate the entire research loop. You can programmatically create a new notebook, import deep research on specific topics, and then trigger the generation of both an Audio Overview (podcast) and a Slide Deck, downloading the final assets in one go.\nRAG Pipelines: Use NotebookLM as a grounded backend for agents (query a notebook -&gt; get cited answers).\nCLI for Humans &amp; LLMs: The package includes a comprehensive CLI designed for both end-users and agents. It even features a command to auto-install itself as a &quot;Skill&quot; for Claude Code, letting you control NotebookLM via natural language in your terminal.\nStability &amp; Testing:\nUnofficial APIs are risky. To mitigate silent breakage, I treated this package like a production product. The repo includes a triple-layer test suite (Unit, Integration, and Daily E2E Cron tests). If Google changes their definitions, my CI fails immediately, and we\u2019ll know before users do.\nAuth Note:\nThe library runs on pure Python (ideal for headless servers), but it requires a valid session cookie to start. The CLI makes this easy: just run notebooklm-py login to perform the one-time extraction (it briefly opens a browser via Playwright). Once you have the token, you can deploy it anywhere without a browser.\nRepo:\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;teng-lin&#x2F;notebooklm-py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;teng-lin&#x2F;notebooklm-py</a>\nHappy to answer questions!", "author": "teng-lin", "timestamp": "2026-01-12T11:10:49+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:16:04.836006+00:00", "processed": false}
{"id": "hn_comment_46588899", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46588899", "title": "Re: Anthropic made a big mistake...", "text": "They did not. Anthropic is protecting its huge asset: the Claude Code value chain, which has proven itself to be a winner among devs (me included, after trying everything under the sun in 2025). If anything, Anthropic&#x27;s mistake is that they are incapable of monetizing their great models in the chat market, where ChatGPT reigns: ie. Anthropic did not invest in image generation, Google did and Gemini has a shot at the market now.<p>Apparently nobody gets the Anthropic move: they are only good at coding and that&#x27;s a very thin layer. Opencode and other tools are game for collecting inputs and outputs that can later be used to train their own models - not necessarily being done now, but they could - Cursor did it. Also Opencode makes it all easily swappable, just eval something by popping another API key and let&#x27;s see if Codex or GLM can replicate the CC solution. Oh, it does! So let&#x27;s cancel Claude and save big bucks!<p>Even though CC the agent supports external providers (via the ANTHROPIC_BASE_URL env var), they are working hard on making it impossible for other models to support their every increasing agent feature set (skills, teleport and remote sessions, LSP, Chrome integration, etc). The move totally makes sense, like it or not.", "author": "ojosilva", "timestamp": "2026-01-12T14:22:14+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:16:05.564090+00:00", "processed": false}
{"id": "hn_comment_46585691", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46585691", "title": "Re: Show HN: Self-hosted micro-learning platform with ...", "text": "Hi HN! Built this self-hosted LMS focusing on micro-learning.<p>Key differences from Moodle&#x2F;Canvas:\n- Bite-sized learning units (quiz, video, assignment, discussion)\n- Built-in AI tutor (OpenAI&#x2F;Anthropic&#x2F;Gemini)\n- Competency framework integration\n- Subtitle search inside videos\n- One-command setup: .&#x2F;dev.sh up<p>Tech choices:\n- SolidJS for fine-grained reactivity (real-time tracking)\n- Django Ninja for modern REST API\n- OpenSearch for content search<p>Happy to answer questions about the architecture or design decisions!", "author": "pigon1002", "timestamp": "2026-01-12T08:38:51+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-12T17:16:11.164866+00:00", "processed": false}
