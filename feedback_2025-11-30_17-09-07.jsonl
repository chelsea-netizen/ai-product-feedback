{"id": "hn_story_46097698", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46097698", "title": "Show HN: Cognitive AI architecture prototype with identity, memory, initiative", "text": "Hi HN,<p>I\u2019m working on a \u201chacker science\u201d experiment called Ai_home.\nIt\u2019s a cognitive architecture prototype that I designed to explore the current limits of LLMs in terms of persistent identity, long-term memory, and autonomy.<p>The system is not just a simple chatbot loop, but a multi-threaded architecture:<p>1. Worker: Handles user interactions and tool use.\n2. Monologue: A background \u201csubconscious\u201d thread that analyzes context and logs intuitions&#x2F;tips for the Worker.\n3. Memory: Manages vector-based long-term memory (Postgres + pgvector) with emotional weighting.\n4. Mind: This layer is responsible for deeper interpretation of messages and for exploring creative alternatives.<p>Because of this, it\u2019s not a synchronous question\u2013answer chatbot.\nThe model and the user (the Helper) can communicate in parallel, and the Worker processes this asynchronously.<p>Technical details:<p>- Hybrid Multi-LLM: I combine multiple models (Gemini, GPT-4, Groq). I use different models for creative idea generation (\u201ccreative\u201d) and for logical processing (\u201cinterpreter\u201d).<p>- Modes: I don\u2019t use a single context window. Depending on the operating mode (General, Developer, Analyst), I partition messages into separate contexts. I\u2019ve introduced a transition process between mode switches to ensure that the essential information is preserved across contexts.<p>- Dynamic Prompt: Based on memories and accumulated experience, I dynamically modify the prompt on every API call so that each conversation can gain a fresh contextual interpretation.<p>- Incubator: The system has an experimental environment where it can attempt to refactor its own code. The results are mixed so far, but it\u2019s fascinating to watch a model interpret its own code.<p>- Identity and Laws: For building identity, the system has a \u201cconstitution\u201d (fundamental laws) and tools for modifying them. The content and structure of this are still an active area of experimentation.<p>Disclaimer:\nThis is an architectural experiment to investigate whether functional patterns of consciousness (global workspace, recurrence) can be mimicked with LLMs in order to create more reliable agents.<p>I explicitly do not claim that the system is sentient, nor that this is a formal academic research project (We don\u2019t have the personnel or infrastructure for that).<p>I\u2019m looking for collaborators not only for coding, but also to help define a development methodology for this open, collaborative experimental project.<p>All feedback is very welcome!", "author": "nDot_io", "timestamp": "2025-11-30T15:59:42+00:00", "score": 1, "num_comments": 2, "products": ["chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-30T17:09:11.623324+00:00", "processed": false}
{"id": "hn_story_46094955", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46094955", "title": "Show HN: AIDictation \u2013 zero data retention dictation app", "text": "Hi HN,<p>I built AIDictation.com, a voice to text app written in Swift. It sends audio to my own backend, runs it through a Whisper-based pipeline, and returns a transcription you can then send straight into an AI chat like ChatGPT or Claude.<p>I\u2019ve been building full\u2011stack apps for ~20 years, but this is my first Swift application. I leaned heavily on AI coding tools to get from zero Swift to a working app and backend in a couple of weeks.<p>What it does<p>Records audio and sends it to my server.\nThe backend runs a pipeline using Whisper V3 Turbo + OpenAI GPT OSS 120B.<p>I intentionally went with a cloud pipeline instead of on\u2011device models so I can:\n- Parallelize work on the backend and tune the pipeline.\n- Mix and match providers and models.\n- Improve latency without shipping new app versions.<p>After transcription, there\u2019s a \u201cshare to AI chat\u201d flow so you can send it with one tap to ChatGPT, Claude, etc.<p>Context rules\nOne feature I missed in Whisper Flow was configurable context rules (similar to the Super Whisper Modes). AIDictation lets you define how transcription should behave depending on what you\u2019re doing.<p>For example:\n- Meetings: keep speaker names and timestamps.\n- Coding: preserve technical terms and code formatting.\n- Journaling: be more forgiving, add punctuation, make the text more readable.\n- You can configure different presets and switch between them.<p>Why cloud instead of on\u2011device<p>A lot of apps focus on running models locally. I chose the opposite trade\u2011off:\n- Provider flexibility: right now I\u2019m using the Groq API because, in my tests, it had the best end\u2011to\u2011end latency (700-800ms), but the backend is built to swap providers and models.\n- This does mean audio leaves the device, so I tried to be very explicit about data handling.<p>No registration needed. You get about 2,000 words per month for free without creating an account or giving an email.<p>Tech stack\nClient: Swift (first real Swift&#x2F;iOS app I\u2019ve shipped).\nBackend: NodeJS on Vercel.\nModels: Whisper V3 Turbo + OpenAI GPT OSS 120B.\nProvider: Groq API at the moment, mainly for latency reasons.<p>I\u2019ve been using AIDictation daily for the past couple of weeks, and I\u2019m happy with it so far, but I\u2019d really like candid feedback from HN\u2014both on the product and on the implementation.", "author": "vood", "timestamp": "2025-11-30T08:40:30+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-11-30T17:09:28.716534+00:00", "processed": false}
{"id": "hn_comment_46095462", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46095462", "title": "Re: AI just proved Erdos Problem #124...", "text": "This is response from mathematician:\n&quot;This is quite something, congratulations to Boris and Aristotle!<p>On one hand, as the nice sketch provided below by tsaf confirms, the final proof is quite simple and elementary - indeed, if one was given this problem in a maths competition (so therefore expected a short simple solution existed) I&#x27;d guess that something like the below would be produced. On the other hand, if something like this worked, then surely the combined talents of Burr, Erd\u0151s, Graham, and Li would have spotted it.<p>Normally, this would make me suspicious of this short proof, in that there is overlooked subtlety. But (a) I can&#x27;t see any and (b) the proof has been formalised in Lean, so clearly it just works!<p>Perhaps this shows what the real issue in the [BEGL96] conjecture is - namely the removal of 1 and the addition of the necessary gcd condition. (And perhaps at least some subset of the authors were aware of this argument for the easier version allowing 1, but this was overlooked later by Erd\u0151s in [Er97] and [Er97e], although if they were aware then one would hope they&#x27;d have included this in the paper as a remark.)<p>At the moment I&#x27;m minded to keep this as open, and add the gcd condition in the main statement, and note in the remarks that the easier (?) version allowing 1 and omitting the gcd condition, which was also asked independently by Erd\u0151s, has been solved.&quot;<p>The commentator is saying: &quot;I can&#x27;t believe this famous problem was solved so easily. I would have thought it was a fake proof, but the computer verified it. It turns out the solution works because it addresses a slightly different set of constraints (regarding the number 1) than what Erd\u0151s originally struggled with.  (Generated by Gemini)", "author": "demirbey05", "timestamp": "2025-11-30T10:20:01+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-30T17:09:32.776087+00:00", "processed": false}
{"id": "hn_comment_46094768", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46094768", "title": "Re: AI just proved Erdos Problem #124...", "text": "This seems to be 2nd in row proof from the same author by using the AI models. First time it was the ChatGPT which wrote the formal Lean proof for Erdos Problem #340.<p><a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2510.19804v1#Thmtheorem3\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2510.19804v1#Thmtheorem3</a><p>&gt; In over a dozen papers, beginning in 1976 and spanning two decades, Paul Erd\u0151s repeatedly posed one of his \u201cfavourite\u201d conjectures: every finite Sidon set can be extended to a finite perfect difference set. We establish that {1, 2, 4, 8, 13} is a counterexample to this conjecture.", "author": "menaerus", "timestamp": "2025-11-30T08:02:18+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["onboarding"], "sentiment": null, "collected_at": "2025-11-30T17:09:32.888617+00:00", "processed": false}
{"id": "hn_story_46091019", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46091019", "title": "Show HN: A neuro-symbolic manufacturing engine built in 1 week with Gemini 3.0", "text": "Last week I challenged myself to test the new Gemini 3.0 to see how far its reasoning capabilities could push actual engineering tasks. The result is OpenForge: a Neuro-Symbolic Manufacturing Engine that translates user intent into flight-proven hardware designs and generates a physics-based simulation to test them.<p>I\u2019ve built complex systems before, but the reasoning performance of Gemini 3.0 cut my development time by roughly 70-80%. It allowed me to go from concept to a working Digital Twin pipeline in less than a week.<p>The Architecture (Neuro-Symbolic):\nWe know LLMs hallucinate. In hardware design, a hallucination leads to parts that don&#x27;t fit or drones that fall out of the sky. To solve this, I used a neuro-symbolic approach:\nThe Neuro Layer (Gemini 3.0): Handles the semantic reasoning. It translates vague intents (e.g., &quot;I need a drone to inspect fences on a cattle ranch&quot;) into engineering constraints, selects components from scraped data, and acts as the Systems Architect.\nThe Symbolic Layer (Python&#x2F;Deterministic): The guardrails. We use trimesh for geometry collision, cannon.js&#x2F;Isaac Sim for physics simulation, and rigid math for Thrust-to-Weight ratios.\nHow it works:\nRecon: Agents scrape real e-commerce sites for parts (Motors, FCs, Frames).\nFusion: Gemini reads spec sheets (even from images) to build a structured component database.\nAssembly: An Engineer agent creates a Bill of Materials.\nValidation: Python scripts mathematically verify voltage matching (6S vs 4S), physical clearance, and electronic interconnects (preventing the Pixhawk without an ESC error).\nSimulation: The system generates OpenSCAD models -&gt; USD files -&gt; runs a flight sim to test the build.\nThe Gemini 3.0 Factor:\nThe standout feature wasn&#x27;t code generation, but context adherence. By front-loading heavy architectural context and strict schemas, the model avoided the Garbage In, Garbage Out cycle that usually plagues complex AI workflows. It acted less like a chatbot and more like a junior engineer following a spec sheet.<p>The project is open source. I\u2019m curious to hear what others think about using newer reasoning models for hardware constraint solving.", "author": "knightbat2004", "timestamp": "2025-11-29T21:37:04+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-11-30T17:09:48.365322+00:00", "processed": false}
