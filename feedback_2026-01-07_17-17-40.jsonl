{"id": "hn_story_46529120", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46529120", "title": "Show HN: I spent 12 months building a conversational agent for social media", "text": "Hi HN,<p>I\u2019m John, founder of PostReach AI. For the last 12 months, my team (Edric, Jasper, and I) has been heads-down building what we call a &quot;conversational engine&quot; for social media.<p>The Problem: As founders, we all know we should be active on LinkedIn, X, and Facebook&#x2F;Instagram. But the reality is a fragmented mess, you use ChatGPT for ideas, Canva for design, and Hootsuite for scheduling. It\u2019s a context-switching tax that most small teams can\u2019t afford.<p>What we built: PostReach isn\u2019t a dashboard, it\u2019s a chat interface. We wanted to see if we could collapse the entire &quot;idea-to-published&quot; stack into a single conversation.<p>Some technical bits the community might find interesting:<p>Brand Ingestion: We don&#x27;t just prompt a generic LLM. We built a &quot;Brand Brain&quot; that ingests your website and past voice to ensure the AI doesn&#x27;t sound like a generic bot.<p>Visual Analysis: Our Image-to-Post feature uses visual reasoning to understand the &quot;mood&quot; of an image. If you upload a photo of your messy dev desk, it won&#x27;t just say &quot;Working hard&quot;, it understands the hardware and context to write something relatable.<p>URL-to-Post: We use a custom summarization pipeline to extract the &quot;hook&quot; from any article or blog post and transform it into platform-specific formats (e.g., a technical post for X vs. a professional takeaway for LinkedIn).<p>Why now? We\u2019ve just moved into beta after aggressive testing. We are looking for our first &quot;real world&quot; cohort of users.<p>The Ask: We would love for the HN community to break this.<p>Does the conversational flow feel faster than a dashboard?<p>Is the AI output &quot;cringe&quot; or actually usable for a technical audience?<p>What\u2019s missing from your current workflow that we should automate next?<p>We\u2019re around all day to answer questions about the tech stack, the 12-month dev cycle, or our thoughts on the future of autonomous agents in marketing.", "author": "John_V", "timestamp": "2026-01-07T17:13:05+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:41.236527+00:00", "processed": false}
{"id": "hn_story_46528730", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46528730", "title": "Show HN: Bind.ly \u2013 Persistent memory for AI across tools", "text": "Hi HN,<p>I\u2019m a product designer by background, not a traditional software engineer.<p>Over the last year, tools like Claude and ChatGPT completely changed how I work.\nI started with small internal tools, and now I\u2019m \u201cvibe coding\u201d multiple highly personalized apps.<p>As I iterated between Claude Code (implementation) and ChatGPT (ideation &#x2F; thinking),\nI kept running into the same problem.<p>To think clearly, I had to repeatedly re-explain:<p>- what the code currently does,<p>- what changed recently,<p>- and why certain decisions were made.<p>That re-summarization step became a real bottleneck.<p>So I built Bindly (bind.ly).<p>Bindly is a persistent knowledge layer that sits outside any single AI tool.\nThe key idea is simple:<p>AI doesn\u2019t remember. It re-reads shared context from the same place every time.<p>The concrete workflow I use today<p>This is the workflow Bindly is built around:<p>1) Claude Code \u2192 Bindly<p>After coding or refactoring, I ask Claude Code to summarize what changed and why,\nand store that context in Bindly.<p>2) ChatGPT \u2192 Ideation using Bindly<p>I then switch to ChatGPT and ideate based on that stored context \u2014\narchitecture, tradeoffs, next steps.\nThose ideation results are saved back into Bindly.<p>3) Claude Code \u2192 Reuse the ideation<p>Finally, I bring those ideation results back into Claude Code\nto continue implementation.<p>Bindly becomes the shared memory that closes this loop,\nwithout constantly restating everything.<p>To reduce both cognitive load and token usage,\nBindly applies lightweight diffs (inspired by Git) and progressive search,\nso AIs only re-read what actually changed\nor what\u2019s relevant right now.<p>In short:<p>- Bindly doesn\u2019t try to replace AI thinking.<p>- It stores what exists, what was decided, and why \u2014\n  so any AI can continue from the same point.<p>Personally, this workflow already saves me a lot of cognitive overhead.\nBut I\u2019m unsure whether this is just a personal productivity hack\nor something others would actually pay for.<p>I\u2019m curious whether others who bounce between multiple AI tools\nrun into the same problem.<p>Infrastructure uncertainty (and why I moved to Cloudflare)<p>I initially built the MVP on Fly.io.\nIt worked, and I don\u2019t think Fly.io is a bad platform.\nBut as Bindly grew, I became uncomfortable with how opaque things felt \u2014\nvolumes, persistence, failure modes.<p>Bindly is meant to be a knowledge layer.\nIf data is lost or hard to reason about,\nthat completely breaks the trust model.<p>For a tool whose purpose is \u201cdon\u2019t lose context,\u201d\nthat risk felt existential.<p>I realized I was spending more time worrying about infrastructure\nthan thinking about the product itself.<p>So I recently consolidated everything onto Cloudflare\n(Pages, Workers, R2, Vectorize),\naiming to reduce operational uncertainty\nand keep the system as boring and predictable as possible.<p>So far, it feels simpler and more transparent.\nBut as a non-engineer, I\u2019m unsure\nif this is a smart long-term decision or a local optimum.<p>What I\u2019d really appreciate feedback on:<p>1) Does a shared \u201cknowledge &#x2F; memory layer\u201d like this\n   feel useful beyond one person?<p>2) As a non-engineer, is consolidating on Cloudflare\n   a reasonable long-term choice?<p>Some extra context (optional):<p>I joined HN back in 2018 to apply to YC\nand interviewed in Nov 2018 \u2014 didn\u2019t make it in the final round.\nThis is my first time posting here in years.<p>Current ways I use Bindly:<p>- via Claude (web)<p>- via the ChatGPT app<p>- via MCP (so AIs can re-query stored knowledge)<p>If you have time to share thoughts \u2014\neven critical ones \u2014\nI\u2019d be very grateful.<p>Happy New Year, and thanks for reading!", "author": "seongjaeryu", "timestamp": "2026-01-07T16:49:37+00:00", "score": 1, "num_comments": 1, "products": ["claude", "chatgpt"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:42.834876+00:00", "processed": false}
{"id": "hn_comment_46528117", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46528117", "title": "Re: LLM Problems Observed in Humans...", "text": "While I haven&#x27;t experienced LLMs correcting most (or any) of the problems listed fully and consistently, I do agree that consistent use of LLMs and dealing with their frustrations has worn my patience for conversations with people who exhibit the same issues when talking.<p>It&#x27;s kind of depressing.  I just want the LLM to be a bot that responds to what I say with a useful response.  However, for some reason, both Gemini and ChatGPT tend to argue with me so heavily and inject their own weird stupid ideas on things making it even more grating to interact with them which chews away at my normal interpersonal patience which, as someone on the spectrum, was already limited.", "author": "chankstein38", "timestamp": "2026-01-07T16:13:40+00:00", "score": null, "num_comments": null, "products": ["chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:50.827134+00:00", "processed": false}
{"id": "hn_comment_46527347", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46527347", "title": "Re: Show HN: KektorDB \u2013 Lightweight, Embeddable Vector...", "text": "Hi HN, author here.<p>I started KektorDB as a personal challenge to learn Go and database internals. Soon, however, I got hooked: I wanted the project to have some dignity beyond a simple &quot;toy project&quot;.<p>I didn\u2019t follow a rigid roadmap; I iterated based on what felt right. I started by implementing caching and a semantic firewall, and from there, the step towards an integrated RAG pipeline was natural.<p>To be honest, the choice to integrate RAG comes from my laziness. I tried building a system using Python and LangChain, but I hated managing external scripts and dependencies just to make data talk to the LLM. I wanted a &quot;batteries-included&quot; solution.<p>However, the first results of my &quot;naive&quot; RAG were disappointing. That\u2019s why I decided to integrate a Lightweight Graph (to semantically link chunks) and techniques like HyDe directly into the engine. All while keeping a fixed constraint: it must remain a single binary, easily embeddable as a Go library.<p>While KektorDB is a general-purpose embeddable Vector + Graph database, its RAG pipeline is intentionally designed as a practical default. It&#x27;s not a replacement for complex, heavily customized RAG infrastructures, but a way to get a local system working quickly.<p>Here is a quick overview of the features:<p>- HNSW Indexing: With support for Float32, Float16, and Int8 quantization.<p>- Hybrid Search: Combines vector similarity with BM25 keyword scoring for better accuracy.<p>- Graph Layer: Maintains a generic adjacency graph alongside vectors. Although the RAG pipeline uses it to link chunks, the system exposes APIs to define arbitrary relationships enabling semantic traversal.<p>- Persistence: AOF (Append-Only File) + Snapshot.<p>- RAG Features: Background worker for document ingestion + integrated proxy for query rewriting and Grounded HyDe (OpenAI-compatible).<p>Current Limitations:<p>1. It is currently RAM-bound (graph and vectors live in memory). I am working on a hybrid disk-storage engine.<p>2. Ingestion parsing can be improved (especially regarding tables in PDFs).<p>The code is pure Go (with optional Rust kernels for specific SIMD operations), all contained in a single binary.<p>The project started out of a desire to learn, but I would like to continue developing it seriously. For this reason, I would appreciate any kind of technical advice or feedback.<p>Thanks for reading.<p>Repository: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;sanonone&#x2F;kektordb\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;sanonone&#x2F;kektordb</a>", "author": "san0n", "timestamp": "2026-01-07T15:17:03+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:54.270187+00:00", "processed": false}
{"id": "hn_comment_46527026", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46527026", "title": "Re: Sora2...", "text": "Discovering Sora 2: A Game Changer in Video Creation\nHey folks! I want to share something truly exciting that\u2019s making waves in the video creation space\u2014Sora 2 from OpenAI. Released in September 2025, this innovative tool is designed to help anyone create stunning videos without the usual hassle. Let\u2019s take a closer look at what makes Sora 2 stand out.\nWhat is Sora 2?\nSora 2 is a versatile video generator that allows you to create videos from text prompts or images. It\u2019s not just about making videos; it\u2019s about making them better and faster. Here are some key features:<p>Synchronized Audio: One of the coolest things about Sora 2 is its ability to generate audio that matches the video perfectly. This means you get seamless dialogue, sound effects, and background music without needing extra editing.<p>Video Length Options: You can create videos that are up to 25 seconds long with the Pro version. This is great for storytelling or showcasing products effectively.<p>Variety of Styles: Whether you want an anime look, a cinematic feel, or something more artistic, Sora 2 lets you choose from various styles, and switching between them is easy.<p>How to Use Sora 2\nGetting started with Sora 2 is simple. Here\u2019s a quick rundown of the steps:<p>Select Your Mode: Choose between text-to-video or image-to-video options.<p>Write Your Scene: Describe what you want in detail. The more specifics you provide, the better the output.<p>Adjust Settings: Decide on the video length, aspect ratio (16:9 or 9:16), and style.<p>Generate Your Video: Click generate and watch as Sora 2 creates your video in just a couple of minutes.<p>Unique Features Worth Noting\nOne standout feature is the Characters option. You can record yourself or someone else, and Sora 2 will integrate that person\u2019s likeness and voice into the generated videos. This adds a personal touch that\u2019s hard to achieve with other tools.\nFor those using the Pro version, there\u2019s a Storyboard mode that allows you to plan and arrange multiple scenes, making it easier to create complex narratives.\nWhy It Matters\nIf you\u2019re into making videos\u2014whether for social media, marketing, or personal projects\u2014Sora 2 can save you a lot of time and effort. It\u2019s designed to help you focus on creativity rather than getting bogged down in technical details.\nThe pricing is also quite reasonable. You can create 10-second videos for just 2 credits, and 15-second videos for 3 credits. The Pro version offers even more features for a few extra credits.\nFinal Thoughts\nSora 2 is a fantastic tool for anyone looking to elevate their video creation game. With its quick generation times and high-quality outputs, it\u2019s perfect for creators who want to produce engaging content without the usual headaches.\nGive Sora 2 a shot and see how it can transform your video projects. I\u2019d love to hear your thoughts or experiences with it in the comments! Happy creating!", "author": "xbaicai", "timestamp": "2026-01-07T14:53:37+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["onboarding", "response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:56.337310+00:00", "processed": false}
{"id": "hn_story_46527020", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46527020", "title": "Show HN: YoloForge \u2013 Create object detection datasets using Gemini 3 Pro", "text": "Hi HN, I\u2019m the creator of YoloForge. I built this because I hit a wall with a hobby computer vision project: I needed a custom dataset, and zero-shot tools like Grounding DINO just weren&#x27;t accurate enough for my specific classes. I decided I\u2019d rather write code for a couple of weeks than draw another box by hand.<p>I previously experimented with Grounding DINO and SAM3. While they are amazing for generic objects, I found they struggle with specific semantic requests (e.g. specific manufacturing parts, game characters or distinguishing &quot;a worker&quot; from &quot;a worker without a helmet&quot;).<p>I discovered that Gemini 3 Pro is surprisingly underrated for bounding box tasks if you prompt it with detailed visual descriptions. It handles semantic understanding significantly better than standard zero-shot detectors.<p>url: yoloforge.com<p>The Workflow:<p>Upload a zip of raw images (stored in Cloudflare R2).\nDescribe class&#x2F;classes in plain English.\nThe system generates a .jsonl batch file and sends it to the Gemini Batch API. This allows us to process thousands of images in parallel at 50% of the standard cost.\nYou review&#x2F;correct boxes in the UI and export the YOLO train&#x2F;val&#x2F;test dataset.<p>Technical Challenges:<p>One hard part was getting valid JSON out of the LLM consistently. I ended up writing a robust parser that uses regex fallback strategies to literally &quot;salvage&quot; valid bounding boxes from malformed responses.<p>The Stack:<p>- Frontend: Next.js\n- Backend: FastAPI, Celery (for async zip processing and polling the batch API), Redis.\n- Storage: Supabase (Auth&#x2F;DB), Cloudflare R2 (Image Storage).\n- Model: Google Gemini 3 Pro via Batch API.<p>There is a live demo on the landing page (no sign-up required) where you can upload a single image to test the detection logic. But of course the tool really shines with datasets that have thousands of images with multiple classes.<p>If you have any technical questions please ask!", "author": "Olibier", "timestamp": "2026-01-07T14:53:17+00:00", "score": 3, "num_comments": 0, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:56.375505+00:00", "processed": false}
{"id": "hn_story_46526774", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46526774", "title": "Anthropic silently rewriting Claude punctuation output in API", "text": "", "author": "firasd", "timestamp": "2026-01-07T14:32:58+00:00", "score": 1, "num_comments": 1, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:59.086270+00:00", "processed": false}
{"id": "hn_comment_46526787", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46526787", "title": "Re: Anthropic silently rewriting Claude punctuation ou...", "text": "Documenting this odd behavior where Claude can&#x27;t seem to output smart quotes at all. As Sonnet notes, the justification is somewhat hard to understand...", "author": "firasd", "timestamp": "2026-01-07T14:34:03+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:17:59.124037+00:00", "processed": false}
{"id": "hn_story_46526088", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46526088", "title": "Show HN: KeelTest \u2013 AI-driven VS Code unit test generator with bug discovery", "text": "I built this because Cursor, Claude Code and other agentic AI tools kept giving me tests that looked fine but failed when I ran them. Or worse - I&#x27;d ask the agent to run them and it would start looping: fix tests, those fail, then it starts &quot;fixing&quot; my code so tests pass, or just deletes assertions so they &quot;pass&quot;.<p>Out of that frustration I built KeelTest - a VS Code extension that generates pytest tests and executes them, got hooked and decided to push this project forward... When tests fail, it tries to figure out why:<p>- Generation error: Attemps to fix it automatically, then tries again<p>- Bug in your source code: flags it and explains what&#x27;s wrong<p>How it works:<p>- Static analysis to map dependencies, patterns, services to mock.<p>- Generate a plan for each function and what edge cases to cover<p>- Generate those tests<p>- Execute in &quot;sandbox&quot;<p>- Self-heal failures or flag source bugs<p>Python + pytest only for now. Alpha stage - not all codebases work reliably. But testing on personal projects and a few production apps at work, it&#x27;s been consistently decent. Works best on simpler applications, sometimes glitches on monorepos setups. Supports Poetry&#x2F;UV&#x2F;plain pip setups.<p>Install from VS Code marketplace: <a href=\"https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=KeelCode.keeltest\" rel=\"nofollow\">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=KeelCode...</a><p>More detailed writeup how it works: <a href=\"https:&#x2F;&#x2F;keelcode.dev&#x2F;blog&#x2F;introducing-keeltest\" rel=\"nofollow\">https:&#x2F;&#x2F;keelcode.dev&#x2F;blog&#x2F;introducing-keeltest</a><p>Free tier is 7 tests files&#x2F;month (current limit is &lt;=300 source LOC). To make it easier to try without signing up, giving away a few API keys (they have shared ~30 test files generation quota):<p>KEY-1: tgai_jHOEgOfpMJ_mrtNgSQ6iKKKXFm1RQ7FJOkI0a7LJiWg<p>KEY-2: tgai_NlSZN-4yRYZ15g5SAbDb0V0DRMfVw-bcEIOuzbycip0<p>KEY-3: tgai_kiiSIikrBZothZYqQ76V6zNbb2Qv-o6qiZjYZjeaczc<p>KEY-4: tgai_JBfSV_4w-87bZHpJYX0zLQ8kJfFrzas4dzj0vu31K5E<p>Would love your honest feedback where this could go next, and on which setups it failed, how it failed, it has quite verbose debug output at this stage!", "author": "bulba4aur", "timestamp": "2026-01-07T13:22:35+00:00", "score": 19, "num_comments": 6, "products": ["claude"], "categories": ["error_messages", "tone", "response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:18:05.377797+00:00", "processed": false}
{"id": "hn_comment_46524718", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46524718", "title": "Re: Continuous AI on Your Terminal...", "text": "Most coding CLIs I&#x27;ve seen lock you into one provider or requires you to bypass by changing BASE_URL and has a lot of conflict. That works fine if you&#x27;re committed to one vendor for coding cli harness, but it breaks down when you want to run local models, test different providers, or avoid API costs entirely.\nSo we tried a different approach. Instead of hardcoding a provider, Autohand code lets you swap between OpenRouter, Anthropic, OpenAI, Ollama, llama.cpp, and MLX from the same codebase. Switch models mid-conversation if you want.\nHigh level, the design optimizes for three things:<p>Machine orchestration: stateless execution, structured outputs, designed for CI&#x2F;CD and batch runs, not just interactive use\nAuto mode: autohand -p &quot;fix the tests&quot; --yes --auto-commit runs the full task without prompts. Three permission levels plus granular command whitelist&#x2F;blacklist\nSkills system: modular instruction packages that activate on demand. Run --auto-skill and it generates skills tailored to your project<p>One thing that&#x27;s been surprisingly useful: because it&#x27;s provider-agnostic, you can prototype with a fast cheap model, then swap to something heavier for the actual run. No code changes, just config.\nIt&#x27;s TypeScript + Bun, 40+ tools (file ops, full git, semantic search, multi-file edits), sessions persist and resume.", "author": "igorpcosta", "timestamp": "2026-01-07T10:31:31+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-07T17:18:15.478224+00:00", "processed": false}
{"id": "hn_comment_46526810", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46526810", "title": "Re: Htmx: High Power Tools for HTML...", "text": "I would probably not build an actual app with HTMX but I found it to be excellent for just making a completely static page feel more dynamic.  I&#x27;m using it on my two blogs and it makes the whole experience feel much snappier and allows me to carry through an animation from page to page.<p>The amount of custom stuff I needed to add was minimal (just mostly ensuring that if network is gone, it falls back to native navigation to error out).<p>Examples: <a href=\"https:&#x2F;&#x2F;lucumr.pocoo.org&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;lucumr.pocoo.org&#x2F;</a> and <a href=\"https:&#x2F;&#x2F;dark.ronacher.eu&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;dark.ronacher.eu&#x2F;</a><p>I also found Claude to be excellent at understanding HTMX so that definitely helps.", "author": "the_mitsuhiko", "timestamp": "2026-01-07T14:35:59+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-01-07T17:18:17.675259+00:00", "processed": false}
