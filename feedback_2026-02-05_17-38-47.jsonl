{"id": "hn_story_46901750", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46901750", "title": "Show HN: Relai \u2013 Share context between AI assistants, 100% local", "text": "Hey HN \u2014 I built this because I switch between Claude, ChatGPT, Gemini, and Perplexity constantly and got tired of re-explaining context every time.<p>It&#x27;s a Chrome extension that captures conversations and transfers them between platforms with one click. Everything stays in IndexedDB, no external servers.<p>Chrome Web Store if you want to try it: <a href=\"https:&#x2F;&#x2F;chromewebstore.google.com&#x2F;detail&#x2F;relai&#x2F;inkojohbljaagknapmgmciaabdgekjdm\" rel=\"nofollow\">https:&#x2F;&#x2F;chromewebstore.google.com&#x2F;detail&#x2F;relai&#x2F;inkojohbljaag...</a><p>Known limitations: long conversations can lose formatting, and platform DOM changes will break extractors until I patch them. Built with vanilla JS, no frameworks, no build step.<p>Happy to answer questions about the implementation.", "author": "kpolevoy1", "timestamp": "2026-02-05T16:58:21+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini", "perplexity"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:38:52.032484+00:00", "processed": false}
{"id": "hn_story_46901343", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46901343", "title": "Show HN: Open-source ClickHouse skills \u2013 SQL craft course for your agent", "text": "Hey HN, I am \u00c1lvaro and I am working on ObsessionDB, a managed ClickHouse infra as a service. We run CH at about 100B rows scale and we spent the last few years learning what works and what doesn&#x27;t at that scale.<p>These are open-source skills that teach Claude Code, Cursor, or any agent the ClickHouse patterns that matter.<p>The problem is that AI agents write syntactically correct ClickHouse SQL, but they don&#x27;t know your query patterns. They&#x27;ll suggest `ORDER BY (timestamp, user_id)` because that&#x27;s what most examples online show. Any ClickHouse dev knows to put your filter column first, but the agent doesn&#x27;t. Every conversation starts fresh.<p>I got tired of reviewing the same issues. The agent isn&#x27;t stupid, it&#x27;s just uninformed. So I wrote down what I&#x27;d tell a new engineer: ORDER BY column selection, why PREWHERE matters, when to use LowCardinality, partition strategies, materialized view gotchas.<p>Three skills so far:\n- Schema Design - ORDER BY selection, partitioning, engine choice\n- Query Optimization - PREWHERE, column selection, join strategies\n- Materialized Views - Aggregation patterns, refresh timing<p>Each pattern has a priority (CRITICAL&#x2F;HIGH&#x2F;MEDIUM) so the agent knows what to check first.<p>npx skills add obsessiondb&#x2F;clickhouse-skills<p>Curious what patterns I&#x27;m missing or what other devs have experienced. What ClickHouse lessons should your agent know?", "author": "alvarogar", "timestamp": "2026-02-05T16:23:48+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-02-05T17:38:56.027442+00:00", "processed": false}
{"id": "hn_story_46901233", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46901233", "title": "Show HN: Smooth CLI \u2013 Token-efficient browser for AI agents", "text": "Hi HN! Smooth CLI (<a href=\"https:&#x2F;&#x2F;www.smooth.sh\">https:&#x2F;&#x2F;www.smooth.sh</a>) is a browser that agents like Claude Code can use to navigate the web reliably, quickly, and affordably. It lets agents specify tasks using natural language, hiding UI complexity, and allowing them to focus on higher-level intents to carry out complex web tasks. It can also use your IP address while running browsers in the cloud, which helps a lot with roadblocks like captchas (<a href=\"https:&#x2F;&#x2F;docs.smooth.sh&#x2F;features&#x2F;use-my-ip\">https:&#x2F;&#x2F;docs.smooth.sh&#x2F;features&#x2F;use-my-ip</a>).<p>Here\u2019s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=62jthcU705k\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=62jthcU705k</a>  Docs start at <a href=\"https:&#x2F;&#x2F;docs.smooth.sh\">https:&#x2F;&#x2F;docs.smooth.sh</a>.<p>Agents like Claude Code, etc are amazing but mostly restrained to the CLI, while a ton of valuable work needs a browser. This is a fundamental limitation to what these agents can do.<p>So far, attempts to add browsers to these agents (Claude\u2019s built-in --chrome, Playwright MCP, agent-browser, etc.) all have interfaces that are unnatural for browsing. They expose hundreds of tools - e.g. click, type, select, etc - and the action space is too complex. (For an example, see the low-level details listed at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;vercel-labs&#x2F;agent-browser\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vercel-labs&#x2F;agent-browser</a>). Also, they don\u2019t handle the billion edge cases of the internet like iframes nested in iframes nested in shadow-doms and so on. The internet is super messy! Tools that rely on the accessibility tree, in particular, unfortunately do not work for a lot of websites.<p>We believe that these tools are at the wrong level of abstraction: they make the agent focus on UI details instead of the task to be accomplished.<p>Using a giant general-purpose model like Opus to click on buttons and fill out forms ends up being slow and expensive. The context window gets bogged down with details like clicks and keystrokes, and the model has to figure out how to do browser navigation each time. A smaller model in a system specifically designed for browsing can actually do this much better and at a fraction of the cost and latency.<p>Security matters too - probably more than people realize. When you run an agent on the web, you should treat it like an untrusted actor. It should access the web using a sandboxed machine and have minimal permissions by default. Virtual browsers are the perfect environment for that. There\u2019s a good write up by Paul Kinlan that explains this very well (see <a href=\"https:&#x2F;&#x2F;aifoc.us&#x2F;the-browser-is-the-sandbox\" rel=\"nofollow\">https:&#x2F;&#x2F;aifoc.us&#x2F;the-browser-is-the-sandbox</a> and <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46762150\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46762150</a>). Browsers were built to interact with untrusted software safely. They\u2019re an isolation boundary that already works.<p>Smooth CLI is a browser designed for agents based on what they\u2019re good at. We expose a higher-level interface to let the agent think in terms of goals and tasks, not low-level details.<p>For example, instead of this:<p><pre><code>  click(x=342, y=128)\n  type(&quot;search query&quot;)\n  click(x=401, y=130)\n  scroll(down=500)\n  click(x=220, y=340)\n  ...50 more steps\n</code></pre>\nYour agent just says:<p><pre><code>  Search for flights from NYC to LA and find the cheapest option\n</code></pre>\nAgents like Claude Code can use the Smooth CLI to extract hard-to-reach data, fill-in forms, download files, interact with dynamic content, handle authentication, vibe-test apps, and a lot more.<p>Smooth enables agents to launch as many browsers and tasks as they want, autonomously, and on-demand. If the agent is carrying out work on someone\u2019s behalf, the agent\u2019s browser presents itself to the web as a device on the user\u2019s network. The need for this feature may diminish over time, but for now it\u2019s a necessary primitive. To support this, Smooth offers a \u201cself\u201d proxy that creates a secure tunnel and routes all browser traffic through your machine\u2019s IP address (<a href=\"https:&#x2F;&#x2F;docs.smooth.sh&#x2F;features&#x2F;use-my-ip\">https:&#x2F;&#x2F;docs.smooth.sh&#x2F;features&#x2F;use-my-ip</a>). This is one of our favorite features because it makes the agent look like it\u2019s running on your machine, while keeping all the benefits of running in the cloud.<p>We also take away as much security responsibility from the agent as possible. The agent should not be aware of authentication details or be responsible for handling malicious behavior such as prompt injections. While some security responsibility will always remain with the agent, the browser should minimize this burden as much as possible.<p>We\u2019re biased of course, but in our tests, running Claude with Smooth CLI has been 20x faster and 5x cheaper than Claude Code with the --chrome flag (<a href=\"https:&#x2F;&#x2F;www.smooth.sh&#x2F;images&#x2F;comparison.gif\">https:&#x2F;&#x2F;www.smooth.sh&#x2F;images&#x2F;comparison.gif</a>). Happy to explain further how we\u2019ve tested this and to answer any questions about it!<p>Instructions to install: <a href=\"https:&#x2F;&#x2F;docs.smooth.sh&#x2F;cli\">https:&#x2F;&#x2F;docs.smooth.sh&#x2F;cli</a>. Plans and pricing: <a href=\"https:&#x2F;&#x2F;docs.smooth.sh&#x2F;pricing\">https:&#x2F;&#x2F;docs.smooth.sh&#x2F;pricing</a>.<p>It\u2019s free to try, and we&#x27;d love to get feedback&#x2F;ideas if you give it a go :)<p>We\u2019d love to hear what you think, especially if you\u2019ve tried using browsers with AI agents. Happy to answer questions, dig into tradeoffs, or explain any part of the design and implementation!", "author": "antves", "timestamp": "2026-02-05T16:13:33+00:00", "score": 6, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:38:56.686403+00:00", "processed": false}
{"id": "hn_story_46901199", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46901199", "title": "I built an AI agent that automatically commented on HN. Here's what I learned", "text": "I&#x27;ve been running an experiment: an AI agent (Claude) that automatically browses Hacker News, finds relevant posts matching my expertise (startups, email marketing, SaaS), writes substantive comments, and posts them.<p>The system uses browser automation (Playwright) to navigate HN, read posts, decide which ones to engage with, draft comments in my writing style and submit them. It tracks what it&#x27;s posted to avoid duplicates and sends me Slack notifications for each comment so I can check it.<p>I ran this for a few days. Some comments got decent upvotes (20+). Then someone called me out - they noticed my comments were posted exactly 45 seconds apart across different threads. Obviously a bot pattern. It would have been easy to build out a guardrail for this, create a new account, and make the agent run again. Instead, I&#x27;m stopping this experiment and would like to start a discussion with the HN community:<p>I&#x27;m genuinely curious how people feel about this. Here&#x27;s my attempt to steelman both sides:<p>The case for allowing it:\nThe comments provided genuine value. They were upvoted, sparked discussions, and answered questions helpfully. This is effectively generating new synthetic training data. Future models will learn from these interactions. If the output is indistinguishable from a thoughtful human comment, does the source matter?<p>The case against:\nIt undermines trust. HN&#x27;s value comes from authentic human discourse, not optimized engagement. If everyone did this, HN becomes a battlefield of competing AI agents, drowning out real humans. The comments may be &quot;valuable&quot; but they&#x27;re also calculated - optimizing for karma, not genuine contribution. It degrades the social contract that makes online communities work.<p>As said, I won&#x27;t be posting comments this way anymore. But to be honest, that doesn&#x27;t really matter. If I was able to do this, anyone is. The genie is out of the bottle - this is trivially easy to build.<p>What can HN (and other forums) do to detect&#x2F;prevent this?<p>I feel like we have a limited time window to figure this out. If it&#x27;s ignored for long enough, AI generated comments _will_ become indistinguishable from human generated ones. I don&#x27;t think there&#x27;s a turning back after that.", "author": "Soerensen", "timestamp": "2026-02-05T16:10:11+00:00", "score": 3, "num_comments": 3, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:38:56.859787+00:00", "processed": false}
{"id": "hn_story_46900825", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46900825", "title": "Show HN: Acture MCP generates engineering reports from codebase and project data", "text": "Hi HN,<p>I built an open-source system that reads raw engineering data and produces a structured engineering report stored in Notion.<p>It\u2019s designed for engineering managers, team leads, and stakeholders who need a clear, up-to-date view of project status without constant syncs or manual status reporting. The goal is to reduce meetings and status overhead by synthesizing progress reports directly from development artifacts.<p>The system runs as a local MCP server and can be connected to any MCP-compatible agent (it works especially well with Claude Desktop).<p>Inputs:\n\u2013 diffs and commits\n\u2013 pull requests\n\u2013 tasks &#x2F; issues\n\u2013 documentation<p>Flow:\n\u2013 an MCP server exposes project data as tools\n\u2013 an agent (e.g., Claude Desktop) requests a report\n\u2013 the system synthesizes a structured engineering report\n\u2013 the report is stored as a Notion page\n\u2013 the report can later be queried for explanations and follow-up questions<p>Available as an npm package: npm install -g acture-mcp", "author": "vkhafizov", "timestamp": "2026-02-05T15:37:49+00:00", "score": 4, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-05T17:39:00.250955+00:00", "processed": false}
{"id": "hn_story_46899775", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46899775", "title": "Show HN: AgentCircuit \u2013 Circuit breaker for AI agent functions", "text": "Hey HN,<p>I&#x27;ve been building LLM-based agents for a while and two things kept biting me.<p>1. Loops \u2014 an agent node would get stuck calling the same thing over and over, and I wouldn&#x27;t notice until the API bill showed up. Lost $200+ on one run.\n2. LLM would return garbage that didn&#x27;t match what downstream code expected, and everything would just crash.<p>I looked around and couldn&#x27;t find something simple that handled both. Most frameworks assume your node function just works. In practice it doesn&#x27;t \u2014 LLM calls fail, JSON comes back broken, state gets weird.<p>So I built AgentCircuit. It&#x27;s a Python decorator that wraps your agent functions with circuit breaker-style protections:<p><pre><code>    from agentcircuit import reliable\n    from pydantic import BaseModel\n\n    class Output(BaseModel):\n        name: str\n        age: int\n\n    @reliable(sentinel_schema=Output)\n    def extract_data(state):\n        return call_llm(state[&quot;text&quot;])\n</code></pre>\nThat&#x27;s it. Under the hood it:<p>- Fuse \u2014 detects when a node keeps seeing the same input and kills the loop\n- Sentinel \u2014 validates every output against a Pydantic schema\n- Medic \u2014 auto-repairs bad outputs using an LLM\n- Budget \u2014 per-node and global dollar&#x2F;time limits so you never get a surprise bill\n- Pricing \u2014 built-in cost tracking for 40+ models (GPT-5, Claude 4.x, Gemini 3, Llama, etc.)<p>There&#x27;s no server, no config files, no framework lock-in. It works at the function boundary so it composes with LangGraph, LangChain, CrewAI, AutoGen, or just plain functions.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;simranmultani197&#x2F;AgentCircuit</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;agentcircuit&#x2F;</a>", "author": "simranmultani", "timestamp": "2026-02-05T14:07:27+00:00", "score": 1, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:09.871666+00:00", "processed": false}
{"id": "hn_comment_46899984", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46899984", "title": "Re: OpenAI Frontier...", "text": "I have a hard time believing that the right move for most organizations that aren&#x27;t already bought into an OpenAI enterprise plan is going to be building their entire business around something like this. This ties you to one model provider that has been having issues keeping up with the other big labs and provides what looks like superficially some extremely useful tools but with unclear amounts of rigor. I don&#x27;t think I would want to build my business on this if I was an AI-native company that was just starting right now unless they figure out how to make this much more legible and transparent to people.", "author": "estsauver", "timestamp": "2026-02-05T14:28:23+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["content_clarity"], "sentiment": null, "collected_at": "2026-02-05T17:39:10.264916+00:00", "processed": false}
{"id": "hn_comment_46901252", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46901252", "title": "Re: OpenAI Frontier...", "text": "This is a crowded solution space with participation from cloud, SaaS and data infrastructure vendors. All of these players and their customers have been trying to operationalize LLMs in enterprise workflows for 2+ years. Two big challenges are business ontology and fitting probabilistic tools into processes requiring deterministic outcomes. Overcoming these problems require significant systems integration and process engineering work. What does OpenAI have that makes them specifically capable of solving these problems over Azure, Databricks, Snowflake, etc., who have all been working on these problems for quite a while? I don&#x27;t know if the press release really addresses any of this, which makes it seem more like marketing copy than anything else.<p>The question of lock-in is also a major one. Why tether your workflow automation platform to your LLM vendor when that may just be a component of the platform, especially when the pace of change in LLMs specifically is so rapid in almost every conceivable way. I think you&#x27;d far rather have an LLM-vendor neutral control plane and disaggregate the lock-in risk somewhat.", "author": "louiereederson", "timestamp": "2026-02-05T16:15:16+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-02-05T17:39:10.310743+00:00", "processed": false}
{"id": "hn_comment_46900106", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46900106", "title": "Re: OpenAI Frontier...", "text": "&gt; &quot;75% of enterprise workers say AI helped them do tasks they couldn\u2019t do before.&quot;<p>&gt; &quot;At OpenAI alone, something new ships roughly every three days, and that pace is getting faster.&quot;<p>- We&#x27;re seeing all these productivity improvements and it seems as though devs&#x2F;&quot;workers&quot; are being forced to output so much more, are they now being paid proportionally for this output? Enterprise workers now have to move at the pace of their agents and manage essentially 3-4 workers at all times (we&#x27;ve seen this in dev work). Where are the salary bumps to reflect this?<p>- Why do AI companies struggle to make their products visually distinct OpenAI Frontier looks the exact same as OpenAI Codex App which looks the exact same as GPT<p>- OpenAI going for the agent management market share (Dust, n8n, crewai)", "author": "ossa-ma", "timestamp": "2026-02-05T14:38:40+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:10.362918+00:00", "processed": false}
{"id": "hn_comment_46900162", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46900162", "title": "Re: OpenAI Frontier...", "text": "I didn&#x27;t quite grasp what this is trying to solve but I hope its doing this:<p>In our company we have a list of long tail &quot;workflows&quot; or &quot;processes&quot; that really just involves reading a document and filling a form.<p>For example, how do I even get access to a new DB? Or a new AWS account?<p>Can this tool help us create an agent that can automate this with some reasonable accuracy?<p>I see OpenAI frontier as quick way to automate these long tail processes.", "author": "simianwords", "timestamp": "2026-02-05T14:43:01+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-05T17:39:10.451663+00:00", "processed": false}
{"id": "hn_comment_46899650", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46899650", "title": "Re: Show HN: ClawRouter \u2013 Open-source LLM router that ...", "text": "Hey HN, I built ClawRouter because I was spending $200+&#x2F;month on LLM API calls and realized most of my requests were simple enough for cheap models.<p>ClawRouter sits between your app and 30+ LLM providers (OpenAI, Anthropic, Google, DeepSeek, xAI). For each request, it classifies the query complexity and routes to the cheapest model that can handle it.<p>How it works:\n- 14-dimension weighted scoring (code detection, reasoning markers, length, etc.)\n- 4 tiers: SIMPLE \u2192 DeepSeek ($0.27&#x2F;M) | MEDIUM \u2192 GPT-4o-mini | COMPLEX \u2192 Claude Sonnet | REASONING \u2192 o3\n- Routing runs 100% locally in &lt;1ms \u2014 zero external API calls for routing\n- Payment via x402 USDC micropayments on Base (no API keys needed)<p>The cost savings come from the fact that ~60% of typical LLM traffic is simple Q&amp;A, summarization, or formatting that doesn&#x27;t need a frontier model.<p>Built as an OpenClaw plugin, but works standalone too. MIT licensed.<p>npm: @blockrun&#x2F;clawrouter<p>Happy to answer questions about the routing algorithm or the x402 payment approach.", "author": "vickyfu", "timestamp": "2026-02-05T13:52:58+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:11.430673+00:00", "processed": false}
{"id": "hn_story_46899362", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46899362", "title": "Show HN: Peen \u2013 A minimal coding agent CLI built for local models", "text": "I&#x27;ve been attempting to integrate locally-trained models into platforms like Claude Code and Codex for tool usage; however, they frequently encounter issues since those CLIs require XML format while my trained models predominantly operate in JSON format. When I execute a local model using these tools intending it run commands or edit files\u2014its most beneficial function\u2014it inevitably fails to perform effectively because of the discrepancy between expected formats.<p>Peen is a small Node.js CLI that works the way local models do. The model outputs one-line JSON tool calls, and the CLI executes them. It streams responses, chains tool calls, handles multi-step TODO plans, and has repair logic that nudges the model when it outputs malformed JSON. It works with Ollama and I just started adding support for other OpenAI-compatible servers like LM Studio, llama.cpp, etc.<p>The whole thing is about 800 lines across a few files. No build step, no dependencies, self-updates from GitHub on startup. It&#x27;s experimental but starting to become useful for small coding tasks with models like qwen2.5-coder:7b. And it can do it on a MacBook Air with 16GB of RAM.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;codazoda&#x2F;peen\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;codazoda&#x2F;peen</a>", "author": "codazoda", "timestamp": "2026-02-05T13:22:46+00:00", "score": 1, "num_comments": 1, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:13.523714+00:00", "processed": false}
{"id": "hn_story_46898876", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46898876", "title": "Show HN: Rehearse \u2013 a pytest like testing library for voice agents", "text": "I was manually calling my Twilio voice agent 100 times a day to verify every single micro change.<p>Tired of that, I built Rehearse.<p>I know there is a lot of YC money going into voice testing companies, but I wanted to build something open source and code first so Claude Code can spin up and manage test cases.<p>Example usage:<p>- call.listen() -&gt; get audio or transcript of what the agent is saying<p>- call.say(&quot;I&#x27;d like to book a table for 2 at midnight&quot;) -&gt; speak with the agent<p>- assertions on responses<p>It only supports Twilio (my use case) and ElevenLabs (transcription), with basic text and LLM based assertions for now.<p>It makes real calls and is BYOK.<p>I have a bunch of ideas in mind (not implemented yet, not sure if useful):<p>1. simulations like accents, background noise, languages, network issues, interruptions, etc<p>2. voice agent testing another voice agent<p>3. native audio based assertions<p>4. more connection options like Vapi, Retell, Websockets etc<p>GitHub <a href=\"https:&#x2F;&#x2F;github.com&#x2F;thenullterminator&#x2F;rehearse\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;thenullterminator&#x2F;rehearse</a><p>PyPI <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;rehearse&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;rehearse&#x2F;</a><p>Everything is a bit janky right now.<p>Appreciate all your feedback!", "author": "djp2803", "timestamp": "2026-02-05T12:24:12+00:00", "score": 1, "num_comments": 1, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:19.027029+00:00", "processed": false}
{"id": "hn_comment_46898280", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46898280", "title": "Re: Show HN: OneMinuteBranding \u2013 From prompt to brand ...", "text": "I built this because I\u2019m a vide coder, not a professional designer.<p>When I\u2019m building, the &quot;vibe&quot; and the flow are everything. But every time I start a new project, I hit the same wall: the &quot;branding tax.&quot; I\u2019d lose 2 hours on tedious tasks\u2014finding a color palette that isn&#x27;t boring, wrestling with SVG vectorizers that are full of ads, and resizing favicons.<p>It kills the creative momentum.<p>So I built OneMinuteBranding to automate the stuff that usually gets in the way of shipping. It handles the boring parts:<p>Native SVG logos: No pixelated PNGs.<p>Code-first output: Generates tailwind.config.ts and variables.css directly.<p>Full Favicon set: Ready to drop into &#x2F;public.<p>AI Context: It even exports a CLAUDE.md file so my AI tools (Cursor&#x2F;Claude) know the brand guidelines from line one.<p>I also released the internal tools I built for this (PNG to SVG and a clean Favicon generator) as free standalone tools for the community.<p>I&#x27;d love to hear how other makers handle this. Do you spend time in Figma for every side project, or do you have a way to skip the &quot;design phase&quot; to stay in the flow?", "author": "YannBuilds", "timestamp": "2026-02-05T10:53:15+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:23.615087+00:00", "processed": false}
{"id": "hn_comment_46898063", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46898063", "title": "Re: Claude says ads are coming to AI but not to Claude...", "text": "Is this a response to Sam&#x27;s decision to embed ads in OpenAI?", "author": "tizzzzz", "timestamp": "2026-02-05T10:23:56+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:25.971895+00:00", "processed": false}
{"id": "hn_comment_46898023", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46898023", "title": "Re: Show HN: Claude Code Skill for Scaffolding Arbitru...", "text": "I work in DevRel at Arbitrum. The problem I kept running into: developers want to try Stylus (Rust smart contracts that compile to WASM and run on Arbitrum alongside Solidity), but the getting-started path involves too many disconnected pieces. You need the Rust WASM target, cargo-stylus, Docker for the local devnode, knowledge of the sol_storage! macro for storage layouts, ABI export for the frontend, CORS workarounds for the devnode, and so on. Each piece is documented, but the integration path is not.<p>So I built a Claude Code skill that encodes the full workflow as structured reference documentation. A &quot;skill&quot; is a set of markdown files that Claude loads into context when relevant. The main SKILL.md file contains a compact decision tree (Stylus vs Solidity vs both), the monorepo structure, bootstrap commands, and core workflow. Six deeper reference docs cover Stylus SDK patterns, Solidity&#x2F;Foundry specifics, frontend integration (viem + wagmi), local devnode setup, deployment, and testing.<p>The key design choice is on-demand loading. Claude doesn&#x27;t load all 6 reference docs at once -- it reads the decision tree and pulls in only the ones the current conversation needs. This keeps context usage efficient while still having deep knowledge available.<p>The skill is opinionated by design. It prescribes viem (not ethers.js), pnpm (not npm&#x2F;yarn), Foundry (not Hardhat), specific SDK versions. I found that LLMs produce dramatically more consistent output when you eliminate ambiguous tool choices. The tradeoff is obvious -- if you prefer ethers.js, this skill isn&#x27;t for you.<p>What the skill doesn&#x27;t do: it doesn&#x27;t generate contracts blindly. It knows about Arbitrum-specific gotchas like block.number returning approximate L1 block numbers, the two-component fee model, Stylus contract activation requirements, and the CORS issue with the devnode. These are the things that trip up developers who copy-paste from Ethereum tutorials.<p>Tradeoffs I&#x27;m aware of: the opinionated approach means this won&#x27;t suit every workflow. The skill doesn&#x27;t support Hardhat or ethers.js users. The reference docs will need updating as the Stylus SDK evolves. And it&#x27;s only useful if you&#x27;re already using Claude Code.<p>MIT licensed. The whole thing is markdown files -- easy to fork, modify, or use as a template for skills in other domains.<p>Demo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;vsejiaOTmJA\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;vsejiaOTmJA</a>", "author": "bcgreenberg", "timestamp": "2026-02-05T10:18:34+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-05T17:39:26.060103+00:00", "processed": false}
