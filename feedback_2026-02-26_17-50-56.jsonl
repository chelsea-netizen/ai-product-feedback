{"id": "hn_story_47169242", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169242", "title": "Show HN: Duck Talk \u2013 Real-time voice interface to talk to your Claude Code", "text": "", "author": "DanyWin", "timestamp": "2026-02-26T17:35:05+00:00", "score": 5, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:50:57.740869+00:00", "processed": false}
{"id": "hn_story_47169193", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169193", "title": "Show HN: Depwire \u2013 Dependency graph and MCP tools so AI stops refactoring blind", "text": "Hey HN,<p>If you use AI coding tools, you&#x27;ve hit this: you start a new chat, ask Claude or Cursor to refactor something, and it hallucinates an import. Or renames a function but misses 3 call sites. Or suggests moving a file with no idea what depends on it.<p>The problem is simple \u2014 AI tools have no map of your codebase. Every new chat starts from zero. They burn tokens scanning files they already saw, guess at dependencies, and give you confident answers based on incomplete context. Lost context = wasted tokens + broken code + time spent fixing what AI was supposed to fix.<p>I built Depwire to solve this. It parses your codebase with tree-sitter, builds a complete dependency graph, and serves it to AI tools via MCP (Model Context Protocol). The graph persists across sessions \u2014 your AI never forgets the architecture.<p>Now when I ask &quot;what breaks if I rename Router?&quot;, I get the exact blast radius: 5 implementing classes, 2 core framework files, 14 downstream consumers, ~25 files total. Not a guess \u2014 a deterministic answer from the actual dependency graph.<p>What it does:\n- Parses TypeScript, JavaScript, Python, and Go (tree-sitter, deterministic)\n- 10 MCP tools: impact analysis, dependency tracing, architecture summaries, symbol search, and more\n- Interactive arc diagram visualization in the browser\n- File watcher keeps the graph current as you edit\n- Zero config: npm install -g depwire-cli. No databases, no cloud, no Docker\n- Everything local. No data leaves your machine.\n- Full repository context, not just single file.<p>Tested on real projects \u2014 Hono (305 files, 5,636 symbols, 2.3s), Excalidraw (320 files), FastAPI, Express, Cobra. Zero parse errors.<p>Install: npm install -g depwire-cli\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;depwire&#x2F;depwire\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;depwire&#x2F;depwire</a>\nSite: <a href=\"https:&#x2F;&#x2F;depwire.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;depwire.dev</a><p>Solo founder, scratching my own itch. BSL 1.1 (converts to Apache 2.0 in 2029). Happy to answer questions.", "author": "atefataya", "timestamp": "2026-02-26T17:31:58+00:00", "score": 2, "num_comments": 2, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:50:57.868658+00:00", "processed": false}
{"id": "hn_story_47169180", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169180", "title": "Show HN: Claude/Gemini/Codex 10-100x faster with pand\u014d (CAD for code)", "text": "Hi HN,<p>I&#x27;m George Ciobanu (<a href=\"https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;georgeciobanunyc\" rel=\"nofollow\">https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;georgeciobanunyc</a>). I built pand\u014d (&#x27;CAD for code&#x27;) because I got tired of watching AI agents burn tokens, take forever, and <i>still</i> get it wrong.<p>Here&#x27;s (one reason) why this happens: AI agents read and edit code as if it&#x27;s just text. But code is more, much more: it has structure, syntax, relationships, and meaning.<p>Code is Data<p>We built pand\u014d around this insight.<p>It indexes and persists your code&#x27;s AST in a database, which gives AI agents instant search results, and enforces syntactically correct edits. Here&#x27;s a full list of benefits that this different approach brings:<p>1. Correct Syntax - compiler-checked and gated edits (unless you force breakage)\n2. Always Safe - auto snapshots after every change, so no matter what the agent does, &#x27;Undo&#x27; is just a click away\n3. 10-100x Faster - text-based tools must 1. read context around every match they find (match number scales with <i>codebase size</i>), 2. send that context to the LLM, 3. wait for inference, and finally 4. wait for the LLM to finish streaming the response. In contrast, pand\u014d edits directly on your computer, in seconds.\n4. Token Savings - &gt;100\u00d7 &quot;token compression&quot; for some operations (e.g. rename costs the same ~40 tokens whether the symbol has one or a thousand references).\n5. Reduced Exposure - pand\u014d operations send as much or less code to the LLM provider as text tools in order to achieve a goal; some operations send no code at all to the LLM \u2014 just the intent of the transform.<p>Every edit is atomic, hash-verified, compiler-checked, and backed up by a snapshot.<p>Download and install the Visual Studio Code extension (keep VSCode with the extension installed open) or download it from the marketplace(<a href=\"https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=humansoftware.pando-extension\" rel=\"nofollow\">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=humansof...</a>), then connect from any MCP-compatible agent.<p>Supported: Typescript, Javascript, Python, C&#x2F;C++, C#.\nComing soon: Java, Rust, Go, Clojure, R, Perl, Swift, Kotlin.<p><a href=\"https:&#x2F;&#x2F;getpando.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;getpando.ai</a>\nfeedback: george [at] human [dot] software", "author": "george_ciobanu", "timestamp": "2026-02-26T17:31:19+00:00", "score": 3, "num_comments": 2, "products": ["claude", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:50:57.917746+00:00", "processed": false}
{"id": "hn_story_47169014", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169014", "title": "I built a 151k-node GraphRAG swarm that autonomously invents SDG solutions", "text": "Hi HN,\nI wanted to share a passion project I&#x27;ve been building: PROMETHEUS AGI.\nI got frustrated that most LLM&#x2F;RAG applications just summarize text. I wanted to see if an agentic swarm could actually perform cross-domain reasoning to invent new physical solutions (focusing on UN SDGs).\nThe Stack:\nNeo4j Aura (Free tier maxed out at 151k nodes &#x2F; 400k edges)\nIngestion: Google BigQuery (Patents) + OpenAlex API\nLLMs: Ollama (Llama 3) for zero-cost local entity extraction, Claude 3.5 via MCP for deep reasoning.\nUI: Streamlit (Digital Twin dashboard) + React&#x2F;Vite (Landing).\nHow it works:\nThe swarm maps problems (e.g., biofouling in water filters) to isolated technologies across different domains (e.g., materials science + nanobiology) and looks for &quot;Missing Links&quot;\u2014combinations that don&#x27;t exist in the patent database yet. So far, the pipeline has autonomously drafted 261+ concept blueprints (like Project HYDRA, a zero-power water purifier).\nWe are currently looking for domain experts (engineers, materials scientists) to validate these AI-generated blueprints and build physical prototypes, as well as grants to scale the graph to 1M+ nodes.\nDashboard: https:&#x2F;&#x2F;project-prometheus-5mqgfvovduuufpp2hypxqo.streamlit.app&#x2F;\nLanding&#x2F;Deck: https:&#x2F;&#x2F;prometheus-agi.tech\nI would love to hear your brutally honest feedback on the architecture, the Neo4j schema design, or the multi-agent approach!", "author": "wisdomagi", "timestamp": "2026-02-26T17:19:50+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:50:58.419146+00:00", "processed": false}
{"id": "hn_story_47168850", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47168850", "title": "Show HN: 20x \u2013 Open-source agent orchestrator for Linear/HubSpot tasks", "text": "Hi HN,<p>We&#x27;re the engineering team at Peakflo <i>(B2B fintech)</i>. We built 20x internally because we kept copy-pasting Linear tickets into Claude, manually setting up branches, and babysitting agent output across terminals. Eventually we just built the infrastructure to connect task systems to agents directly \u2014 and decided to open source it.<p>20x is an open-source desktop app (macOS only \u2014 Linux and Windows on the roadmap) that orchestrates AI coding agents against your existing task systems.<p>In practice: a Linear ticket gets pulled in \u2192 the triage agent assigns Claude Code + relevant skills \u2192 a git worktree is created for branch isolation \u2192 the agent writes code with live-streamed output \u2192 it pauses for your approval before pushing \u2192 a PR is opened.<p>We started with engineering tasks because current agents perform best there.<p><i>The most interesting architectural piece: Self-improving Skills</i>. Skills are reusable instruction templates attached to task types. After each task completes, the agent proposes updates based on what worked. Over time, skills accumulate institutional knowledge, like a runbook that writes itself. Each carries a confidence score that tracks reliability over time.<p>This is the main differentiator versus hosted tools like Devin or Factory. Unlike those tools, 20x is local-first, open-source, and agent-agnostic \u2014 it orchestrates your existing agents rather than replacing them.<p>A few architectural decisions:<p>- Local-first: SQLite, no accounts, no cloud sync, everything on-device\n- Agent-agnostic: supports Claude Code, OpenCode, and Codex\n- Integrations: Linear, HubSpot, Peakflo, and GitHub Issues (Peakflo is listed because we dogfood it internally \u2014  it was our first integration and how we catch regressions.)\n- Git worktrees per task for branch isolation\n- Skill updates are written back after task completion<p>MIT licensed.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;peakflo&#x2F;20x\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;peakflo&#x2F;20x</a>\nDemo: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;3zKmMz6aFek\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;3zKmMz6aFek</a><p>Happy to answer questions about the architecture, tradeoffs, or what didn&#x27;t work.", "author": "dmitryv", "timestamp": "2026-02-26T17:09:54+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:50:58.841160+00:00", "processed": false}
{"id": "hn_comment_47169399", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169399", "title": "Re: Palm OS User Interface Guidelines [pdf, 2003]...", "text": "I&#x27;m adding this to my repertoire of HIGs to study for a new desktop environment project I&#x27;m working on. I&#x27;m trying to synthesize the best parts of every computer interaction method, primarily focusing on desktops but looking at mobile designs as well.<p>There are 2 principle reasons for this project:\n1. UNIX desktops objectively suck compared to their Mac and Windows cousins, either being too complex to learn and bombarding the user with options (KDE, XFCE) or being so dumbed down and rigid to be actually usable (GNOME, to a lesser extend CDE)\n2. I&#x27;m a massive fan of the GNU project and the way it designs software and none of the current desktops integrate well with it (EG: texinfo manuals, emacs-y keybinds, A wealth of customization if you want it but otherwise easy to pick up and use)", "author": "mghackerlady", "timestamp": "2026-02-26T17:46:44+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:50:59.332796+00:00", "processed": false}
{"id": "hn_story_47168723", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47168723", "title": "Show HN: SkillFortify, a formal verification for AI agent skills", "text": "Hi HN,<p>In January 2026, 1,200 malicious skills infiltrated the OpenClaw agent marketplace\n(ClawHavoc campaign). A month later, researchers catalogued 6,487 malicious agent\ntools that VirusTotal cannot detect. The first agent-software RCE was assigned\nCVE-2026-25253.<p>The response: a dozen heuristic scanning tools (pattern matching, LLM-as-judge,\nYARA rules). They all carry the same caveat: &quot;no findings does not mean no risk.&quot;<p>SkillFortify takes a different approach. Instead of checking for known bad patterns,\nit formally verifies what a skill CAN do against what it CLAIMS to do. Five\nmathematical theorems guarantee soundness -- if SkillFortify says a skill is safe,\nit provably cannot exceed its declared capabilities.<p>What it does:\n- skillfortify scan .          -- discover and analyze all skills in a project\n- skillfortify verify skill.md -- formally verify against capability declaration\n- skillfortify lock            -- generate skill-lock.json for reproducible configs\n- skillfortify trust skill.md  -- compute trust score (provenance + behavior)\n- skillfortify sbom            -- CycloneDX 1.6 Agent Skill Bill of Materials<p>Supports Claude Code skills, MCP servers, and OpenClaw manifests.<p>Evaluated on 540 skills (270 malicious, 270 benign): F1=96.95%, zero false positives.<p>Paper: [ZENODO_DOI_URL]\nInstall: pip install skillfortify\nCode: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;skillfortify\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;varun369&#x2F;skillfortify</a><p>Built as part of the AgentAssert research suite. Happy to answer\nquestions about the formal model, threat landscape, or benchmark methodology.", "author": "varunpratap369", "timestamp": "2026-02-26T17:00:45+00:00", "score": 1, "num_comments": 2, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:50:59.382214+00:00", "processed": false}
{"id": "hn_comment_47168344", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47168344", "title": "Re: How AI skills are quietly automating my workday...", "text": "A year ago I was managing my week from a dozen different dashboards. Hubspot for tracking sales. Slack for what\u2019s happening. Notion for what we decided. PostHog for website and product analytics. Some spreadsheets for priorities. I think this is how most workdays still look today.<p>Today, I open one chat on Monday morning and ask: \u201cWhat should I focus on this week?\u201d<p>And I get a genuinely good answer. Not a generic one. A prioritized list based on what happened in Slack last weeks, what\u2019s in my knowledge base, what projects are moving. It takes about 3 minutes. Then I can continue in the same chat: \u201cgive me an overview of website traffic over the weekend,\u201d \u201chow many signups do we have in the product,\u201d \u201cdo we have any peaks or drops that require my attention.\u201d<p>This is what AI skills unlocked for me, and I want to walk through how.<p>So what are skills, actually?\nIf you\u2019ve been following the AI wave, you\u2019ve probably heard this word thrown around. It confused me at first too. Feels a lot like \u201cprompts,\u201d which have been around since ChatGPT day one.<p>The difference is meaningful though.<p>Skills are sets of instructions, commands, and context that your AI can access and use automatically when it recognizes a relevant task. You don\u2019t have to remember to feed them in every time. AI can also update and improve them on the go. It calls them when it needs them.<p>Four things make skills genuinely different from regular prompts:<p>They activate themselves. If I ask \u201cdraft an email to our investors,\u201d my AI checks if it has a skill for that and uses it. With plain prompts, I\u2019d need to remember to paste in my tone guidelines every single time. Minor inconvenience in isolation, significant friction multiplied across a week. It also brings you way closer to agentic and automated workflows.<p>They improve over time. After I run a skill, I can tell the AI: \u201clearn from what I just corrected and update the skill.\u201d So the next time it runs, it\u2019s closer to what I actually want. I use this constantly with my WordPress publishing flow as I described in my previous post. The first time took a long conversation with a lot of supervision and iterations. Now it knows exactly how I want posts formatted, which callouts and FAQs to include exactly. It just does it.<p>They turn into real capabilities. This is the big one. When you start combining skills with tools, especially ones that connect to your apps via CLI, you stop clicking through interfaces entirely. You describe outcomes. The skill handles execution.<p>They\u2019re efficient with your LLM\u2019s context. When you paste a long prompt into a chat, it takes up your entire context window upfront. That\u2019s consuming your AI\u2019s working memory which is a very critical asset right now. So skills work differently by design. The AI sees a short description of each skill at startup, and only loads the full instructions when it actually needs them. So instead of dumping everything in at once, your context stays clean for the actual work.<p>The skill I use every Monday morning\nAt the end of each week, I ask Desktop Commander to run my \u201cknowledge base update\u201d skill.<p>Press enter or click to view image in full size<p>Skills are enabled automatically based on what you prompt your LLM.\nHere\u2019s what happens: DC connects to my Slack, finds my channels, and scrapes relevant messages from the past week. Then it opens my local knowledge base, a folder of structured notes on my machine, learns how it\u2019s organized, and adds new company information to the right sections.<p>So by Monday, when I ask for my weekly priorities, the AI isn\u2019t guessing. It\u2019s pulling from an updated knowledge base that includes everything that happened at the company last week, plus any longer-term strategic context already in there.<p>The priority list it creates is surprisingly good.", "author": "rkrizanovskis", "timestamp": "2026-02-26T16:35:31+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["tone", "onboarding", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:00.690795+00:00", "processed": false}
{"id": "hn_comment_47169390", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169390", "title": "Re: The Pentagon Feuding with an AI Company Is a Bad S...", "text": "&quot;Anthropic had built its brand around promoting AI safety, emphasizing red lines it said it wouldn\u2019t cross. Its usage guidelines contain strict limitations that prohibit Claude from facilitating violence, developing or designing weapons, or conducting mass surveillance.&quot;<p>I can&#x27;t say that I fully trust this at face value, but I will say, at least at face value, that this commitment to non-violence is something I wish more tech companies in history had made. Whether it&#x27;s an authentic commitment or just PR remains to be fully seen.", "author": "yosito", "timestamp": "2026-02-26T17:46:07+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:51:01.194799+00:00", "processed": false}
{"id": "hn_story_47168064", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47168064", "title": "Show HN: Cc-pipeline \u2013 Autonomous Claude Code pipeline that builds your project", "text": "I kept doing the same thing manually: write a spec, have Claude implement it, review the code, fix issues, reflect, commit, repeat \u2014 resetting the context window by hand at each step. So I automated the loop.<p>cc-pipeline takes a BRIEF.md describing what you want built and orchestrates Claude Code through the whole SDLC: spec \u2192 research \u2192 plan \u2192 build \u2192 review \u2192 fix \u2192 reflect \u2192 commit. Phase by phase, overnight, while you sleep. Each step gets a fresh context window via the Claude Agent SDK (I tried tmux + send-keys first \u2014 do not recommend).<p>I&#x27;ve run it on a dozen projects: an Elixir port of an existing open-source gateway, a Trello-style kanban board twice (Astro and Rails), a statistical analysis in R (I&#x27;ve never written R), Tetris twice from the same brief, and a Kirby platformer attempt (that failed).<p>Full write-up of the experiments here: <a href=\"https:&#x2F;&#x2F;curiousagents.substack.com&#x2F;p&#x2F;experiments-in-building-an-automatic\" rel=\"nofollow\">https:&#x2F;&#x2F;curiousagents.substack.com&#x2F;p&#x2F;experiments-in-building...</a><p>Quick start:<p>```\ncd your-project\nnpx cc-pipeline@latest init\n```<p>Then open Claude Code and ask it to help you write the BRIEF.md. Then `npx cc-pipeline run` and walk away.<p>The steps are defined in .pipeline&#x2F;workflow.yaml \u2014 you can add, remove, or reorder them. The prompts are plain markdown. Easy to tune to your own SDLC.<p>Would love feedback on the approach, especially from anyone who&#x27;s tried similar things.", "author": "timothyjoh", "timestamp": "2026-02-26T16:16:36+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["error_messages"], "sentiment": null, "collected_at": "2026-02-26T17:51:01.786684+00:00", "processed": false}
{"id": "hn_comment_47169189", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47169189", "title": "Re: Will vibe coding end like the maker movement?...", "text": "My general take on most vibe coding projects (&quot;Hey, look, I built this over the weekend&quot;), is general dismissiveness. Mostly because of the effort required, i.e. why should I care about something that someone did with almost zero effort, a few prompts?<p>If someone tells me they ran a marathon, I&#x27;m impressed because I know that took work. If someone tells me they jogged 100 meters, I don&#x27;t care at all (unless they were previously crippled or morbidly obese etc.).<p>I think there are just a ton of none-engineers who are super hyped right now that they built something&#x2F;anything, but don&#x27;t have any internal benchmark or calibration about what is actually &quot;good&quot; or &quot;impressive&quot; when it comes to software, since they never built anything before, with AI or otherwise.<p>Even roughly a year ago, I made a 3D shooting game over an evening using Claude and never bothered sharing it because it seemed like pure slop and far too easy to brag about. Now my bar for being &quot;impressed&quot; by software is incredibly high, knowing you can few shot almost anything imaginable in a few hours.", "author": "saberience", "timestamp": "2026-02-26T17:31:40+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-02-26T17:51:02.923825+00:00", "processed": false}
{"id": "hn_comment_47168236", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47168236", "title": "Re: Why Developers Keep Choosing Claude over Every Oth...", "text": "I&#x27;ve been using ChatGPT (Thinking). I like how it has learned how I do stuff, and keeps that in mind. Yesterday, I asked it to design an API, and it referenced a file I had sent in, for a different server, days earlier, in order to figure out what to do.<p>I&#x27;m not using it in the same way that many folks do. Maybe if I get to that point, I&#x27;ll prefer Claude, but for my workflow, ChatGPT has been ideal.<p>I guess the best part, is that it seems to be the absolute best, at interpreting my requirements; including accounting for my human error.", "author": "ChrisMarshallNY", "timestamp": "2026-02-26T16:27:16+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:51:05.654031+00:00", "processed": false}
{"id": "hn_comment_47168496", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47168496", "title": "Re: Why Developers Keep Choosing Claude over Every Oth...", "text": "I use Claude for a few reasons.<p>1) I don&#x27;t want to give OpenAI my money. I don&#x27;t like how they are spending so much money to shape politics to benefit them. That seems to fly in the face of this being a public benefit. If you have to spend money like that because you&#x27;re afraid of what the public will do, what does that say?<p>2) I like how Claude just gives me straight text on one side, examples on the other, and nothing else. ChatGPT and Gemini tend to go overboard with tables, lists, emojis, etc. I can&#x27;t stand it.<p>3) A lot of technical online conversation seems to have been hollowed out in recent years. The amount of people making blog posts explaining how to use something new has basically tanked.", "author": "mrdependable", "timestamp": "2026-02-26T16:45:33+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-02-26T17:51:05.751210+00:00", "processed": false}
{"id": "hn_story_47167691", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47167691", "title": "Show HN: AgentSecrets \u2013 Zero-Knowledge Credential Proxy for AI Agents", "text": "After seeing 8,000+ MCP servers exposed this month and the OpenClaw&#x2F;ClawHavoc campaign compromise 30,000+ instances, I built a proxy that keeps credentials in the OS keychain. The agent makes authenticated API calls but never sees the key values.<p>The core insight: AI agents are users, not applications. Applications need credential values to authenticate. Agents just need to make authenticated calls. Those are different things.<p>AgentSecrets sits between the agent and the upstream API. The agent says &quot;use STRIPE_KEY&quot;. The proxy resolves the real value from the OS keychain, injects it into the request at the transport layer, and returns only the response. The key never enters agent memory.<p>Technical details:\n-Local HTTP proxy on localhost:8765 with session token (blocks rogue processes on same machine)\n-OS keychain backed \u2014 macOS Keychain, Linux Secret Service, Windows Credential Manager\n-6 injection styles: bearer, basic, custom header, query param, JSON body, form field\n-SSRF protection blocking private IPs and non-HTTPS targets\n-Redirect stripping \u2014 auth headers not forwarded on redirects\n-JSONL audit log \u2014 key names only, no value field in the struct, structurally impossible to log values\n-MCP server for Claude Desktop and Cursor\n-Native OpenClaw skill\n-Global storage mode config \u2014 set keychain-only once during init, applies everywhere<p>Honest limitations: if a malicious skill has independent network access outside AgentSecrets it can still make its own calls. This removes credentials as an attack surface specifically, not every attack surface.<p>For the specific attack that just hit 30,000 OpenClaw users \u2014 a malicious skill exfiltrating plaintext credentials \u2014 it is structurally prevented. The keys were never on the filesystem.\nMIT, open source.", "author": "steppacodes", "timestamp": "2026-02-26T15:50:32+00:00", "score": 2, "num_comments": 2, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:05.880554+00:00", "processed": false}
{"id": "hn_comment_47167466", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47167466", "title": "Re: Show HN: CodevOS \u2013 Human-AI dev OS that shipped 10...", "text": "Hey HN, I&#x27;m Waleed. CodevOS is the system I&#x27;ve been building to explore a question: what happens when you stop thinking of AI as a coding assistant and instead think of the problem of having a human-AI joint software dev team?<p>The 106 PRs in 14 days was one person \u2014 me \u2014 with AI agents doing the implementation. The article walks through the ideas that make this work:<p>- Multi-model review: Three independent AI models (Claude, Gemini, Codex) review every phase. They catch different things \u2014 Codex finds security edge cases, Claude catches runtime semantics, Gemini catches architecture problems. No single model found more than 55% of the bugs.<p>- An agent that helps you organize agents. You work with an Architect agent that spawns Builder agents that work simultaneously in isolated git worktrees. While one is implementing a feature, another is fixing a bug, and you&#x27;re reviewing a third&#x27;s PR. Your job shifts from writing code to keeping the pipeline fed.<p>- Natural language is the source code. Specs, plans, and reviews are version-controlled in git alongside the source code \u2014 treated with the same rigor as the code itself. The AI&#x27;s instructions live in the repo, not in someone&#x27;s chat history that&#x27;s already been compressed. You always know why something was built and how it was designed.<p>- Deterministic execution. Instead of asking the AI to follow a process and hoping it does, a state machine (Porch) enforces it. Human gates, build-verify loops, mandatory review phases. The AI can&#x27;t skip steps, and if it exhausts its context window, the next agent picks up from the exact checkpoint.<p>- Annotation over editing. Most of the work is writing and reviewing these natural language documents \u2014 specs that define what to build, plans that define how. The documents guide the agents. You&#x27;re directing, not coding.<p>- Whole lifecycle, git at the center. From idea through specification, planning, implementation, review, PR, and merge \u2014 the entire development lifecycle is managed. Git is the backbone: worktrees for isolation, branches for workflow, PRs for integration.<p>It&#x27;s free and open source:<p>npm install -g @cluesmith&#x2F;codev<p>(and <a href=\"https:&#x2F;&#x2F;github.com&#x2F;cluesmith&#x2F;codev\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;cluesmith&#x2F;codev</a> for the code)<p>The article includes a controlled comparison against unstructured Claude Code \u2014 honest about the tradeoffs (costs more, takes longer, but catches more bugs and ships with tests).<p><i>I&#x27;m genuinely looking for feedback on this</i>. What resonates? What doesn&#x27;t? What would you do differently? This is still early and I want to hear what the HN community thinks. Happy to answer questions too.", "author": "waleedk", "timestamp": "2026-02-26T15:33:59+00:00", "score": null, "num_comments": null, "products": ["claude", "gemini"], "categories": ["naming_terminology", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:07.482395+00:00", "processed": false}
{"id": "hn_story_47167271", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47167271", "title": "Show HN: Phone a Friend for Claude Code \u2013 GPT, Gemini, DeepSeek via MCP", "text": "I built an MCP server that gives Claude Code a &quot;phone a friend&quot; lifeline. Instead of relying on one model&#x27;s perspective, Claude can pull in GPT, Gemini, DeepSeek, or any OpenAI-compatible model for a structured multi-round debate \u2014 and participate as an active debater itself.<p>How it works:<p>You ask Claude to brainstorm a topic\nAll configured models respond in parallel (Round 1)\nClaude reads their responses and pushes back with its own take\nModels see each other&#x27;s responses and refine across rounds\nA synthesizer produces the final consolidated output\nClaude isn&#x27;t just orchestrating \u2014 it has full conversation context, so it knows what you&#x27;re working on and argues its position alongside the other models. They genuinely build on and challenge each other&#x27;s ideas.<p>A 3-round debate with 3 models costs ~$0.02-0.05. One model failing doesn&#x27;t kill the debate \u2014 results are resilient.<p>npm: npx brainstorm-mcp\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;spranab&#x2F;brainstorm-mcp\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;spranab&#x2F;brainstorm-mcp</a>\nSample debate (GPT-5.2 vs DeepSeek vs Claude): <a href=\"https:&#x2F;&#x2F;gist.github.com&#x2F;spranab&#x2F;c1770d0bfdff409c33cc9f98504318e3\" rel=\"nofollow\">https:&#x2F;&#x2F;gist.github.com&#x2F;spranab&#x2F;c1770d0bfdff409c33cc9f985043...</a><p>Free, MIT licensed. Works with any OpenAI-compatible API including local Ollama.", "author": "spranab", "timestamp": "2026-02-26T15:21:24+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:07.870780+00:00", "processed": false}
{"id": "hn_story_47167217", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47167217", "title": "Show HN: StageWright \u2013 A performance-focused Playwright reporter with AI", "text": "Hi HN,<p>I\u2019m the creator of StageWright (and the open-source playwright-smart-reporter).<p>I\u2019ve been frustrated by the &quot;black box&quot; nature of E2E test failures. Standard reporters tell you that a test failed, but they don&#x27;t help you understand why it\u2019s failing across 50 different runs or whether its execution time is trending toward a regression.<p>I built StageWright to treat test results as a performance and stability dataset.<p>Key Technical Features:<p>Historical Flakiness Detection: Unlike Playwright&#x27;s default &quot;retry&quot; logic, we track failures across runs. A test only gets a high &quot;Stability Grade&quot; if it consistently passes over time.<p>Flamechart Step Timelines: We added a color-coded flamechart for test steps (v1.0.8). It categorizes steps into Navigation, Action, and API, making it easy to see if a 10s test is hanging on a locator or a slow backend response.<p>2-Sigma Anomaly Detection: The trends view uses moving averages and 2-sigma outlier detection to flag performance regressions that might otherwise go unnoticed.<p>AI-Powered Failure Clustering: We batch failures and use Claude&#x2F;GPT-4 to cluster similar errors. Instead of 20 separate failures, you see &quot;1 cluster: TimeoutError on payment-submit-btn.&quot;<p>Virtual Scroll Performance: We optimized the UI with virtual scrolling to handle suites with 500+ tests without the browser freezing\u2014a common issue with the default HTML reporter.<p>Native Trace &amp; Network Logs: Traces and network waterfalls are embedded directly in the report. No downloading .zip files from CI; they open instantly in an inline viewer.<p>The Architecture:\nStageWright is built to be &quot;Playwright-native.&quot; It hooks into the reporter API and can run locally (outputting a standalone HTML&#x2F;JSON history) or via our new Starter&#x2F;Pro cloud tiers. The Pro tier provides a centralized dashboard for teams, long-term history retention, and cross-project analytics.<p>I\u2019m currently supporting both Node.js and Python (pytest-playwright) environments.<p>I\u2019d love to hear what the community thinks\u2014especially regarding how you handle &quot;test debt&quot; in large CI pipelines. I&#x27;m here for any questions!", "author": "qagaryparker", "timestamp": "2026-02-26T15:18:02+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["error_messages", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:08.217719+00:00", "processed": false}
{"id": "hn_story_47167192", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47167192", "title": "Show HN: How AI Content Automation Is Reshaping SaaS Marketing in 2025", "text": "Show HN: How AI Content Automation is Reshaping SaaS Marketing in 2025<p>I&#x27;ve spent 5 years building SaaS and tracking how AI revolutionizes marketing. Here&#x27;s what the data shows:<p>KEY FINDINGS:<p>- AI-integrated SaaS products grew 40% YoY (GitNux, 2026)\n- Companies using AI publish 3.2x more content than human-only teams\n- Cost per article dropped from $157 to $12-18 (AI-assisted)\n- Top-quartile SaaS allocate 65% of marketing budget to automation (up from 23% in 2022)<p>WHERE AI WORKS BEST:<p>High-Value Automation:\n- Blog + SEO (5-10x faster drafts with human editing)\n- Social media scheduling (30+ variants from one idea)\n- Email personalization (22% higher open rates)\n- Video repurposing (fully automated)<p>Still Human Touch:\n- Brand storytelling and nuanced messaging\n- Complex sales outreach\n- Community management\n- Strategic pivots<p>ARCHITECTURE:<p>Modern engine: Semi-automated ideation \u2192 AI-assisted research \u2192 LLM drafting with brand fine-tuning \u2192 Human editing \u2192 Automated scheduling \u2192 Performance analysis feeding back.<p>Key: Orchestration, not replacement.<p>CASE STUDY - 50 ARTICLES IN 7 DAYS:<p>Built custom GPT-4 fine-tuned on my style, integrated with NewsAPI&#x2F;Reddit&#x2F;HN trends, WordPress&#x2F;Twitter APIs, human review gate.<p>RESULTS:\n- 47 articles published across 3 blogs in 7 days\n- 45 min avg production time (human)\n- 12 articles hit Google first page within 3 weeks\n- $380 on AI APIs for 47 articles<p>Takeaways:\n- Automate 70%, but 30% human touch is non-negotiable\n- Bottleneck shifted from writing to <i>idea generation</i>\n- SEO favors comprehensive, data-rich AI content\n- Thin AI content gets penalized<p>TOOLS TO WATCH:<p>- NextBlog.ai: Automated blogging pipeline (research \u2192 outline \u2192 images \u2192 SEO)\n- Xbeast.io: Twitter growth with RLHF optimization\n- RedDBot.ai: Reddit automation (ethical use critical)\n- VidMachine.ai: Video repurposing (blog \u2194 video)<p>EVALUATION CHECKLIST:<p>1. Model transparency? Can I fine-tune?\n2. Output quality? Trial available?\n3. Control over brand guidelines?\n4. Content ownership?\n5. Plagiarism&#x2F;originality score?\n6. GDPR&#x2F;CCPA compliance?\n7. Scalable pricing?<p>MINIMAL STACK (&lt;$500&#x2F;mo):\n- Writing: ChatGPT Plus ($20) or Claude Pro ($20)\n- SEO: SurferSEO ($89) or Clearscope ($149)\n- Images: Midjourney ($10-30) or DALL-E API\n- Publication: Native APIs + Buffer (free)\n- Analytics: Google Analytics + Search Console (free)<p>30-DAY PLAN:\n- Week 1: Audit current content cost&#x2F;ROI\n- Week 2: Test one tool, generate 10-15 pieces\n- Week 3: Measure quality vs human, track savings\n- Week 4: Scale if within 10-15% of human performance<p>PHILOSOPHY: &quot;AI should be a force multiplier, not a replacement.&quot;<p>AI handles: routine content, drafting, optimization, amplification.\nHumans handle: brand storytelling, crisis comms, high-stakes content, genuine community building.<p>Balance this and you can produce 10x content with a fraction of the team.<p>---<p>Jack Co-Founder builds AI marketing tools:\nxbeast.io (Twitter), nextblog.ai (blogging), reddbot.ai (Reddit), vidmachine.ai (video)<p>Follow my Beehiv newsletter for weekly insights.<p>Published: 2025-02- | SaaS, AI, Marketing Automation", "author": "jackcofounder", "timestamp": "2026-02-26T15:15:53+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:08.365032+00:00", "processed": false}
{"id": "hn_story_47166431", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47166431", "title": "Show HN: Tiqd \u2013 a checklist library for life tasks", "text": "Last year I was traveling as a digital nomad with a dog and spent too much time checking the requirements for each country. Every search result was a blog post, a YouTube explainer, or an old Reddit. I just wanted a list I could check off.<p>I had the idea of building a checklist library many years ago, but found myself with the barrier of generating the content. Now that&#x27;s a solved issue.<p>I built tiqd.app. It&#x27;s a library of checklists for the stuff life throws at you, like visas, moving, travel, job interviews, home buying. Not a task manager, just find a checklist and check things off. Statically generated from Postgres at build time (Next.js 14 SSG), so every page is just HTML. No logins, no backend for user state.<p>The share feature encodes all checked IDs as lz-string in a ?p= URL param. Felt like a nicer UX than forcing a signup just to compare notes with someone.<p>I optimized the architecture and development for programmatic SEO. Vibed coded most of it with clear instructions focused on distribution. A few days old and already seeing referral traffic from ChatGPT, which I wasn&#x27;t expecting.<p>I&#x27;m also using Claude Code for the distribution side - branding, content calendar, social copy. Treating the whole launch as a learning exercise in programmatic SEO and distribution.<p>The content scales programmatically but most distribution advice out there is manual. If anyone has cracked programmatic distribution for a content site, I&#x27;d genuinely like to know what worked.", "author": "rvalley", "timestamp": "2026-02-26T14:17:55+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:11.291135+00:00", "processed": false}
{"id": "hn_story_47166396", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47166396", "title": "Show HN: Sonde \u2013 Open-source LLM analytics (track brand mentions across LLMs)", "text": "Hi HN!<p>We built Sonde (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;compiuta-origin&#x2F;sonde-analytics\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;compiuta-origin&#x2F;sonde-analytics</a>) after noticing - probably like everybody else - our website traffic was declining while prospects were telling us &quot;I found you through ChatGPT&quot;.<p>We wanted to understand our visibility across LLM responses, as we did in the good old days of SEO and web analytics. Since existing tools were enterprise-tier expensive, we built Sonde as a simple internal project to:<p>- schedule prompts to run against multiple LLMs, with web search enabled\n- track whether your brand is mentioned in responses, how it ranks vs. competitors and general sentiment\n- monitor trends over time<p>Tech stack: Supabase, Next.js, OpenRouter as LLM wrapper.<p>Sonde is fully open-source and you can self-host it via Docker Compose. We&#x27;re also offering a managed version for convenience, running with complete feature parity.<p>Sonde has fundamentally changed how we approach content strategy for our products: I&#x27;d love to get feedback on it, or hear how you&#x27;re currently tracking LLM visibility!", "author": "marcopinato", "timestamp": "2026-02-26T14:15:54+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:11.548542+00:00", "processed": false}
{"id": "hn_story_47166316", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47166316", "title": "OpenAI Codex and Figma launch seamless code-to-design experience", "text": "", "author": "JeanKage", "timestamp": "2026-02-26T14:09:28+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:51:11.825338+00:00", "processed": false}
{"id": "hn_comment_47166042", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47166042", "title": "Re: Show HN: Compression API for LLM prompts (40-60% t...", "text": "Hey HN,<p>I built AgentReady \u2014 a compression API that sits between your code and your LLM. It deterministically strips filler words, redundant connectors, duplicate lines, and boilerplate from prompts before you send them. Same meaning, fewer tokens.<p>How it works (two-step pattern):<p>Key design decisions:<p>Your LLM key never leaves your machine. AgentReady only sees the text to compress. You call OpenAI&#x2F;Anthropic&#x2F;etc. directly.\nNot a summarizer. It removes linguistic noise (filler, verbose phrasing, whitespace) while preserving all semantic content, code blocks, URLs, and numbers.\n~5ms overhead. Deterministic text transforms, no ML inference in the compression path.\n0.4% avg accuracy delta tested across GPT-4, Claude, and Gemini (BLEU&#x2F;ROUGE &lt; 2% delta).\nThree compression levels:<p>Level Savings What it does\nlight 20-30% Whitespace + boilerplate cleanup\nstandard 40-50% + filler removal, dedup, connectors\naggressive 50-60% + stop words, short-line pruning\nWhat else is included:<p>SDKs for Python (pip install agentready-sdk) and Node.js (npm install agentready-sdk)\nMCP server for Claude Desktop &#x2F; Cursor\nMonkey-patch mode: agentready.patch_openai() \u2014 zero code changes to existing apps\nChrome Extension to convert any webpage to clean Markdown (strips 90%+ of HTML noise)\nWorks with any LLM provider and frameworks (LangChain, LlamaIndex, CrewAI, Vercel AI SDK)\nFree during open beta \u2014 no limits, no credit card. I want real-world feedback before setting pricing.<p>Live demo + interactive playground: <a href=\"https:&#x2F;&#x2F;agentready.cloud&#x2F;hn\" rel=\"nofollow\">https:&#x2F;&#x2F;agentready.cloud&#x2F;hn</a><p>Happy to answer questions about the compression approach, quality benchmarks, or architecture.", "author": "christalingx", "timestamp": "2026-02-26T13:48:55+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["tone", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:13.634218+00:00", "processed": false}
{"id": "hn_comment_47165918", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47165918", "title": "Re: Anthropic is hiring more SWEs than ever, despite A...", "text": "Hi all,<p>It feels like every month a quote from Anthropic goes viral about how SWEs won&#x27;t exist in the future due to AI.<p>I wanted to see if Anthropic is actually hiring less as a result of AI. So, I compiled a dataset of their monthly SWE job openings juxtaposed with quotes from execs about AI replacement.<p>The results are clear: Anthropic is claiming that SWE jobs will go away, while simultaneously hiring more SWEs than ever. Since Jan &#x27;25 their open SWE roles are up 170% and the curve is accelerating.<p>It&#x27;s important to remember that AI companies have an incentive to claim that their tech will automate away jobs because that&#x27;s what their customers&#x2F;investors want to hear.", "author": "kylem866", "timestamp": "2026-02-26T13:37:42+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:14.186062+00:00", "processed": false}
{"id": "hn_story_47165879", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47165879", "title": "Show HN: VibeBar \u2013 macOS Menu Bar Monitor for Claude Code, Codex and OpenCode", "text": "I&#x27;ve been running multiple AI coding sessions in parallel and kept losing track of which one was thinking, which one was waiting for my input, and which one had quietly\n  finished. Alt-tabbing through terminals every 30 seconds got old fast.<p><pre><code>  VibeBar is a lightweight macOS menu bar app that solves this. It shows real-time session states (running &#x2F; awaiting_input &#x2F; idle &#x2F; stopped) for Claude Code, Codex, and\n  OpenCode without you leaving your current window.\n\n  How it works\n\n  Rather than relying on a single detection method, it uses three data channels and merges them:\n\n  1. PTY wrapper (vibebar): wraps the CLI in a pseudo-terminal and monitors output patterns to infer interaction state with high fidelity.\n  2. Plugin events (vibebar-agent): a local Unix socket server that receives lifecycle events from dedicated hooks in Claude Code and OpenCode.\n  3. Process scanner: a fallback that runs ps to detect running tool processes when the stronger channels aren&#x27;t available.\n\n  State priority when channels conflict: running &gt; awaiting_input &gt; idle &gt; stopped &gt; unknown.\n\n  Honest caveats\n\n  - Without the plugin, awaiting-input detection uses regex heuristics on terminal output \u2014 it works but isn&#x27;t perfect.\n  - Codex has no plugin event channel yet, so it relies on the PTY wrapper or process scanner.\n  - Automated test coverage is still thin.\n  - macOS 13+ only.\n\n  Built in Swift 6.2 with strict concurrency. The menu bar icon has four styles (Ring, Particles, Energy Bar, Ice Grid) and supports English, \u4e2d\u6587, \u65e5\u672c\u8a9e, \ud55c\uad6d\uc5b4.\n\n  Source and releases: https:&#x2F;&#x2F;github.com&#x2F;yelog&#x2F;VibeBar\n\n  Happy to answer questions about the PTY&#x2F;plugin architecture or the Swift implementation.</code></pre>", "author": "yelog", "timestamp": "2026-02-26T13:35:07+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:14.313428+00:00", "processed": false}
{"id": "hn_comment_47165605", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47165605", "title": "Re: Show HN: OpenTweet \u2013 I built a way to stay active ...", "text": "Hey HN, I&#x27;m Branko, solo dev behind OpenTweet.<p>I built this because I kept falling off X for weeks at a time. I knew staying active mattered for my projects, but I hated opening the app \u2014 the feed is designed to waste your time. So I built a place to write, schedule, and publish posts without ever touching the timeline.<p>The part I&#x27;m most proud of is the connector system. You hook up an RSS feed, a GitHub repo, a Stripe account, or any API endpoint, and it drafts posts from your actual activity. Push a commit? It drafts a tweet. Publish a blog post? Same thing. Hit a revenue milestone on Stripe? It catches that too. You review everything in a visual calendar and edit before anything goes out. Nothing posts without your approval unless you explicitly set up auto-publish.<p>There&#x27;s AI built in (Claude, GPT-4o, Gemini, you pick the model) for when you&#x27;re staring at a blank screen, but it&#x27;s a drafting tool, not autopilot.<p>I launched a few months ago and have 84 paying users so far. Most are indie hackers and solo founders who want to stay visible on X but would rather spend their time building. Their feedback is what shaped the connector system, turns out people don&#x27;t want AI to invent tweets, they want it to turn things they&#x27;re already doing into content.<p>I also built a free Chrome extension that shows posting patterns and engagement data for any X profile, useful even if you don&#x27;t use the rest of the product.<p>Stack is Next.js, MongoDB, Auth0, AWS S3. There&#x27;s a REST API if you want to build on top of it or let an AI agent post for you.<p>$11.99&#x2F;mo after a 7-day free trial. Solo founder, priced to keep the lights on.<p>Would love honest feedback, especially on whether the auto-posting connectors feel useful or too hands-off. That&#x27;s the thing I keep going back and forth on.", "author": "brankopetric", "timestamp": "2026-02-26T13:12:48+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "gemini"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-02-26T17:51:16.011499+00:00", "processed": false}
{"id": "hn_story_47165572", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47165572", "title": "Show HN: A bridge from Copilot SDK to ACP agents", "text": "Hi HN! I built MeshAway to connect Copilot SDK apps to ACP-compatible agents like Gemini, Codex, Opencode, etc.<p>There\u2019s been some discussion around interoperability in this space (for example: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;github&#x2F;copilot-sdk&#x2F;issues&#x2F;377\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;github&#x2F;copilot-sdk&#x2F;issues&#x2F;377</a>). In practice, integrating across agent ecosystems still often means writing custom adapters or relying on different language-specific libraries in each project.<p>MeshAway is a plug-and-play bridge that lets Copilot SDK communicate with ACP agents beyond Copilot CLI. The goal is to keep this layer minimal so developers can use different CLI agents without rewriting their stack. It also includes an optional web interface (hub) for debugging sessions and trying prompts in a playground.<p>Curious if others are running into this fragmentation problem and how you\u2019re approaching interoperability today.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;offbeatport&#x2F;meshaway\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;offbeatport&#x2F;meshaway</a><p>Would love your thoughts!", "author": "offbeatport", "timestamp": "2026-02-26T13:09:37+00:00", "score": 1, "num_comments": 0, "products": ["gemini", "copilot"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-26T17:51:16.184493+00:00", "processed": false}
{"id": "hn_story_47165464", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47165464", "title": "Show HN: BreakMyAgent \u2013 Open-source red-teaming sandbox for LLM system prompts", "text": "As a developer, I got tired of manually testing my AI agents and chatbots against the same prompt injections and jailbreaks every time I tweaked a system prompt. Our QA team was struggling with the exact same bottleneck, so I built BreakMyAgent.<p>It\u2019s an open-source sandbox that runs an automated barrage of standard exploits against your target LLM to see if it leaks data or ignores core instructions.<p>How it works under the hood:\n- The UI is built with Streamlit, backend is FastAPI, and dependency management is handled by `uv`.\n- You paste your system prompt and hit run. It fires 12 baseline attack vectors (Direct leaks, XSS payloads, Context overflows, etc.) concurrently.\n- The core mechanic is &quot;LLM-as-a-Judge&quot;. It uses a hardcoded `gpt-4.1-mini` with strict alignment rules to systematically evaluate the target&#x27;s responses.\n- It supports OpenAI, Anthropic, and a solid list of open-weight models via OpenRouter (including DeepSeek V3&#x2F;R1, Qwen 2.5, and Llama 3.3).<p>There is a hosted free version if you want to play with it immediately (I capped it at 15 requests&#x2F;IP to survive the launch), but the entire tool is open-source and takes 30 seconds to spin up locally with Docker or `uv`.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;BreakMyAgent&#x2F;breakmyagent-os\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;BreakMyAgent&#x2F;breakmyagent-os</a>\nLive demo: <a href=\"https:&#x2F;&#x2F;breakmyagent.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;breakmyagent.dev</a><p>Next on the roadmap: I&#x27;m building a dedicated CLI&#x2F;GitHub Action so teams can drop this into their own CI&#x2F;CD pipelines to block prompt regressions. I&#x27;m also developing a PoC for multi-turn agentic fuzzing and expanding the payload database for complex tool-spoofing.<p>I\u2019d love to hear your feedback! What other test configurations (besides temperature and response format) do you think are essential for a tool like this? Also open to any feedback on the architecture, the judge prompt, or specific zero-day vectors you&#x27;d like to see included in the public database.", "author": "breakmyagent", "timestamp": "2026-02-26T13:00:17+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:16.437859+00:00", "processed": false}
{"id": "hn_story_47165439", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47165439", "title": "Show HN: Cowork Context Kit \u2013 tiered context management for Claude's file access", "text": "I was running Claude Desktop (Cowork) on a 462 file project folder. Claude reads every file at the start of each session, which burns context window on outdated and irrelevant docs. Output quality was degrading noticeably. Built a near-line tiering system: a manifest template per folder, global instructions for tiered access, and a Cowork skill for consistency. ~10 min setup. MIT licensed.", "author": "Hughtopian", "timestamp": "2026-02-26T12:58:05+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:16.561967+00:00", "processed": false}
{"id": "hn_story_47164969", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47164969", "title": "Claude Code Bug triggers Rate limits without usage", "text": "Starting an hour ago, i received the following message &quot;API Error: Rate limit reached&quot; in claude code on a 5x Max subscription.<p>I had not used the model extensively, but accepted it. I waited 10min and asked again on how to go about a localization task on a website. Nothing code intensive, just a Pointer on what path to take given the infrastructure. However the same error message.\nI checked claude status, i checked HN and started the support bot, i reviewed the API ratelimits. But all seemed normal. And nowhere did it seem like i exceeded. I waited another 30min and tried again. \nThe error message persists, according tot the doc, it should tell me how long to wait. it doesnt.<p>Anyone else experiencing this?\nBased in Switzerland, Europe, on Linux", "author": "busssard", "timestamp": "2026-02-26T12:05:38+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["error_messages"], "sentiment": null, "collected_at": "2026-02-26T17:51:19.761581+00:00", "processed": false}
{"id": "hn_story_47164842", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47164842", "title": "Show HN: I built this toolbox with AI \u2013 never wrote a line myself", "text": "Hey HN! I work at a game company and after staring at code\nall day, I didn&#x27;t want to write more at night.<p>So I used Claude and Cursor to build this \u2014 architecture, design, infra, CI&#x2F;CD. I just directed and reviewed. Took a few weekends.<p>It&#x27;s a collection of tools I personally Google all the time:\n  JSON formatter, image resizer, timestamp&#x2F;timezone converters,\n  UUID generator, QR code, and ~30 more.<p>Happy to answer questions about the AI workflow or anything else.", "author": "harrykoreanlee", "timestamp": "2026-02-26T11:52:19+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-26T17:51:20.518822+00:00", "processed": false}
