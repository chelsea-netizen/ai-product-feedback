{"id": "hn_comment_47090522", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47090522", "title": "Re: Beware Project-Wrecking GitHub Copilot Premium SKU...", "text": "n the middle of an AI project, I unknowingly blew through my Copilot Pro quota and got auto\u2011downgraded. Output tanked, panic ensued, and my chat buddy and I had words. Moral: always read the fine print.", "author": "daram", "timestamp": "2026-02-20T16:56:03+00:00", "score": null, "num_comments": null, "products": ["copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:15.184582+00:00", "processed": false}
{"id": "hn_story_47090181", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47090181", "title": "Show HN: Prothon \u2013 docs-first Python project generator for AI development", "text": "Prothon is a Python project generator that scaffolds a uv-based project with eight quality tools (ruff, ty, pytest, hypothesis, mutmut, bandit, vulture, complexipy) and a documentation-driven workflow for AI coding agents.<p>The problem it solves: AI assistants lose context between sessions and drift from your decisions as context windows fill up. Prothon addresses this with three ideas:<p>1. A three-level doc hierarchy (SPEC, DESIGN, PATTERNS) where each level scopes a single concern and higher always overrides lower. The AI implements from these docs, not from memory. Each doc-writing step is a dedicated agent that refuses content belonging at another level.<p>2. Automated agents that keep things consistent: one cross-checks the three docs for contradictions, one generates reference skills from live documentation for your stack, one verifies code implements what the docs describe.<p>3. A &quot;promise&quot; system for execution. Before each task, the agent declares what files it will touch, success criteria, and expected line counts. The promise is verified against the actual git diff. This makes context drift measurable rather than something you notice after the fact.<p>CLI:<p>```\nprothon new          # scaffold from template\nprothon init         # add to existing project\nprothon spec         # requirements Q&amp;A\nprothon design       # architecture Q&amp;A\nprothon patterns     # conventions Q&amp;A\nprothon execute      # plan, promise, build\nprothon compliance   # verify code vs docs\n```<p>Built on copier, so existing projects can pull upstream template improvements. Currently targets Claude Code, designed to be agent-agnostic.<p><a href=\"https:&#x2F;&#x2F;prothon.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;prothon.dev</a>\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;jackedney&#x2F;prothon\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jackedney&#x2F;prothon</a>", "author": "jackedney", "timestamp": "2026-02-20T16:31:00+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:15.821158+00:00", "processed": false}
{"id": "hn_comment_47090635", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47090635", "title": "Re: No Skill. No Taste...", "text": "Application design is still a challenge. I had Monday off and vibe-coded up an app that I&#x27;ve been wanting to use for years. The thing is, I can tell it&#x27;s going to be challenging to make it something sticky that I actually use.<p>Which makes sense. The reason I wanted to make this app is that there are two very popular paid apps in the same category that I use every day that don&#x27;t quite feel the way I want them to. It&#x27;ll be easy to fix the little annoyances and missing features, but there&#x27;s a feeling that&#x27;s missing from them as well. I don&#x27;t think it&#x27;s wrong to say that I&#x27;m put off by a lack of taste, at least according to my taste. I don&#x27;t know if I can do better, but I&#x27;m looking forward to trying, and I love that Claude makes me fast enough that the project has finally tipped from &quot;I&#x27;d love to tackle this, but I know it&#x27;s too big for me&quot; (which is what I&#x27;ve been thinking for the last 5-10 years) to &quot;I can make a credible attempt at this.&quot;", "author": "dkarl", "timestamp": "2026-02-20T17:03:47+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-20T17:29:16.492568+00:00", "processed": false}
{"id": "hn_story_47089757", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47089757", "title": "Show HN: Expectllm \u2013 \"expect\"-style pattern matching for LLM conversations", "text": "I&#x27;ve been experimenting with agent frameworks and noticed that many workflows reduce to a simple pattern:<p>- Send input\n- Wait for a pattern\n- Branch on the match<p>This is essentially the classic Unix expect model, but applied to LLM conversations.<p>So I built expectllm \u2014 a minimal pattern-matching conversation flow library (365 lines of code).<p>Example:<p><pre><code>    from expectllm import Conversation\n\n    c = Conversation()\n    c.send(&quot;Review this code for security issues&quot;)\n    c.expect(r&quot;found (\\d+) issues&quot;)\n\n    if int(c.match.group(1)) &gt; 0:\n        c.send(&quot;Fix the top 3&quot;)\n</code></pre>\nNo chains, no schema definitions, no output parsers.<p>Features:\n- expect_json(), expect_number(), expect_yesno()\n- Regex \u2192 auto format instructions\n- Full conversation history for multi-turn flows\n- Auto-detects OpenAI &#x2F; Anthropic via environment variables<p>The idea: treat LLM conversations as explicit state machines, where each expect() is a state transition.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;entropyvector&#x2F;expectllm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;entropyvector&#x2F;expectllm</a>\nPyPI: pip install expectllm<p>Would love feedback \u2014 especially on where this abstraction breaks down.", "author": "entropyvector", "timestamp": "2026-02-20T16:03:52+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:16.680375+00:00", "processed": false}
{"id": "hn_comment_47089434", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47089434", "title": "Re: Show HN: My 7 year old makes games with AI, I made...", "text": "My 7-year-old uses Claude on his iPad to make games. He can barely read but uses voice to describe what he wants. He can read enough to make text edits when voice transcription gets it wrong. It&#x27;s been pretty cool to see where his imagination takes him, and I wanted a way for him to be able to easily publish and share games he (and others) make, so I made www.kidhubb.com.<p>Paste HTML, get a live game URL. No accounts (just creator codes), no build tools, single HTML files. Every game&#x27;s source is viewable and remixable.<p>I designed the site so AI assistants are first-class visitors. There&#x27;s a www.kidhubb.com&#x2F;for-ai page that acts as a living briefing for any AI that visits, along with hidden context blocks on every page. The idea is that a kid&#x27;s AI should be able to understand the platform just by visiting it, and be able to help them get it published. Try it yourself - just ask your AI to &quot;help me publish a game on <a href=\"https:&#x2F;&#x2F;www.kidhubb.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.kidhubb.com</a>&quot;.<p>Note: AI needs the full url initially so it can actually visit the site and from there it can follow instructions to help you&#x2F; your kid publish. It&#x27;s a new site so just saying &quot;kidhubb&quot; without the full url won&#x27;t work.<p>Github repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mlapeter&#x2F;kidhubb\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mlapeter&#x2F;kidhubb</a><p>My kid&#x27;s first game: <a href=\"https:&#x2F;&#x2F;www.kidhubb.com&#x2F;play&#x2F;meteor-dodge-solarscout64\" rel=\"nofollow\">https:&#x2F;&#x2F;www.kidhubb.com&#x2F;play&#x2F;meteor-dodge-solarscout64</a>", "author": "mlapeter", "timestamp": "2026-02-20T15:42:54+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-20T17:29:17.512641+00:00", "processed": false}
{"id": "hn_comment_47089103", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47089103", "title": "Re: Cothought: A Markdown zettelkasten journal for you...", "text": "A rash of these second mind type tools have emerged, but they all seem overly focused (business or whatever), or performative.<p>This one started from me just typing stuff into Claude Code, then adding more ideas and skills &#x2F; formatting changes as I went until it grew into something useful enough I started telling other people about what it&#x27;s done for me. Then they asked me to share it so I packaged it up, polished it a bit, and here we are! There&#x27;s no other editor, I don&#x27;t use Obsidian or anything with it (although you definitely could). I just find the simplicity of one interface to be really freeing and helps me focus on actually journaling instead of maintaining a card &#x2F; link system.<p>It helps me think and reflect across time in a way that my meat mind seems to be not as good at. It&#x27;s just so fast at searching through things and making associations even with the default Opus 4.6 somewhat limited context. I&#x27;ve ended up capturing and working on so many more of my ideas and I&#x27;m so much more inspired on a day to day basis. I hope you enjoy it as much as I do.<p>It&#x27;s all on GitHub here, and obviously free and open source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;elliotbonneville&#x2F;claude-cothought\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;elliotbonneville&#x2F;claude-cothought</a>", "author": "elliotbnvl", "timestamp": "2026-02-20T15:20:18+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-20T17:29:18.196869+00:00", "processed": false}
{"id": "hn_story_47089055", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47089055", "title": "Show HN: OpenGUI \u2013 Desktop GUI for OpenCode with prompt queuing", "text": "Hey HN. I built a desktop GUI for OpenCode (opencode.ai), the open-source AI coding assistant. Multitasking and orchestration is complicated in the TUI. I wanted to queue prompts while the AI is busy, fork conversations to try different approaches, and revert entire exchanges when things go sideways. None of that fits well in a terminal. OpenGUI is an Electron + React app that wraps OpenCode and adds those features along with multi-project support and live context usage tracking. Built with Bun as the full toolchain. The whole thing was built in about 4 days using Claude and ChatGPT. It&#x27;s at v0.1.10, works on Linux and macOS, and I use it daily. There are definitely bugs and rough spots. MIT licensed. Would appreciate feedback on the feature set and anything you&#x27;d want from a desktop AI coding tool.", "author": "akemmanuel", "timestamp": "2026-02-20T15:17:29+00:00", "score": 4, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-20T17:29:19.378698+00:00", "processed": false}
{"id": "hn_story_47088813", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47088813", "title": "Show HN: Tropes.fyi \u2013 Name and shame AI writing", "text": "Today I come to you on this beautiful Friday with a freshly hardthink-ed solution to a proliferous problem plaguing our world: the loss of original voice. The blanket of blandness slowly suffocating centuries of writing.<p>Or to put it bluntly: AI writing is trash.<p>It is disrespectful to expect ME to read something YOU could not even be bothered to write (or likely even read).\nThe lingering human connection that remained resilient on the internet - through\nyears of SEO optimisation, propaganda bots, ads, regional firewalls, politics, censorship - is now being diluted even further.<p>Many of the posts I eagerly click on HN (especially sorting by new) are now disappointingly, ragingly, completely AI generated.\nThis includes several that reach the front page on a daily basis. It&#x27;s shameless.\nUnfortunately, many of you educated readers are oblivious to the tropes and tells or too eager to discuss the topic to notice.<p>To combat this (and because I&#x27;m petty) I (and ironically Claude) built a directory and several tools to name and shame these patterns. Both to educate those are not AI-literate and to spread awareness. NOT TO HARASS.<p>tropes.fyi is a catalogue of the recurring tropes, tics, structures and word choices that give away AI-generated writing. Each trope is documented with a name, description, real-world examples and the category it falls under (word choice, sentence structure, tone, formatting, composition).<p>There are currently 32 tropes. Some highlights:<p>- Negative Parallelism: &quot;It&#x27;s not X \u2014 it&#x27;s Y.&quot; The SINGLE MOST COMMON AI writing tell. One per piece is fine. Ten in a blog post is insulting. AND I HAVE SEEN TEN IN A BLOG POST.<p>- Tricolon Abuse: The compulsive rule of three. &quot;Products impress people; platforms empower them. Products solve problems; platforms create worlds.&quot;<p>- Em-Dash Addiction: A beautiful punctuation. Once the sign of a brilliant writer now too many of these is a red flag. I still use them, you won&#x27;t break my spirit.<p>- &quot;Delve&quot; and Friends: Perhaps the most infamous. &quot;Delve&quot; went from uncommon to appearing in a staggering percentage of AI text. See also: &quot;leverage&quot;, &quot;robust&quot;, &quot;harness&quot;.<p>full directory: <a href=\"https:&#x2F;&#x2F;tropes.fyi&#x2F;directory\" rel=\"nofollow\">https:&#x2F;&#x2F;tropes.fyi&#x2F;directory</a><p>ai;dr - Paste a link to any article. It will strip the AI fluff using our trope detection rules, then feed the skeleton to a lightweight llm which reverse-engineers it back to the prompt they likely* typed :D.\n3000 words of slop \u2192 30-word prompt + whatever unique insights (if any) survived the bloat. try it: <a href=\"https:&#x2F;&#x2F;tropes.fyi&#x2F;aidr\" rel=\"nofollow\">https:&#x2F;&#x2F;tropes.fyi&#x2F;aidr</a><p>AI Vetter - Paste a URL, get a verdict. The AI Vetter fetches article content, runs rule-based pattern matching against all 32 tropes and returns a score on the scale: Human \u2192 AI-assisted \u2192 Suspicious \u2192 Barely Legible \u2192 Pure AI Slop. try it: <a href=\"https:&#x2F;&#x2F;tropes.fyi&#x2F;vetter\" rel=\"nofollow\">https:&#x2F;&#x2F;tropes.fyi&#x2F;vetter</a><p>tropes.md -  A single file you can drop into any AI assistant&#x27;s system prompt so it avoids these patterns. Sorry for helping them become invisible, I&#x27;d just rather not see it.<p>&quot;You&#x27;re a hypocrite I&#x27;m sure you use AI for writing code!&quot; \u2014 AI generated code is fine, it&#x27;s one computer (LLM) talking to another (compiler). How else do you think AI is so good at writing code? They&#x27;re speaking the same language! But prose? That&#x27;s from human to human, it&#x27;s sacred and meant for other people. Using AI for that is deceitful.", "author": "ossa-ma", "timestamp": "2026-02-20T14:58:25+00:00", "score": 3, "num_comments": 2, "products": ["claude"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-02-20T17:29:20.032332+00:00", "processed": false}
{"id": "hn_story_47088761", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47088761", "title": "Show HN: OSS Durable Memory for LLMs", "text": "Today we\u2019re open-sourcing the core memory engine behind Mnexium.com : CORE-MNX<p>GItHub (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;mnexium&#x2F;core-mnx\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mnexium&#x2F;core-mnx</a>)\nNPM (<a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@mnexium&#x2F;core\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@mnexium&#x2F;core</a>)<p>For us, this is a product decision and a philosophy decision.<p>Memory infrastructure is becoming foundational for serious AI products, and we believe the core layer should be transparent, inspectable, and extensible by the teams building on top of it. We also just want feedback - we want to build the best memory system given the tools we have access to today. We also want to make LLMs perform better then they already do OOTB.<p>CORE-MNX is the backend layer that powers durable memory workflows:\n    memory storage and retrieval,\n    claim extraction and truth-state resolution,\n    memory lifecycle management, and event streaming for real-time systems.\n    It\u2019s Postgres-backed, API-first, and built to integrate into real production stacks.<p>We tried our best to make this system as standalone as possible. Ultimately, its fairly difficult we needed LLMs (Cerebras for fast token output, ChatGPT for intelligence etc), Databases for storage etc. We have intentionally made the project API interfaced so your project can be code agnostic.<p>Open-sourcing CORE lets builders:\n    understand exactly how memory behavior works,\n    self-host or extend the engine for their own products,\n    and avoid reinventing the same memory infrastructure from scratch.<p>What stays on Mnexium.com\nMnexium\u2019s long-term direction is still the same: make AI systems more useful over time through durable memory and reliable recall. We&#x27;ve just figured out that hosting memory isnt the moat we once thought it was - the real moat we believe is making the LLM system(s) as easy to use as possible. The feature-set we&#x27;ve built around memory is what is differentiating.<p>Open-sourcing CORE is how we make that foundation available to everyone building in this space. Open to everyone to lend an opinion on improvements and how we make this problem solvable.<p>Would love feedback, opinions and bugs you may find. We release it isn&#x27;t perfect, but certainly a good start we&#x27;d love to improve upon.", "author": "Mnexium", "timestamp": "2026-02-20T14:53:57+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:20.128937+00:00", "processed": false}
{"id": "hn_comment_47088420", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47088420", "title": "Re: Show HN: tnnl - Self-hosted ngrok alternative with...", "text": "Built this because ngrok&#x27;s free tier got too restrictive and the alternatives either don&#x27;t do HTTP subdomain routing or need 50 lines of TOML to get started.<p>tnnl is two modes in one binary - run `tnnl server` on a VPS, then `tnnl http 3000` on your machine. You get a public HTTPS subdomain instantly. No account, no signup, no interstitial page.<p>Pass --inspect to see full request&#x2F;response headers and body in the terminal. Every request is saved so you can `tnnl replay 3` after fixing your code. Self-host it on a $4 VPS or use the free public server.<p>Public server at tnnl.run if you just want to try it:<p><pre><code>    curl -fsSL https:&#x2F;&#x2F;tnnl.run&#x2F;install.sh | sh\n    tnnl http 3000</code></pre>", "author": "jbingen", "timestamp": "2026-02-20T14:25:51+00:00", "score": null, "num_comments": null, "products": ["grok"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:21.111947+00:00", "processed": false}
{"id": "hn_comment_47087747", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47087747", "title": "Re: Show HN: InkSight \u2013 An open-source, LLM-powered e-...", "text": "Hi HN,<p>I\u2019m sharing an open-source project out of our lab called InkSight (code-named inco). Like many of us, our team found ourselves constantly distracted by notifications, glowing monitors, and endless doomscrolling on our phones. We wanted a way to consume high-quality, low-frequency information (like a Stoic quote, a minimalist daily briefing, or a quick recipe) without the cognitive load of a traditional screen.<p>So, we built InkSight\u2014an open-source &quot;slow tech&quot; desktop companion. It uses an ESP32-C3 and an e-ink display to fetch customized LLM-generated content.<p>The Tech Stack &amp; Architecture:<p>Hardware: ESP32-C3 written in C&#x2F;C++ (Arduino framework). It supports common 2.13&quot; &#x2F; 1.54&quot; e-ink panels.<p>Backend: Python &amp; FastAPI. It acts as the brain, parsing user-defined JSON prompt templates and calling any OpenAI-compatible LLM (OpenAI, DeepSeek, Kimi, etc.).<p>Web Dashboard: Pure HTML&#x2F;JS&#x2F;CSS for easy configuration without digging into the code.<p>What we think HN might find interesting (The Optimizations):\nInstead of the ESP32 waking up, making an API call to the LLM, and waiting 5-10 seconds for a response (which drains the LiPo battery significantly), we built a smart caching layer in the backend.\nThe backend pre-generates and caches the content. When the ESP32 wakes up, it fetches the cached payload in sub-seconds, updates the e-ink screen, and immediately goes back to deep sleep. This allows it to run for 3 to 6 months on a single charge and stay resilient against temporary network drops.<p>Deployment:\nWe wanted to make it accessible to everyone, so the backend can be 1-click deployed to Vercel (for free), and the hardware uses a Captive Portal for zero-code WiFi and API configuration. Of course, it\u2019s completely self-hostable if you prefer to run it locally.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;datascale-ai&#x2F;inksight\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;datascale-ai&#x2F;inksight</a><p>I\u2019d love to hear your thoughts on the &quot;slow tech&quot; philosophy, any feedback on the architecture, or ideas for new content templates! I&#x27;ll be hanging around the thread to answer any questions.", "author": "xx123122", "timestamp": "2026-02-20T13:23:56+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:22.981247+00:00", "processed": false}
{"id": "hn_comment_47086992", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47086992", "title": "Re: Show HN: Geo-lint \u2013 open-source linter for GEO (AI...", "text": "I run multiple content-heavy sites and got tired of one thing: there was no\ndeterministic way to validate with the AI agent whether my content was actually optimized. Not\n&quot;probably fine&quot; \u2014 actually checked against concrete rules, both for traditional\nsearch and for AI search engines.<p>SEO linters exist, but they&#x27;re either paid SaaS, not automatable, or ignore\nthe structural patterns that LLMs use when deciding what to cite. So I built\none.<p>geo-lint is a CLI with 92 rules: 32 SEO (table stakes \u2014 titles, descriptions,\nheadings, slugs, canonical URLs, schema, the works), 35 GEO rules specifically\nfor AI citation readiness, 14 content quality checks inspired by Yoast, and\nthe rest for technical and i18n validation. We extensively researched the\ncurrent state of GEO and AEO to make sure the rules reflect what actually\ngets content cited by ChatGPT, Perplexity, and Google AI Overviews \u2014 not\noutdated advice.<p>The design principle: the linter is deterministic, the AI agent is creative.\nSame content in, same violations out, every time. The agent does the fixing.<p>In practice, I install it in a project, paste one prompt into Claude Code or\nCursor:<p><pre><code>    Run npx geo-lint --format=json, fix every violation using the suggestion\n    field, re-run until the output is [].\n</code></pre>\nAnd walk away. The agent reads the JSON violations, edits the files, re-lints,\nloops \u2014 no manual input. When it finishes, the content is validated for both\ntraditional search ranking and AI citation. One command, both outcomes, across\nan entire site.<p>Every rule outputs a machine-readable suggestion field that tells the agent\nexactly what to change. The JSON has no formatting, no ANSI colors \u2014 pure\nstructured data. Works with Markdown&#x2F;MDX out of the box, extensible to Astro,\nHTML, or any CMS via custom adapters.<p>This is extracted from the toolchain I use on production client content at my\nagency. MIT licensed, zero peer dependencies, TypeScript.<p>Happy to answer questions about GEO patterns, what makes content citable by\nLLMs, or how the agentic loop works in practice.", "author": "ijonis", "timestamp": "2026-02-20T12:06:27+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "perplexity"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:25.083820+00:00", "processed": false}
{"id": "hn_story_47086913", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47086913", "title": "Show HN: OkaiDokai, tool-level firewall for OpenClaw, Claude Code and Codex", "text": "Hey HN, Sascha here, developer of OkaiDokai. Like many others, I fell in love with OpenClaw, but was longing for more control over what it can do without interrupting its autonomous, agentic nature. OkaiDokai solves this for me by allowing me to set up my own rule set of what is allowed by default, what is not, and what it should ask permission for. It comes with a hosted API, web and native apps including push notifications (with quick-response support), plugins for OpenClaw and Claude Code, and soon Codex.<p>I&#x27;m still working on getting the apps onto the App Store (currently on TestFlight and Android beta channels), as well as giving the rule engine and UI a bit more love. It should be good enough for a quick test if anyone dares to try it!<p>I just created a new Discord server (<a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;M25cCwJ5x7\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;M25cCwJ5x7</a>), so please say &quot;hello&quot; if you\u2019d like to give it a go and provide me with any feedback you have. I&#x27;m adding new features and improvements pretty much around the clock right now. I&#x27;m also planning to release the source code under the Sustainable Use License soon, as I think it&#x27;s important to get more eyes on it and allow others to contribute should they want to.<p>Thanks for reading :)", "author": "cedel2k1", "timestamp": "2026-02-20T11:57:13+00:00", "score": 11, "num_comments": 0, "products": ["claude"], "categories": ["naming_terminology", "response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:25.230509+00:00", "processed": false}
{"id": "hn_story_47086383", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47086383", "title": "Show HN: Legal RAG Bench", "text": "Hey HN,\nThis is Legal RAG Bench, the first benchmark for legal RAG systems to simultaneously evaluate hallucinations, retrieval failures, and reasoning errors.<p>The key takeaways of our benchmark are:\n 1. Embedding models, not generative models, are the primary driver of RAG accuracy. Switching from a general-purpose embedder like OpenAI&#x27;s Text Embedding 3 Large to a legal domain embedder like  Kanon 2 Embedder can raise accuracy by ~19 points.\n 2. Hallucinations are often triggered by retrieval failures. Fix your retrieval stack, and, in most cases, you end up fixing hallucinations.\n 3. Once you have a solid legal retrieval engine, it doesn\u2019t matter as much what generative model you use; GPT-5.2 and Gemini 3.1 Pro perform relatively similarly, with Gemini 3.1 Pro achieving slightly better accuracy at the cost of more hallucinations.\n 4. Google&#x27;s latest LLM, Gemini 3.1 Pro, is actually a bit worse than its predecessor at legal RAG, achieving 79.3% accuracy instead of 80.3%.<p>These findings confirm what we already suspected, that information retrieval sets the ceiling on the accuracy of legal RAG systems. It doesn\u2019t matter how smart you are, you aren\u2019t going to magically know what the penalty is for speeding in California without access to an up-to-date copy of the California Vehicle Code.<p>Even still, to our knowledge, we\u2019re the first to actually show this empirically.<p>Unfortunately, as we highlight in our write-up, high-quality open legal benchmarks like Legal RAG Bench and our earlier Massive Legal Embedding Benchmark (MLEB) are few and far between.<p>We point out, for example, that the popular Vals AI CaseLaw (v2) benchmark yields LLM rankings inexplicably and starkly different from ours while also failing to properly evaluate end-to-end RAG performance. Because CaseLaw (v2) is a private and proprietary benchmark, we are unable to confirm the source of the discrepancies we discovered, though we suspect they lie in a seriously flawed evaluation and labeling methodology.<p>In the interests of transparency, we have not only detailed exactly how we built Legal RAG Bench, but we\u2019ve also released all of our data openly on Hugging Face here: <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;isaacus&#x2F;legal-rag-bench&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;isaacus&#x2F;legal-rag-bench&#x2F;</a>. We will also soon be publishing our write up as a paper.", "author": "beowa", "timestamp": "2026-02-20T10:58:12+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:26.452629+00:00", "processed": false}
{"id": "hn_comment_47086241", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47086241", "title": "Re: Show HN: VaultAI \u2013 42 AI models on a portable SSD,...", "text": "Hey HN \u2014 maker here.<p>Quick context on why I built this: I was personally spending ~$1,200&#x2F;month on Claude API (I use it for everything \u2014 coding, writing, analysis). That\u2019s $14,400&#x2F;year. Even if I dropped to a single $20&#x2F;month sub, every query I send tells Anthropic what I\u2019m working on, what I\u2019m thinking about, what problems I have.<p>I wanted Claude-quality inference without the cloud dependency. That\u2019s not possible yet for large models, but what IS possible locally is surprisingly good.<p>What\u2019s actually on it:<p>- 15 chat models \u2014 Qwen3-235B-A22B (MoE, fast on consumer hardware), LLaMA 3.3 70B Q4, DeepSeek-R1 32B, Gemma 3 27B, Phi-4\n- 14 vision models \u2014 drag in an image, get analysis locally. No upload to any server.\n- 5 coding assistants \u2014 Qwen2.5-Coder 32B is genuinely impressive for local inference\n- 3 image generators via ComfyUI \u2014 FLUX.1-schnell, Z-Image-Turbo, Qwen-Image. This is the part I haven\u2019t seen anyone else ship as plug-and-play. Most local image gen setups require 30 min of dependency hell.\n- RAG&#x2F;knowledgebase via Qdrant \u2014 drop in PDFs, docs, notes; semantic search across them locally\n- Medical AI (MedGemma), uncensored models (Dolphin, Nous-Hermes, Abliterated variants)<p>The target user: Anyone paying $200+&#x2F;month across Claude, ChatGPT, Midjourney, GitHub Copilot, Perplexity. That\u2019s $2,400&#x2F;year with zero privacy. The Core tier ($399) pays for itself in 2 months if you cancel just one sub.<p>Stack: Tauri + Rust + React frontend. Ollama for inference. Qdrant for vector search. ComfyUI for image gen. Everything runs from the SSD \u2014 unplug and zero trace on the host machine.<p>Why not just software? Three reasons:\n1. Downloading all of this takes 6-8 hours and 300+ GB \u2014 we pre-load\n2. It\u2019s portable between machines (home Mac, work PC, travel laptop)\n3. Zero install, zero config \u2014 double-click and it works<p>Honest limitations:\n- 8GB RAM minimum, 16GB recommended for the larger models\n- Not a GPU box \u2014 these are quantized models optimized for CPU&#x2F;unified memory\n- Not a replacement for frontier models (GPT-4, Claude Opus) for the hardest tasks\n- First batch is 25 units. This is validation.<p>Tiers:\n- Core: 20 models, 256GB \u2014 $399\n- Pro: 34 models, 512GB \u2014 $599\n- Ultra: 42 models, 1TB \u2014 $899<p>Happy to answer architecture questions, model quantization tradeoffs, or anything else.", "author": "laramie_co", "timestamp": "2026-02-20T10:41:28+00:00", "score": null, "num_comments": null, "products": ["claude", "chatgpt", "copilot", "perplexity"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-20T17:29:26.861997+00:00", "processed": false}
{"id": "hn_comment_47086059", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=47086059", "title": "Re: Show HN: Behavr \u2013 Run realistic user simulations o...", "text": "Hi HN, I built Behavr - <a href=\"https:&#x2F;&#x2F;behavr.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;behavr.ai&#x2F;</a> - it runs AI agents through your Figma prototypes to find UX issues in minutes, providing quantitative and qualitative UX insights.<p>The Problem: \nMost product teams either skip UX research entirely or spend weeks (even months) testing. This is expensive and time consuming. Behavr offers UX insights in minutes, allowing you to iterate and test as fast as you design.<p>How it works: \nPaste your Figma URL, Behavr spawns AI users with diverse personas, they navigate your prototype like real users, you get UX insights in under 3 minutes.<p>What makes the AI users realistic:\nEach agent gets a profile, motivation (focused completer, casual browser, skeptical evaluator), tech literacy, age, and gender \u2014 these all affect patience and behaviour\nEffort budgets calibrated to task complexity. A casual browser bails on a 7-screen signup flow, a focused completer won&#x27;t\nGrounded in UX research: F-pattern scanning, choice overload, Fitts&#x27;s Law, decision fatigue. Behavioural priors drawn from published studies by Nielsen Norman Group, Baymard Institute, and Google UX Research\nIssues mapped to Nielsen&#x27;s 10 usability heuristics<p>Under the hood:\nFigma API extracts screens and interactive element maps\nClaude Sonnet handles vision analysis and agent simulation\nParallel execution via Celery\nOutlier detection and hallucination checking before synthesis<p>Stack: \nNext.js, FastAPI, Celery + Redis, PostgreSQL, Claude Sonnet, Vercel + Railway.<p>Free beta open now. 3 tests, no credit card.<p>Would love feedback from anyone who designs or builds products. Thanks!", "author": "Behavrai", "timestamp": "2026-02-20T10:17:50+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-02-20T17:29:27.285862+00:00", "processed": false}
