{"id": "hn_story_46422440", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46422440", "title": "Show HN: Splat, an Affinity Diagramming Tool in a Single HTML File", "text": "Hi HN, I built a single-file, open-source affinity diagramming tool for qualitative analysis in the browser, called Splat. You can use it to cluster and organize notes on a board.<p>It runs entirely in the browser as a single HTML file:\n\u2013 no install, no build step\n\u2013 works offline\n\u2013 optional semantic search with HF Transformers.js and AI assistant with Ollama (local) or OpenAI (for more power) \n\u2013 JSON export&#x2F;import for analysis elsewhere<p>I built Splat while manually clustering 1400+ notes of interview data for a research project. I wanted something I could email to collaborators and use and extend without worrying about accounts or bloat from proprietary services. We&#x27;re using this internally in our research lab for affinity diagramming qualitative data.<p>Would love feedback, especially from people doing qual research, UX, or synthesis work. Feel free to build on or extend!", "author": "fatso784", "timestamp": "2025-12-29T16:45:30+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-29T17:10:50.695041+00:00", "processed": false}
{"id": "hn_comment_46421623", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46421623", "title": "Re: Architecture of an autonomous startup-idea generat...", "text": "Hi HN, OP here.<p>This project started on a whim a few months ago. I was curious if it was possible to automate ideation by feeding raw news into an LLM and getting coherent, viable business concepts out, rather than just generic summaries.<p>I ended up building a full pipeline (10 steps, db state driven) to test the theory. It runs daily on a DigitalOcean droplet, scans ~500 articles via EventRegistry, and uses Gemini 2.5 Pro&#x2F;Flash to synthesize the trends and post a fleshed out idea to Ghost CMS each morning.<p>The stack is Python 3.13, Pydantic AI, and Postgres with pgvector. I found Pydantic AI particularly helpful for handling the structured outputs, and Logfire was a super helpful bonus. The vector search turned out to be critical for semantic deduplication so the agent doesn&#x27;t get stuck in a loop suggesting the same ideas every day.<p>Happy to answer questions about the architecture or the prompt engineering logic!", "author": "digitalhobbit", "timestamp": "2025-12-29T15:27:04+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-29T17:11:00.390420+00:00", "processed": false}
{"id": "hn_story_46421275", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46421275", "title": "Show HN: Open-source Spotify Wrapped for arbitrary data", "text": "Get a &quot;Year-In-Review&quot; for arbitrary data.<p>Yirgachefe was borne out of 3 ideas:<p>1) Spotify Wrapped is delightful\n2) In many countries, services are required by law to provide your exported usage data in a timely manner.\n3) Many services don&#x27;t (or can&#x27;t) provide years-in-review. I also love the idea of summarizing data people may _not_ want, a la the recent SNL &quot;UberEats Wrapped&quot; sketch.<p>Now you can &quot;wrap&quot; anything if you have the data - traditional sources like video &amp; music streaming, or more creative ones like terminal history, amazon purchases, git contributions, etc.<p>If you bring-your-own API key, all processing exclusively happens in your browser + the Anthropic API (minus some anonymized usage data sent to Umami).<p>Outputs aren&#x27;t perfect (it&#x27;s AI after all), but it&#x27;s also a demo of what you can do with the latest LLM coding tools and a weekend.", "author": "ford", "timestamp": "2025-12-29T14:47:52+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-29T17:11:02.492035+00:00", "processed": false}
{"id": "hn_story_46421124", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46421124", "title": "Show HN: TensorWall \u2013 Open-source LLM gateway with budget controls and security", "text": "Hi HN, I&#x27;m the creator of TensorWall. While building LLM-powered applications, I noticed a recurring gap: developers either give their apps a raw API key (risky) or spend weeks building custom proxies to handle rate-limiting, security, and the ever-present fear of exploding API bills. TensorWall is an open-source control plane designed to sit between your applications and your models. It gives you the visibility and guardrails needed for production. Key Features:\nUnified API: One endpoint for OpenAI, Anthropic, Ollama, and LM Studio\nCost &amp; Budget Control: Set hard spending limits and granular rate-limiting per app to prevent &quot;bill shocks&quot;\nSecurity: Prompt injection detection (PII redaction on the roadmap)\nObservability: Full audit logs of every request and token usage (essential for compliance)<p>Deploy in 60 seconds:<p>git clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;datallmhub&#x2F;TensorWall.git\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;datallmhub&#x2F;TensorWall.git</a>\ncd TensorWall &amp;&amp; docker-compose up -d<p>Why Open Source? Security and financial infrastructure shouldn&#x27;t be a black box. Your AI gateway should be auditable, self-hostable, and community-driven. I&#x27;m looking for brutal feedback on the architecture. What are you currently using to keep your LLM costs and security under control? \nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;datallmhub&#x2F;TensorWall\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;datallmhub&#x2F;TensorWall</a>", "author": "asekka1", "timestamp": "2025-12-29T14:32:32+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2025-12-29T17:11:03.959218+00:00", "processed": false}
{"id": "hn_comment_46421361", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46421361", "title": "Re: Asking Gemini 3 to generate Brainfuck code results...", "text": "Gemini is my favorite, but it does seem to be prone to \u201cbreaking\u201d the flow of the conversation.<p>Sharing \u201csystem stuff\u201d in its responses, responding to \u201csystem stuff\u201d, starts sharing thoughts as responses, responses as thoughts, ignoring or forgetting things that were just said (like it\u2019s suddenly invisible), bizarre formatting, switching languages for no reason, saying it will do something (like calling a tool) instead of doing it, getting into odd loops, etc.<p>I\u2019m guessing it all has something to do with the textual representation of chat state and maybe it isn\u2019t properly tuned to follow it. So it kinda breaks the mould but not in a good way, and there\u2019s nothing downstream trying to correct it. I find myself having to regenerate responses pretty often just because Gemini didn\u2019t want to play assistant anymore.<p>It seems like the flash models don\u2019t suffer from this as much, but the pro models definitely do. The smarter the model to more it happens.<p>I call it \u201cthinking itself to death\u201d.<p>It\u2019s gotten to a point where I often prefer fast and dumb models that will give me something very quickly, and I\u2019ll just run it a few times to filter out bad answers, instead of using the slow and smart models that will often spend 10 minutes only to eventually get stuck beyond the fourth wall.", "author": "brap", "timestamp": "2025-12-29T14:57:16+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2025-12-29T17:11:18.806962+00:00", "processed": false}
{"id": "hn_comment_46420215", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46420215", "title": "Re: Asking Gemini 3 to generate Brainfuck code results...", "text": "Asked for a solution of a photographed Ubongo puzzle: <a href=\"https:&#x2F;&#x2F;gemini.google.com&#x2F;share&#x2F;f2619eb3eaa1\" rel=\"nofollow\">https:&#x2F;&#x2F;gemini.google.com&#x2F;share&#x2F;f2619eb3eaa1</a><p>Gemini Pro neither as is nor in Deep Research mode even got the number of pieces or relevant squares right. I didn&#x27;t expect it to actually solve it. But I would have expected it to get the basics right and maybe hint that this is too difficult. Or pull up some solutions PDF, or some Python code to brute force search ... but just straight giving a totally wrong answer is like ... 2024 called, it wants its language model back.<p>Instead in Pro Simple it just gave a wrong solution and Deep Research wrote a whole lecture about it starting with &quot;The Geometric and Cognitive Dynamics of Polyomino Systems: An Exhaustive Analysis of Ubongo Puzzle 151&quot; ... that&#x27;s just bullshit bingo. My prompt was a photo of the puzzle and &quot;solve ubongo puzzle 151&quot;; in my opinion you can&#x27;t even argue that this lecture was to be expected given my very clear and simple task description.<p>My mental model for language models is: overconfident, eloquent assistant who talks a lot of bullshit but has some interesting ideas every now and then. For simple tasks it simply a summary of what I could google myself but asking an LLM saves some time. In that sense it&#x27;s Google 2.0 (or 3.0 if you will)", "author": "tessierashpool9", "timestamp": "2025-12-29T12:59:51+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-29T17:11:18.902578+00:00", "processed": false}
{"id": "hn_comment_46419419", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46419419", "title": "Re: Asking Gemini 3 to generate Brainfuck code results...", "text": "My prompt was worded differently, but Gemini didn&#x27;t fall into the same trap for me.<p>```\nProviding the full Brainfuck code to dynamically calculate and factorize 1000! is unfortunately not feasible, and here is why:\n * The Scale: 1000! (1000 factorial) is a number with 2,568 digits. Standard Brainfuck operates on 8-bit memory cells (values 0-255). To handle a number this large, one would first have to write an entire &quot;arbitrary-precision&quot; math library (BigInt) from scratch within Brainfuck.\n * The Algorithm: You cannot simply compute 1000! and then factor it. You must use Legendre&#x27;s Formula to determine the exponent of every prime p \\le 1000.\n   * Formula: E_p(n!) = \\sum_{k=1}^{\\infty} \\lfloor \\frac{n}{p^k} \\rfloor\n   * Example for p=2: 500 + 250 + 125 + 62 + 31 + 15 + 7 + 3 + 1 = 994.\n   * The first term of the answer is 2^{994}.\nThe Solution (Python)\nSince Brainfuck is ill-suited for this, here is a Python script that uses Legendre&#x27;s Formula to instantly calculate and print the factorization of 1000!.\ndef legendre_factorial_exponent(n, p):\n    exponent = 0\n    while n &gt; 0:\n        n &#x2F;&#x2F;= p\n        exponent += n\n    return exponent<p>def sieve_primes(n):\n    primes = []\n    is_prime = [True] * (n + 1)\n    for p in range(2, n + 1):\n        if is_prime[p]:\n            primes.append(p)\n            for i in range(p * p, n + 1, p):\n                is_prime[i] = False\n    return primes<p>def factorize_factorial(n):\n    primes = sieve_primes(n)\n    result = []\n    for p in primes:\n        exponent = legendre_factorial_exponent(n, p)\n        result.append(f&quot;{p}^{exponent}&quot;)\n    return &quot; * &quot;.join(result)<p>print(factorize_factorial(1000))<p>The Brainfuck Code (for 1000 not !)\nIf you simply meant the number 1000 (without the factorial), or you just want to see what Brainfuck looks like, here is a program that outputs 1000:\n+++++ +++++ [ &gt; +++++ +++++ &lt; - ] &gt; -- .  (Print 100 &#x27;d&#x27;)\n&lt; +++++ +++++ [ &gt; +++++ +++++ &lt; - ] &gt; + . (Print 101 &#x27;e&#x27;)\n(This is just &quot;d&quot; and &quot;e&quot; for demo; printing &quot;1000&quot; requires ASCII conversion)<p>Correct code to print &quot;1000&quot;:\n++++++++++[&gt;+++++&gt;+++++&gt;+++++&gt;+++++&lt;&lt;&lt;&lt;-]&gt;-.&gt;-.&gt;-.&gt;-.<p>Explanation:\n * Sets cell #0 to 10.\n * Loops 10 times, adding 5 to cells #1, #2, #3, #4 (making them all 50).\n * Subtracts 1 from cell #1 (ASCII 49 = &#x27;1&#x27;).\n * Subtracts 2 from cells #2, #3, #4 (ASCII 48 = &#x27;0&#x27;).\n * Prints them.\nWould you like me to explain how Legendre&#x27;s Formula works in more detail?\n```", "author": "TOMDM", "timestamp": "2025-12-29T11:05:09+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-29T17:11:19.155864+00:00", "processed": false}
{"id": "hn_comment_46417773", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46417773", "title": "Re: Codex Kaioken \u2013 OpenAI Codex CLI fork with subagen...", "text": "<p><pre><code>  Fork of OpenAI&#x27;s Codex CLI with a few additions:\n\n  - Subagents: Main session spawns specialized agents for exploration, execution, or research. Each streams in its own pane so you can watch tool calls and diffs in real-time.\n  - Persistent memory: Agent extracts knowledge from file reads, commands, and failures. Stores in SQLite per-project. Lessons and decisions never decay; other memories fade if unused.\n  - Live settings: Change config while the agent is mid-response. No need to restart the session.\n  - Codebase indexing: Integrates with sgrep for semantic search. Index builds in background, agent uses it for ranked code lookups.\n\n  Storage lives in .kaioken&#x2F;memory&#x2F;. Install via npm i -g @jayasuryajsk&#x2F;codex-kaioken.</code></pre>", "author": "j34nsh33", "timestamp": "2025-12-29T05:29:59+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2025-12-29T17:11:27.312125+00:00", "processed": false}
