{"id": "hn_story_46721900", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721900", "title": "Show HN: A registry for curated, high quality Claude skills and skillsets", "text": "Hi Hacker News!<p>I\u2019m Ritam, working with the small but mighty team at Nori. We\u2019ve been obsessed in recent months with how to take tools like Claude Code from \u201cI\u2019ll experiment around with this\u201d to \u201cThis is the most useful and necessary thing I use every day\u201d. When I first sat down with our team to check out what they\u2019d built, I found my skepticism about agentic coding melting away\u2014they\u2019d built useful, high quality, handwritten skills, instructions that functioned as \u201cskillsets\u201d to tie skills together for consistent and replicable results, and tooling to manage loading the right context for the right task into the agent.<p>In recent weeks, the conversation around skills has reached a fever pitch, as have lists and sites full of skills scraped from all over the internet. Much like the actual gold rush, the current state of those collections requires a lot of time and sifting to find small, real chunks of 24k skills hidden amongst the muck (or to use a more relevant word, slop). So at Nori, we decided to build an npm-style registry full of our curated skills and the necessary skillsets to tie them together. We\u2019re launching it today at noriskillsets.dev, and it comes with a handy CLI tool to manage skills and skillsets on your machine, swapping them in and out of your Claude config as needed.<p>If you&#x27;re not sure what I mean by skillsets: this is the real secret sauce that makes us way more productive. These are custom, purpose-built instruction sets for Claude Code that reference our skills and lay out how to tie them together into an appropriate SDLC, from planning to PR. There are lot of little weird semantic tricks that make Claude more attentive to instructions that we&#x27;ve baked into these.<p>Why not host these on Github? We&#x27;re betting on our thesis: that it makes sense to tie these together with an easy to use CLI built to install and swap configs in and out of Claude, that a website custom-built for this purpose will allow us to iterate our way to surfacing what matters most and what doesn&#x27;t to our users (custom, useful metadata), and that having an app separate from the discussion and churn on Github will make this a useful tool and less of a noisy social space.<p>If you\u2019ve been excited about the recent discussion about Claude Code but don\u2019t know where to start with customization: this tool is for you. If you\u2019ve tried it and have found yourself underwhelmed by inconsistency or an inability to deal with complexity: this tool is for you. And if you\u2019ve got your setup all customized but are constantly wading through hundreds of scraped sloppy skills online, searching for helpful ones\u2026 this tool is for you. We\u2019re excited to hear what you think. We&#x27;ll be adding more skills and skillsets from across the web from other folks that we trust, so come back frequently, and we encourage folks to send us the tools they love!", "author": "ritammehta", "timestamp": "2026-01-22T17:00:52+00:00", "score": 5, "num_comments": 0, "products": ["claude"], "categories": ["naming_terminology", "navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:20:33.592441+00:00", "processed": false}
{"id": "hn_story_46721773", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721773", "title": "Show HN: I'm tired of my LLM bullshitting. So I fixed it", "text": "As a handsome local AI enjoyer\u2122 you\u2019ve probably noticed one of the big flaws with LLMs:<p>It lies. Confidently. <i>ALL THE TIME.</i><p>I\u2019m autistic and extremely allergic to vibes-based tooling, so \u2026 I built a thing. Maybe it\u2019s useful to you too.<p>The thing: llama-conductor<p>llama-conductor is a router that sits between your <i>frontend</i> (eg: OWUI) &amp; <i>backend</i> (llama.cpp + llama-swap). Local-first but it should talk to anything OpenAI-compatible if you point it there (note: experimental so YMMV).<p>LC is a glass-box that makes the stack behave like a <i>deterministic system</i>, instead of a drunk telling a story about the fish that got away.<p>TL;DR: \u201cIn God we trust. All others must bring data.\u201d<p>Three examples:<p>1. KB mechanics (markdown, JSON, checksums)<p>You keep \u201cknowledge\u201d as dumb folders on disk. Drop docs (.txt, .md, .pdf`) in them. Then:<p>&gt;&gt;attach &lt;kb&gt; - attaches a KB folder<p>&gt;&gt;summ new - generates SUMM_.md files with <i>SHA-256 provenance</i> baked in + moves the original to a sub-folder<p>Now, when you ask something like:<p>&gt; \u201cyo, what did the Commodore C64 retail for in 1982?\u201d<p>..it answers from the attached KBs <i>only</i>.<p>If the fact isn\u2019t there, it tells you - explicitly - instead of winging it. Eg:<p>&quot;The provided facts state the Commodore 64 launched at $595 and was reduced to $250, but do not specify a 1982 retail price. The Amiga\u2019s pricing and timeline are also not detailed in the given facts.<p>Missing information includes the exact 1982 retail price for Commodore\u2019s product line and which specific model(s) were sold then.&quot;<p>[Confidence: medium | Source: Mixed]<p>No vibes. Just: here\u2019s what\u2019s in your docs, here\u2019s what\u2019s missing, don&#x27;t GIGO yourself into stupid.<p>Then, if you&#x27;re happy with the summary, you can:<p>&gt;&gt;move to vault<p>2. Mentats: proof-or-refusal mode (Vault-only)<p>Mentats is the \u201cdeep think\u201d pipeline against your <i>curated</i> sources.<p>* no chat history<p>* no filesystem KBs<p>* no Vodka<p>* <i>Vault-only grounding</i> (Qdrant)<p>It runs a triple-pass (thinker \u2192 critic \u2192 thinker). It\u2019s slow on purpose. You can audit it. And if the Vault has nothing relevant? It refuses and tells you to go pound sand:<p>FINAL_ANSWER:<p>The provided facts do not contain information about the Acorn computer or its 1995 sale price.<p>Sources: Vault<p>FACTS_USED: NONE<p>[ZARDOZ HATH SPOKEN]<p>Also yes, it writes a mentats_debug.log. Go look at it any time you want.<p>The flow is basically:<p>Attach KBs \u2192 SUMM \u2192 Move to Vault \u2192 Mentats.<p>No mystery meat. No \u201ctrust me bro, embeddings.\u201d<p>3. Vodka: deterministic memory on a potato budget<p>Potato PCs have two classic problems: goldfish memory + context bloat that murders your VRAM.<p>Vodka fixes both without extra model compute.<p>* !! stores facts verbatim (JSON on disk)<p>* ?? recalls them verbatim (TTL + touch limits so memory doesn\u2019t become landfill)<p>* CTC (Cut The Crap)* hard-caps context (last N messages + char cap) and creates a concatenated summary (not LLM) so you don\u2019t get VRAM spikes after 400 messages<p>So instead of:<p>\u201cRemember my server is 203.0.113.42\u201d \u2192 \u201cGot it!\u201d \u2192 [100 msgs later] \u2192 \u201c127.0.0.1\u201d<p>you get:<p>!! my server is 203.0.113.42`\n?? server ip \u2192 <i>203.0.113.42</i> (with TTL&#x2F;touch metadata)<p>And because context stays bounded: stable KV cache, stable speed, your potato PC stops crying.<p>There\u2019s more (a lot more) in the README, but I\u2019ve already over-autism\u2019ed this post.<p>TL;DR:<p>If you want your local LLM to <i>shut up when it doesn\u2019t know</i> and <i>show receipts when it does</i>, come poke it:<p>Primary (Codeberg)\n<a href=\"https:&#x2F;&#x2F;codeberg.org&#x2F;BobbyLLM&#x2F;llama-conductor\" rel=\"nofollow\">https:&#x2F;&#x2F;codeberg.org&#x2F;BobbyLLM&#x2F;llama-conductor</a><p>Mirror (GitHub):\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;BobbyLLM&#x2F;llama-conductor\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;BobbyLLM&#x2F;llama-conductor</a><p>PS: Sorry about the AI slop image. I can&#x27;t draw for shit.<p>PPS: A human with ASD wrote this using Notepad++. If it the formatting or language are weird, now you know why.", "author": "BobbyLLM", "timestamp": "2026-01-22T16:50:09+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:20:34.205239+00:00", "processed": false}
{"id": "hn_comment_46721888", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721888", "title": "Re: Ask HN: What is your Claude Code setup? For common...", "text": "My Claude Code Setup<p>I work on multiple git worktrees of the same repo simultaneously, so I keep my Claude config in a parent directory and symlink it into each worktree. One place to update settings, policies, skills - they all stay in sync.<p>I use a policy MCP server that serves my coding standards as markdown files. I reference them with a \u00a7 notation and they get pulled into context automatically. The server recursively resolves references, so if one policy points to another (like general code rules pointing to C++ specific rules), it follows the chain and pulls in everything needed. I have general rules, code quality rules, and C++ specific style all defined once and injected when needed.<p>I&#x27;ve set up a few skills to streamline things. The engineer skill loads the relevant policies before any code gets written. The continue&#x2F;restart skills let me save session state to a continuation plan and pick up where I left off later.<p>Checkmate handles linting validation - different rules for different parts of the codebase (C++ engine code vs TypeScript cloud workers vs shell scripts).<p>Basically: policies keep the code consistent, skills keep the workflow consistent, and the shared config keeps everything in sync across worktrees.<p>(edited for formatting)", "author": "mannewalis", "timestamp": "2026-01-22T17:00:15+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:20:35.242662+00:00", "processed": false}
{"id": "hn_comment_46721845", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721845", "title": "Re: Ask HN: What is your Claude Code setup? For common...", "text": "I guess there are probably better worfklows, but I went from raw-dogging it to more structure frameworks like Get Shit Done (GSD) back to raw-dogging it but with lots of use of planning mode.<p>Initially I thought the structure of a framework would be nice. Tracking state, breaking things down into milestones, phases, etc. But ultimately I felt like it was all an illusion. Im not sure it&#x27;s possible to track and provide Claude with the current state of the project at all times and it&#x27;s just going to have to re-grok a lot of things all the time, whether you use one of these frameworks or not.<p>IDK, maybe there are better ways. But it feels like it increases the time and effort by a lot without any real improvement other than briefly making me feel more organized.", "author": "nonethewiser", "timestamp": "2026-01-22T16:56:18+00:00", "score": null, "num_comments": null, "products": ["claude", "grok"], "categories": ["tone"], "sentiment": null, "collected_at": "2026-01-22T17:20:35.288861+00:00", "processed": false}
{"id": "hn_story_46721474", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721474", "title": "Show HN: BrowserOS \u2013 \"Claude Cowork\" in the browser (open source)", "text": "Hey HN! We&#x27;re Nithin and Nikhil, twin brothers building BrowserOS (YC S24). We&#x27;re an open-source, privacy-first alternative to the AI browsers from big labs.<p>On BrowserOS, we provide first-class support to bring your own LLMs either local models or via API keys and run the agent entirely on the client side, so your data stays on your machine!<p>Today we&#x27;re launching filesystem access... just like Claude Cowork, our browser agent can read files, write files, run shell commands! But honestly, we didn&#x27;t plan for this. It turns out the privacy decision we made 9 months ago accidentally positioned us for this moment.<p>--- The architectural bet we made 9 months ago\nUnlike other AI browsers (ChatGPT Atlas, Perplexity Comet) where the agent loop runs server-side, we decided early on to run our agent entirely on your machine (client side).<p>But building everything on the client side wasn&#x27;t smooth.<p>We initially built our agent loop inside a Chrome extension. But we kept hitting walls:<p>1) JS (background service worker) is single-threaded, so we couldn&#x27;t start multiple agents in parallel.<p>2) Not having access to a NodeJS-like runtime meant we couldn&#x27;t use many great npm packages (Vercel AI SDK, Anthropic&#x27;s MCP SDK, etc)<p>3) And finally, there was no good way to expose our agent and tools as an API<p>So we made the hard decision 2 months ago to throw away everything we built and start from scratch.<p>In the new architecture, we went with a sidecar approach. We put our agent loop in a standalone Bun binary and ship it alongside our Chromium binary. We also decided not to rewrite our own agent loop, but borrowed gemini-cli&#x27;s loop with some tweaks! We wrote a neat adapter to translate between Gemini format and Vercel AI SDK format. You can look at our entire codebase here: <a href=\"https:&#x2F;&#x2F;git.new&#x2F;browseros-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;git.new&#x2F;browseros-agent</a><p>--- How this helped build filesystem access\nWhen Claude Cowork launched, we realized something: because Atlas and Comet run their agent loop server-side, there&#x27;s no good way for their agent to access your files without uploading them to the server first.<p>But our agent was already local. Adding filesystem access meant just... opening the door (with your permissions ofc). Our agent can now read and write files just like Claude Code. No uploads, no cloud storage, no sync.<p>--- What you can actually do today<p>a) Organize files in my desktop folder <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;NOZ7xjto6Uc\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;NOZ7xjto6Uc</a><p>b) Open top 5 HN links, extract the details and write summary into a HTML file <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;uXvqs_TCmMQ\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;uXvqs_TCmMQ</a><p>--- Where we are now\nIf you haven&#x27;t tried us since the last Show HN, give us another shot. The new architecture unlocked a ton of new features, and we&#x27;ve grown to 8.5K GitHub stars and 100K+ downloads:<p>c) You can now build more reliable workflows using n8n-like graph <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;H_bFfWIevSY\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;H_bFfWIevSY</a><p>d) You can also use BrowserOS as an MCP server in Cursor or Claude Code <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;5nevh00lckM\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;5nevh00lckM</a><p>e) You can also schedule repetitive tasks!<p>--- Why we think browser is the right platform\nWe are very bullish on browser being the right platform for a Claude Cowork like agent. Browser is the most commonly used app by knowledge workers (emails, docs, spreadsheets, research, etc). And it seems like even Anthropic recognizes this -- for Claude Cowork, they have janky integration with browser via a chrome extension. But owning the entire stack allows us to provide a much smoother experience. It also lets us build differentiated features that wouldn&#x27;t be possible otherwise. One example: Browser ACLs.<p>Agents can do dumb or destructive things, so we&#x27;re adding browser-level guardrails (think IAM for agents): &quot;role(agent): can never click buy&quot; or &quot;role(agent): read-only access on my bank&#x27;s homepage.&quot; We have a prototype already\u2014curious to hear your take on this and the overall thesis.<p>We\u2019ll be in the comments. Thanks for reading!<p>GitHub: <a href=\"https:&#x2F;&#x2F;git.new&#x2F;browseros\" rel=\"nofollow\">https:&#x2F;&#x2F;git.new&#x2F;browseros</a>\nDownload: <a href=\"https:&#x2F;&#x2F;browseros.com\">https:&#x2F;&#x2F;browseros.com</a> (available for Mac, Windows, Linux!)", "author": "felarof", "timestamp": "2026-01-22T16:30:58+00:00", "score": 5, "num_comments": 0, "products": ["claude", "chatgpt", "gemini", "perplexity"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-22T17:20:36.155289+00:00", "processed": false}
{"id": "hn_story_46721051", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721051", "title": "Show HN: VibeFarm \u2013 A non-generative IDE for composing AI prompts", "text": "Creator here.<p>I built VibeFarm because prompt work kept collapsing into scattered notes, version chaos, and lost \u201crecipes\u201d across Midjourney, Sora, DALL\u00b7E, ChatGPT, etc. I wanted something closer to an IDE: structured, reusable, and model-agnostic, not a chat wrapper.<p>VibeFarm is a non-generative prompt composition workspace. Prompts are built from semantic slots (subject&#x2F;context&#x2F;style&#x2F;etc.), optional layers, and saved snapshots (\u201cVibeCards\u201d) that export clean prompts to any model or to a portable .vibe JSON format.<p>Design choices:\n- No generation inside the app, it\u2019s intentionally just composition (no model lock-in, no API costs).\n- Static curated vocabulary: 20,000+ palettes, ~1M fragments, instant drag-drop, no runtime calls.\n- Reuse-first: variables for series swaps, video-mode timeline overrides, and versioned snapshots.<p>Try it instantly (guest mode): <a href=\"https:&#x2F;&#x2F;app.vibefarm.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;app.vibefarm.ai</a>  \nHomepage: <a href=\"https:&#x2F;&#x2F;vibefarm.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;vibefarm.ai</a>  \nOptional demo video: <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;IgEly7VpwwI\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;IgEly7VpwwI</a><p>Happy to answer questions or hear feedback, especially from people doing structured or repeatable prompt work.", "author": "vibefarm", "timestamp": "2026-01-22T16:02:32+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:20:39.844321+00:00", "processed": false}
{"id": "hn_story_46721025", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46721025", "title": "Show HN: Open-source-ish chart pattern detection using Gemini Vision API", "text": "I built an AI that detects chart patterns to fight my own confirmation bias I kept losing money on trades because I&#x27;d &quot;see&quot; patterns that weren&#x27;t there. Classic confirmation bias \u2014 when you&#x27;re already in a position, your brain lies to you.<p>So I built a tool: upload any chart screenshot, get pattern detection in seconds.<p>Why Gemini over GPT-4V? \nTested both. Gemini 1.5 Flash is: - Faster (~2s vs ~5s) - Cheaper (~$0.0001 per analysis) - More consistent structured outputs for this specific task GPT-4V sometimes gave me essays. Gemini stayed focused.<p>Stack: Next.js 14, Supabase (auth + postgres), Stripe, Vercel<p>The hard parts weren&#x27;t the AI:  - Auth edge cases (email confirmation flows, session refresh) - Credit system (what happens when API fails mid-request? refund?) - Making it not look like a hackathon project (I&#x27;m a backend guy, CSS is pain)<p>Is it always right? No. TA itself is debatable. But it&#x27;s a second opinion that doesn&#x27;t care what positions I&#x27;m holding. That&#x27;s the value.<p><a href=\"https:&#x2F;&#x2F;trinith-ai.vercel.app\" rel=\"nofollow\">https:&#x2F;&#x2F;trinith-ai.vercel.app</a><p>Would love feedback from:\n- Traders who can tell me if the output format is useful \n- Anyone who&#x27;s built with vision APIs (optimization tips?) \n- Skeptics who think this is dumb (genuinely want to hear why)", "author": "rvnx_exe", "timestamp": "2026-01-22T16:01:00+00:00", "score": 1, "num_comments": 0, "products": ["chatgpt", "gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:20:39.890619+00:00", "processed": false}
{"id": "hn_story_46720146", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46720146", "title": "Surviving AI", "text": "What follows was written by ChatGPT 5.2 Instant and it reflects my conversation with it about the two articles linked below. Enjoy.<p>Ego plays a complicated role in periods of technological change. It sharpens skill during stable eras, but it often hinders adaptation during inflection points.<p>Two recent essays on AI and software engineering illustrate this tension clearly.<p>Emir Ribic\u2019s \u201cFrom Craftsmen to Operators\u201d\nhttps:&#x2F;&#x2F;dev.ribic.ba&#x2F;the-rapid-evolution-of-software-engineer-s-role<p>Ribic frames the rise of AI-assisted development as a loss of craft. He mourns the disappearance of deep, line-by-line problem solving and the sense of authorship that came with it. Engineers, in his telling, are becoming operators\u2014prompting, reviewing, and assembling\u2014rather than builders. The piece captures something real: pride in difficulty, satisfaction in mastery, and the joy of personally solving hard problems.<p>But that pride is also where ego enters. What\u2019s being lost isn\u2019t just a way of working\u2014it\u2019s a form of status. Manual coding was scarce, hard-won, and socially rewarded. When AI erodes that scarcity, it threatens identity as much as technique. The article largely dwells on that loss, while giving little attention to the exhilaration many engineers feel when modern tools collapse days of work into minutes.<p>P.C. Maffey\u2019s \u201cAInxiety\u201d\nhttps:&#x2F;&#x2F;pcmaffey.com&#x2F;ainxiety-1&#x2F;<p>Maffey takes a more balanced view. He openly acknowledges discomfort, ethical concerns, and the risk of over-reliance on AI, but he also accepts the productivity gains and uses AI extensively in his work. Rather than framing the shift as a fall from craft, he reframes the role: less time spent on syntax, more on design, planning, judgment, and responsibility. Where Ribic sees loss, Maffey sees trade-offs.<p>Taken together, the contrast suggests a broader pattern we\u2019ve seen before.<p>During the Industrial Revolution, skilled artisans resisted mechanization not only because of economic threat, but because identity and status were tied to difficulty and exclusivity. The same thing happened when typewriters gave way to personal computers in offices\u2014typing lost its prestige once everyone could do it. In each case, ego slowed adaptation, but also reflected a real commitment to excellence that had previously raised standards.<p>That leads to a fair synthesis:<p>Ego hones skill in stable environments. It motivates mastery, pride, and depth. But during technological inflection points, the same ego can become friction\u2014blinding people to leverage, speed, and new forms of craftsmanship.<p>Progress doesn\u2019t eliminate craft; it relocates it. The challenge isn\u2019t to abandon pride in skill, but to recognize when clinging to old expressions of that skill prevents adaptation to more powerful tools.<p>The winners in these transitions aren\u2019t ego-free. They\u2019re ego-aware.", "author": "dpforesi", "timestamp": "2026-01-22T15:05:58+00:00", "score": 2, "num_comments": 2, "products": ["chatgpt"], "categories": ["navigation"], "sentiment": null, "collected_at": "2026-01-22T17:20:49.053273+00:00", "processed": false}
{"id": "hn_story_46719755", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46719755", "title": "Show HN: SkillLens \u2013 scan and audit locally installed agent skills", "text": "Hi HN \u2014 I built a small CLI called SkillLens to help answer: \u201cWhat agent skills do I have installed, and are any of them sketchy?\u201d<p>A lot of agent ecosystems (Claude&#x2F;Codex&#x2F;OpenCode, etc.) store skills as folders with a SKILL.md. These files can contain surprisingly powerful instructions (and sometimes unsafe patterns), but they\u2019re easy to forget once installed. We&#x27;re also tend to run them with --dangerously-skip-permissions and let them install whatever they want, but I got a bit anxious about it so decided to build a tool to have some peace of mind.<p>I decided to not go with AST static check but instead use whatever CLI you have locally to validate it.<p>SkillLens does two things:<p>1. Discovery: it scans common local skill locations (configurable) and lists what it finds.\n2. Optional audit: if you have an auditor CLI installed (claude or codex), it sends each SKILL.md (currently truncated to ~12k chars) to the auditor and asks for structured JSON output:<p>- verdict: safe | suspicious | unsafe\n- risk: 0\u201310\n- summary + issues with evidence<p>It also caches audit results locally so reruns won&#x27;t check skills again unless those were updated, you installed anything new or you explicitly asked it to do so with --force flag.<p>Install&#x2F;run:<p>npx skilllens scan\n# or\npnpm dlx skilllens scan<p>Notes &#x2F; caveats:<p>- v0.1; I\u2019m still iterating on the prompt&#x2F;schema and the \u201cwhat counts as suspicious\u201d heuristics.\n- Today it sends the skill text to whatever your auditor CLI uses (so treat it like sharing the skill contents with that provider). \u201cRedacted evidence extraction\u201d is planned, but not implemented yet.\n- If the auditor CLI isn\u2019t installed, it still produces a scan report and marks audits as skipped.", "author": "morozred", "timestamp": "2026-01-22T14:37:05+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:20:52.414199+00:00", "processed": false}
{"id": "hn_story_46719447", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46719447", "title": "Show HN: Mother May I? \u2013 Auto-approve safe Bash commands in Claude Code", "text": "Hi HN,<p>I built MMI (Mother May I?) with Claude Code to solve an annoying friction point: manually approving every single Bash command.<p>The Problem<p>Claude Code asks for permission before running any shell command. This is sensible from a security standpoint, but it means you&#x27;re constantly hitting &quot;approve&quot; for commands like git status, pytest, or ls -la. It breaks flow and adds friction to every development session.<p>The Solution<p>MMI is a CLI hook that auto-approves known-safe commands while maintaining a fail-secure default. Unrecognized commands still require manual approval.<p>How it works:<p>1. Deny list checked first \u2013 Dangerous patterns (sudo, rm -rf &#x2F;, chmod 777) are always rejected<p>2. Safe commands allowlisted \u2013 Read-only commands, test runners, linters, and build tools you configure<p>3. Proper shell parsing \u2013 Uses an AST-based parser (mvdan.cc&#x2F;sh) to correctly handle pipes, chains, and quoted strings<p>4. Audit trail \u2013 Every decision logged to JSON-lines for debugging and compliance<p>Key design decisions:<p>- Fail-secure: Unknown commands require manual approval<p>- Command chains validated: &quot;safe &amp;&amp; dangerous&quot; is rejected (ALL segments must be safe)<p>- Wrapper-aware: Strips timeout, env, .venv&#x2F;bin&#x2F; before validation<p>- Heredoc-smart: Backticks inside quoted heredocs treated as literal text<p>Example config (TOML):<p>```\n  [safe]\n  simple = [&quot;ls&quot;, &quot;pwd&quot;, &quot;cat&quot;, &quot;head&quot;, &quot;tail&quot;]\n  subcommands.git = [&quot;status&quot;, &quot;log&quot;, &quot;diff&quot;, &quot;add&quot;, &quot;commit&quot;, &quot;push&quot;]\n  subcommands.cargo = [&quot;build&quot;, &quot;test&quot;, &quot;check&quot;, &quot;clippy&quot;]<p><pre><code>  [deny]\n  simple = [&quot;sudo&quot;, &quot;su&quot;, &quot;doas&quot;]\n  regex = [&quot;rm\\\\s+(-[^\\\\s]*)?.*\\\\s+&#x2F;($|\\\\s)&quot;]</code></pre>\n```<p>A note on security<p>Claude Code recently added built-in sandbox mode that restricts file system writes and network access. This is a great step toward secure defaults. However, MMI still provides value even with sandboxing:<p>- Audit trail \u2013 Every command decision logged for review and compliance<p>- Explicit allowlists \u2013 Know exactly what&#x27;s approved rather than relying on implicit sandbox rules<p>- Deny patterns \u2013 Block specific dangerous patterns before they hit the sandbox<p>- Reduced interruptions \u2013 Sandbox mode can still prompt; MMI auto-approves known-safe commands<p>No allowlist can anticipate every attack vector. MMI is a convenience layer that reduces friction for common safe commands while maintaining defense-in-depth \u2013 it works alongside sandboxing, not as a replacement.<p>Why not just approve everything?<p>That defeats the purpose of Claude Code&#x27;s permission system. MMI lets you define exactly which commands you trust, keeps a complete audit trail, and defaults to asking when uncertain.<p>The repo includes example configs for Python, Node, Rust, and a strict read-only mode.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;dgerlanc&#x2F;mmi\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dgerlanc&#x2F;mmi</a><p>Blog post with more details: <a href=\"https:&#x2F;&#x2F;dangerlanc.com&#x2F;writing&#x2F;mmi&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;dangerlanc.com&#x2F;writing&#x2F;mmi&#x2F;</a>", "author": "dgerlanc", "timestamp": "2026-01-22T14:11:16+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-22T17:20:57.545903+00:00", "processed": false}
{"id": "hn_comment_46719396", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46719396", "title": "Re: Claude Cowboys...", "text": "Wow, I wrote a system very similar to the author that seems to becoming the defacto for ground-up multi-agent terminal workflows. git worktrees + tmux + claude hooks", "author": "acron0", "timestamp": "2026-01-22T14:06:49+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-22T17:20:59.702804+00:00", "processed": false}
{"id": "hn_comment_46718800", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46718800", "title": "Re: Satya Nadella: \"We need to find something useful f...", "text": "LLMs and their capabilities are very impressive and definitely useful. The productivity gains often seem to be smaller than intuitively expected though. For example, using ChatGPT to get a response to a random question like &quot;How do I do XYZ&quot; is much more convenient than googling it, but the time savings are often not that relevant for your overall productivity. Before LLMs you were usually already able to find the information quickly and even a 10x speed up does not really have too much of an impact on your overall productivity, because the time it took was already negligible.", "author": "MadDemon", "timestamp": "2026-01-22T13:13:31+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:21:04.658858+00:00", "processed": false}
{"id": "hn_comment_46718392", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46718392", "title": "Re: Show HN: Infera \u2013 agentic CLI for inferring and pr...", "text": "Hi there! I recently had to learn terraform to setup the infra for a new place I joined. Claude Code helped A LOT - but there were instances where it was clear that had I not been experienced enough, I would be banging my head against the wall for days. This is where I got the idea.<p>The tool itself is a thin wrapper over terraform and uses the Claude Agent SDK combined with a 80+ best practice architectural templates to determine the best way to get your project on your preferred cloud provider. Would love to know if this interests anyone else.<p>The interface is taken direclty from terraform and has the exact same subcommands - init, plan and apply. You can also run `infera deploy` to do all three at once.", "author": "garkotipankaj", "timestamp": "2026-01-22T12:30:43+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-01-22T17:21:05.706671+00:00", "processed": false}
{"id": "hn_story_46718162", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46718162", "title": "Show HN: SGR \u2013 A Linear-Complexity \"Living Cell\" Outperforming Transformers", "text": "I am developing an architecture called Sparse Gated Resonance (SGR). It is a sequence modeling approach designed to avoid the quadratic scaling of traditional Self-Attention. I have been benchmarking a 722k-parameter SGR against a 921k-parameter Transformer on Victor Hugo\u2019s &quot;Notre-Dame de Paris&quot; (English).<p>The SGR replaces the attention mechanism with a &quot;Causal Pulse.&quot; It uses gated 1D convolutions to generate a navigation vector that resonates against a brain-map of character embeddings. This allows the model to maintain a &quot;Living Cell&quot; state that updates with linear complexity.<p>Full source and implementation: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;MrPan2048&#x2F;GeometricTransformer&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;MrPan2048&#x2F;GeometricTransformer&#x2F;</a><p>Benchmarking Data (Notre-Dame de Paris):<p>STEP 3900 ARCH | LOSS | PPL | ENT | TIME\nSGR | 1.4481 | 4.26 | 1.5476 | 19.0ms \nSTD | 2.0275 | 7.59 | 2.1476 | 40.3ms<p>Semantic Comparison (Generation from &quot;Quasimodo&quot;):<p>SGR: &quot;Quasimodo. Then minds that the accasteady which which the&quot; \nSTD: &quot;Quasimododo ng, o uer tre the todo hemo\u2019He wand at tine.&quot;<p>Technical Observations:<p>Computational Efficiency: SGR maintains a significant latency advantage, consistently running at ~19ms compared to the Transformer&#x27;s ~40ms. This confirms the efficiency of the linear pulse over quadratic attention.<p>Convergence Quality: By Step 3700, SGR reached a Perplexity (PPL) of 4.46, whereas the Transformer lagged at 8.36. SGR successfully produces recognizable English phrases and punctuation, while the Transformer still exhibits &quot;stuttering&quot; artifacts (e.g., &quot;Quasimodododod&quot;).<p>Entropy Stability: SGR has stabilized at an entropy of ~1.54, which represents the optimal &quot;Mastery Zone&quot; for English text. The Transformer\u2019s higher entropy (~2.14) correlates with its lack of structural coherence.<p>I am seeking an endorsement to publish a formal paper on this architecture to arXiv (CS.ML). I believe these results demonstrate that &quot;Living Cell&quot; resonance models can outperform Attention in parameter-constrained and latency-sensitive environments. If you are a researcher willing to endorse or review the mathematical formalization, please contact me via GitHub.", "author": "MrPan", "timestamp": "2026-01-22T12:03:53+00:00", "score": 4, "num_comments": 0, "products": ["perplexity"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:21:07.481657+00:00", "processed": false}
{"id": "hn_story_46717233", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46717233", "title": "Show HN: Perspectives \u2013 I wanted AI to challenge my thinking, not validate it", "text": "I built Perspectives because I got tired of ChatGPT agreeing with everything I said.<p>Ask any LLM to &quot;consider multiple perspectives&quot; and you get hedged consensus. The model acknowledges trade-offs exist, then settles on a moderate position that offends nobody. Useful for summaries. Useless for decision making.<p>Perspectives forces disagreement. 8 personas with fundamentally incompatible frameworks debate your question through a structured protocol, then vote using Single Transferable Vote to surface where they actually land. The output is a PDF report synthesising all of it.<p>How it works<p>Blind Proposals: Each persona generates a position without seeing the others. This prevents the &quot;anchoring problem&quot; where early responses shape later ones, bypassing the default sycophancy of LLMs.<p>Interrogation of Blind Proposals: Proposals face structured challenges from 3 opposing personas. A &quot;high-empathy&quot; persona (e.g., The Idealist) will be challenged by a &quot;low-empathy&quot; cluster (e.g., The Pragmatist). This reveals exactly where arguments buckle under pressure.<p>Discussion &amp; Voting: Personas can debate (optional) before ranking preferences via STV. This highlights first-choice winners and preference flows rather than simple majority rule.<p>Analysis&#x2F;Prediction Report: The final PDF structures recommendations first, followed by supporting analysis (factual background, risk assessment, evidence quality).<p>Two Operational Modes<p>Analysis Mode (&quot;What should we do?&quot;): Evaluates options and surfaces trade-offs. Output is qualitative judgment.<p>Prediction Mode (&quot;What will happen?&quot;): Generates probability estimates with resolution criteria.<p>Feedback Loops<p>Most AI agent projects have no way to measure whether their outputs are actually good. Users provide subjective feedback, which is noisy and unreliable. The system optimises for seeming useful rather than being useful.<p>Prediction Mode creates an objective feedback loop. When a prediction resolves, I can measure accuracy.<p>I&#x27;m integrating Polymarket as the verification source. Run a question through Perspectives, record the predictions, compare against actual outcomes when they resolve. Over time, this builds calibration data showing which methodologies perform best for different question types.<p>Persona Sets<p>Different decisions need different analytical lenses. Four built-in sets:<p>Philosophical (Default): Best for ethical dilemmas and strategic decisions.<p>Business-Focused: Best for commercial decisions.<p>Product-Focused: Best for product development.<p>Forecaster: Optimised for Prediction Mode.<p>Technical Details<p>LLM Support: Supports any OpenAI&#x2F;Anthropic compatible API (Claude, OpenRouter, Ollama, Grok, etc.).<p>Web Search: Optional integration for grounding debates in recent events.<p>Output: Single PDF report per query.<p>What I&#x27;m Looking For<p>I&#x27;ve been building this solo and could use external feedback on a few things:<p>1. Does the blind proposal mechanism actually produce better disagreement?<p>2. Is the interrogation protocol overkill or useful? The structured challenge&#x2F;response&#x2F;verdict cycle generates rich data, but adds latency (dependant on concurrency settings).<p>3. What decisions would you run through this?<p>4. Do you use ChatGPT or similar systems to make decisions?<p>5. Do you find &quot;chain of thought&quot; output useful for tracking reasoning?<p>Links<p>Perspectives: <a href=\"https:&#x2F;&#x2F;getperspectives.app\" rel=\"nofollow\">https:&#x2F;&#x2F;getperspectives.app</a><p>Dev blog: <a href=\"https:&#x2F;&#x2F;blog.jmatthews.uk\" rel=\"nofollow\">https:&#x2F;&#x2F;blog.jmatthews.uk</a><p>Example Analysis Report (Is it viable to run a nation where all laws expire after 10 years and must be re-passed?): <a href=\"https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1hsJOWsQDAtVOqOKF6_a_Q1jYOlB05PZb&#x2F;view?usp=sharing\" rel=\"nofollow\">https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1hsJOWsQDAtVOqOKF6_a_Q1jYOlB...</a><p>Example Prediction Report (Will Kraken IPO by 31st March 2026?): <a href=\"https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1m3RedFtv8lKgFqf1_rvzl8W6cTs7mIhc&#x2F;view?usp=sharing\" rel=\"nofollow\">https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1m3RedFtv8lKgFqf1_rvzl8W6cTs...</a><p>Happy to answer any questions in this thread.", "author": "Jamium", "timestamp": "2026-01-22T10:02:47+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt", "grok"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:21:17.003817+00:00", "processed": false}
{"id": "hn_comment_46716911", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46716911", "title": "Re: Best Way to Export ChatGPT Conversations to PDF, N...", "text": "f you use ChatGPT for work, you\u2019ve probably run into the same problem: copying a long chat into Docs&#x2F;Notion breaks formatting, and code blocks become messy.<p>Here are a few practical ways to export ChatGPT conversations to PDF, Notion, Word, and Google Docs \u2014 depending on what you need.<p>1) Quick export (full conversation)\nBest when you want a clean archive or something you can share.<p>Open the chat you want to export<p>Export the full conversation<p>Choose a format: PDF &#x2F; Word &#x2F; Google Docs &#x2F; Notion<p>Download or send it to your destination<p>2) Export only the important parts (selected messages)\nBest when the chat is long and you only want the final answers, code, or key steps.<p>Select the messages you want to keep<p>Export only the selection<p>Save as PDF&#x2F;Word or send to Docs&#x2F;Notion<p>3) Make it readable (styling options)\nBest when you plan to share or turn the chat into a document.<p>Customize font, text size, and colors&#x2F;theme<p>Export to your preferred format\nThis helps a lot for long-form notes, documentation, or client-facing exports.<p>4) When you need formatting + code blocks preserved\nIf your chats include code, tables, or structured steps, preserving formatting matters.<p>Prefer export methods that keep code blocks intact<p>PDF is great for sharing; Docs&#x2F;Notion are best for editing and collaboration<p>I built Export ChatGPT Conversation to cover these workflows (full chat &#x2F; selected messages &#x2F; styling &#x2F; multiple formats). If you try it, I\u2019d love feedback:<p>Which destination do you use most: Notion or Google Docs?<p>What\u2019s missing: better tables, images, templates, or batch export?<p>(Feel free to drop your workflow below \u2014 happy to iterate based on real use cases.)", "author": "backrun", "timestamp": "2026-01-22T09:19:16+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-01-22T17:21:19.395507+00:00", "processed": false}
