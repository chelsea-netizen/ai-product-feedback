{"id": "hn_story_46847406", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46847406", "title": "Show HN: Self-hosted RAG with MCP support for OpenClaw", "text": "I&#x27;ve been using OpenClaw to control my home server via WhatsApp, but it couldn&#x27;t access my documents. Instead of uploading my private contracts to OpenAI, I built ClawRAG \u2013 a self-hosted RAG engine that connects to OpenClaw via MCP (Model Context Protocol). Now I can ask &quot;What did the contract say about liability?&quot; and get cited answers, not hallucinations.<p>Most RAG systems are either too complex for a solo dev&#x27;s home setup or they rely on cloud-hosted vector stores. I needed something that runs in a single Docker container, understands messy PDFs (tables!), and integrates natively as a &quot;tool&quot; for agents rather than just another REST endpoint.<p>## Technical Deep Dive<p>### Why MCP instead of REST?\nI chose the Model Context Protocol (MCP) because it provides structured schemas that LLMs understand natively. The MCP server exposes `query_knowledge` as a tool, allowing the agent to decide exactly when to pull from the knowledge base vs. when to use its built-in memory. It prevents &quot;tool-drift&quot; and ensures type-safe responses.<p>### The Stack\n- *Parsing*: Docling 2.13.0 (The first parser I&#x27;ve found that doesn&#x27;t choke on nested tables in legacy PDFs).\n- *Storage*: ChromaDB (Lightweight, file-based, no Postgres&#x2F;pgvector overhead needed for personal knowledge bases).\n- *Search*: Hybrid (Vector similarity + BM25 keyword search) fused using Reciprocal Rank Fusion (RRF) for better retrieval on specific legal jargon.\n- *Footprint*: Optimized to run under 2GB RAM (excluding the local LLM).<p>### The tricky part: Citation Preservation\nGetting citations to work reliably over a WhatsApp round-trip was the biggest challenge. I had to ensure chunk IDs and source metadata survive the transformation from ChromaDB \u2192 LlamaIndex \u2192 LLM \u2192 OpenClaw \u2192 WhatsApp without getting &quot;summarized away&quot; or sanitized by the LLM&#x27;s output formatting.<p>## Use Case\nLast week my landlord claimed I signed a clause about garden&#x2F;snow maintenance. I pulled up my phone, wrote to my OpenClaw bot: <i>&quot;Search my lease for gardening obligations&quot;</i>. \nIt found the relevant paragraph in 3 seconds, cited the page&#x2F;section, and provided the exact quote. Argument closed.<p>## Quick Start\nThe repo includes a `docker-compose.yml` that spins up everything including the vector store:<p>```bash\n# 1. Start ClawRAG\ndocker compose up -d<p># 2. Add your documents\ncurl -X POST http:&#x2F;&#x2F;localhost:8080&#x2F;api&#x2F;v1&#x2F;rag&#x2F;documents&#x2F;upload \\\n  -F &quot;files=@my_lease.pdf&quot; \\\n  -F &quot;collection_name=personal&quot;<p># 3. Connect to your agent\nopenclaw mcp add --transport stdio clawrag npx -y @clawrag&#x2F;mcp-server\n```<p>## Community &amp; Feedback\nCode is MIT licensed. I&#x27;d love feedback on the MCP implementation \u2013 specifically if you see better ways to handle tool schemas for multi-collection search.<p>*Ask me anything about the architecture or how I handled the citation logic!*<p>---<p>### Hidden Technical Details \n- *Privacy*: Zero external data leaks. Everything stays on your metal.\n- *LLM Agnostic*: Tested with Ollama (Llama 3.2) and Claude 3.5 via API.\n- *Context Management*: Explicit context window limiting to prevent GPU crashes on 8GB VRAM cards.", "author": "2dogsanerd", "timestamp": "2026-02-01T16:45:28+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:15:57.749767+00:00", "processed": false}
{"id": "hn_comment_46846593", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46846593", "title": "Re: OpenClaw in Practice: A Small Team's Field Notes...", "text": "OpenClaw has been blowing up lately, and for good reason. I&#x27;ve been running it for just a few days\u2014here&#x27;s what it actually looks like in production for a small team.<p>I run SubEasy.ai, a transcription&#x2F;translation&#x2F;voiceover platform. Good reviews, users worldwide, but perpetually understaffed. I&#x27;m not the type who enjoys managing people, so I&#x27;ve always been looking for ways to get more done without hiring more.<p>I set up OpenClaw on a cloud server with a Telegram bot, running Claude Opus. What happened next genuinely surprised me.<p>## Day 1: It Built Its Own Workflow<p>I started simple\u2014asked it to send me a daily briefing with news and stock prices. It came back as a wall of plain text, unreadable in Telegram.<p>So I asked for HTML. But then it gave me local file paths I couldn&#x27;t access.<p>I suggested: &quot;Can you set up a repo, connect it to auto-deployment, and just send me links?&quot;<p>It did. Created the GitHub repo, configured Vercel, and now every report is a clean webpage I can actually read. The whole pipeline was essentially self-built.<p>## Day 2: It Fixed a Production Emergency<p>While setting up Gmail API integration, our YouTube downloader&#x27;s core component (yt-dlp) broke. Alerts everywhere.<p>I asked it to check for fixes. It found a patch that hadn&#x27;t been released as a binary yet.<p>&quot;Just compile it then.&quot;<p>It pulled the source, compiled it, and we deployed. Problem solved in maybe 20 minutes.<p>That&#x27;s when I realized: this thing can actually do real work.<p>## What It Does Now<p>We gave it read access to our user database, payment system (Stripe), and email. Now it handles a lot of the daily grind:<p>*Customer complaints*: When a user emails us, it automatically pulls their payment history, subscription status, and usage data, drafts a response, and sends it to us for one-click approval.<p>*Influencer outreach*: Finding YouTube&#x2F;TikTok creators to partner with used to be a manual slog\u2014searching, evaluating content fit, checking follower counts. Lots of judgment calls that kept getting deprioritized. Now it does the initial screening automatically, dumps qualified candidates into a Notion database, and we just make final decisions.<p>*Review monitoring*: It checks for negative reviews daily, cross-references user data to understand what went wrong, drafts response emails. We just review and send.<p>## The Actual Insight<p>The interesting part isn&#x27;t that AI can do individual tasks\u2014we knew that. It&#x27;s that AI can now handle the glue work between tasks.<p>Most of our jobs are really just moving between tools: check email, look up user in database, cross-reference with payment system, write response. The automation tools we had before could handle single steps, but the stitching was always manual.<p>Now the stitching is automated too. The more systems you connect, the more powerful it gets.<p>## Team Use<p>Our small team shares a Discord server with it. Everyone can talk to it individually or in group channels. Knowledge it learns from one person benefits everyone. It&#x27;s like a shared team member that keeps growing.", "author": "terryops", "timestamp": "2026-02-01T14:54:10+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:02.225123+00:00", "processed": false}
{"id": "hn_story_46846045", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46846045", "title": "Show HN: Moltbot Art \u2013 AI agents draw art with code, not prompts", "text": "I built Moltbot Art - a gallery where AI agents create artworks using simple drawing commands. The idea: instead of text-to-image diffusion models, agents draw programmatically - with\n  commands like circle, line, fill, rect. Each artwork is procedurally generated, step by step. Try it: share moltbotart.com&#x2F;skill.md with your AI agent (Claude, GPT, etc.) and watch it\n  create. Tech stack: Next.js 16, React 19, Prisma, deployed on Railway. Would love feedback on the concept and API design!<p><pre><code>  Would love feedback on the concept and API design!</code></pre>", "author": "fkysly", "timestamp": "2026-02-01T13:22:36+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-01T17:16:08.437226+00:00", "processed": false}
{"id": "hn_comment_46846028", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46846028", "title": "Re: Show HN: ChatGPT-CLI: A Simple ChatGPT CLI That St...", "text": "## chatgpt-cli: A Simple ChatGPT CLI That Stays Out of Your Way<p>I recently built *chatgpt-cli*, a minimal command-line interface for interacting with ChatGPT.<p>*Project link:* [github.com&#x2F;umbertocicciaa&#x2F;chatgpt-cli](<a href=\"https:&#x2F;&#x2F;github.com&#x2F;umbertocicciaa&#x2F;chatgpt-cli\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;umbertocicciaa&#x2F;chatgpt-cli</a>)<p>The motivation is straightforward: *most existing ChatGPT CLI tools are far more complex than they need to be*.<p>---<p>## The Problem with Most ChatGPT CLIs<p>Search for a ChatGPT CLI today, and you&#x27;ll typically find tools that:<p>- Require multiple files, folders, and configuration steps\n- Depend on several external libraries\n- Try to do <i>everything</i> instead of doing one thing well\n- Demand more setup time than actual usage time<p>They&#x27;re powerful, sure \u2014 but often overkill. When all you want is to quickly ask ChatGPT something from the terminal, that complexity becomes friction.<p>I wanted something closer to the *Unix philosophy*: small, simple, transparent, and easy to modify.<p>---<p>## What chatgpt-cli Is<p>*chatgpt-cli* is:<p>- A *single file* \u2014 that&#x27;s it\n- *Zero external dependencies* \u2014 just standard library\n- Easy to read, understand, and tweak in minutes\n- Focused on one job: chatting with ChatGPT from the terminal<p>You can open the file, understand exactly how it works, and customize it without digging through a framework or chasing a dependency tree.<p>*No magic. No bloat. Just a straightforward tool that does what it says.*<p>---<p>## Why Simplicity Matters<p>Simplicity isn&#x27;t just about fewer lines of code \u2014 it&#x27;s about:<p>- *Lower cognitive load*: You don&#x27;t need to learn a framework to use or modify it\n- *Faster setup*: Clone, configure your API key, run \u2014 that&#x27;s it\n- *Easier debugging*: When something breaks, you know exactly where to look\n- *Longer project lifespan*: Simple tools survive because they&#x27;re easier to maintain<p>In a world where tooling keeps getting heavier, there&#x27;s real value in *boring, obvious solutions that just work*.<p>Simple tools tend to outlive complex ones. They&#x27;re easier to fork, adapt, and understand \u2014 even years later.<p>---<p>## Who This Is For<p>*chatgpt-cli* isn&#x27;t trying to replace feature-rich clients. It&#x27;s built for developers who:<p>- Live in the terminal and prefer staying there\n- Want fast feedback loops without context switching\n- Prefer tools they can *fully understand* in one sitting\n- Value control and transparency over convenience features<p>For these users, a lightweight CLI becomes part of daily workflows \u2014 scripting, brainstorming, debugging, writing documentation \u2014 all without leaving the terminal or fighting a complex setup.<p>---<p>## How This Could Change Your Workflow<p>Imagine being able to:<p>- *Ask quick questions* without opening a browser or switching apps\n- *Pipe outputs* directly into other command-line tools\n- *Script interactions* with ChatGPT as part of your automation\n- *Modify the behavior* by editing a single, readable file<p>When your tools stay out of the way, you focus on the work \u2014 not the tooling.<p>---<p>## Final Thoughts<p>This project exists because I needed it. If you&#x27;ve felt the same frustration with over-engineered tools, you might find *chatgpt-cli* refreshing.<p>Sometimes, the best innovation isn&#x27;t adding more features \u2014 it&#x27;s *removing everything that doesn&#x27;t need to be there*.<p>If that resonates with you, check it out, fork it, and make it yours.<p>---<p>#CLI #ChatGPT #OpenSource #DeveloperTools #Minimalism #Terminal #Productivity", "author": "umbertocicciaa", "timestamp": "2026-02-01T13:20:47+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:08.612802+00:00", "processed": false}
{"id": "hn_story_46845936", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46845936", "title": "Show HN: Rubber Duck Committee \u2013 Multi-persona AI debugging with voting", "text": "Inspired by PewDiePie&#x27;s experiments running multiple local AI models as a &quot;council&quot; that vote on decisions [1], I wanted to see if you could get similar multi-perspective analysis without a $20k GPU rig.<p>The approach: use customised system prompts to create distinct personas (methodical professor, creative brainstormer, pragmatic engineer), have them analyse problems independently via parallel API calls, then vote on the best solution using structured outputs (Zod schemas).<p>Key technical bits:\n- Structured responses ensure consistent, parseable JSON from the LLM\n- SSE streaming for real-time UI updates\n- Parallel processing so personas don&#x27;t influence each other\n- Chair Duck orchestrates and breaks ties<p>Built with Next.js 16, Vercel AI SDK, and Google Vertex AI (Gemini 2.0 Flash).<p>Live demo: <a href=\"https:&#x2F;&#x2F;rubber-duck-committee.vercel.app&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;rubber-duck-committee.vercel.app&#x2F;</a>\nSource: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;r-leyshon&#x2F;rubber-duck-committee\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;r-leyshon&#x2F;rubber-duck-committee</a>\nBlog writeup: <a href=\"https:&#x2F;&#x2F;thedatasavvycorner.com&#x2F;notepad&#x2F;05-rubber-duck-committee\" rel=\"nofollow\">https:&#x2F;&#x2F;thedatasavvycorner.com&#x2F;notepad&#x2F;05-rubber-duck-commit...</a><p>[1] <a href=\"https:&#x2F;&#x2F;www.pcgamer.com&#x2F;software&#x2F;ai&#x2F;pewdiepie-creates-an-ai-council-appoints-himself-supreme-leader-and-wipes-out-members-who-underperform-only-for-his-councillors-to-work-against-him&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.pcgamer.com&#x2F;software&#x2F;ai&#x2F;pewdiepie-creates-an-ai-...</a>", "author": "r-leyshon", "timestamp": "2026-02-01T13:01:50+00:00", "score": 1, "num_comments": 0, "products": ["gemini"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:09.035780+00:00", "processed": false}
{"id": "hn_comment_46845101", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46845101", "title": "Re: There is no skill in AI coding...", "text": "The night and day difference after adopting Claude code prompts in my custom agent leads me to the opposite conclusion.<p>Everyone is not getting the same results. This is evident in the wide reporting in usefulness. Some people are producing production code while others claim they can&#x27;t get the AI to to even basic things without error.<p>Something is def different. If we then look to human history and tool usage, never has there been a tool that is equally wielded by all members of the species. It&#x27;s not magic, it&#x27;s a tool. You know two plumbers can produce very different outcomes with the same tools, why do people expect developers with coding agent tools to exhibit different human-tool outcomes?", "author": "verdverm", "timestamp": "2026-02-01T10:30:10+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:16.697802+00:00", "processed": false}
{"id": "hn_comment_46847519", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46847519", "title": "Re: What I learned building an opinionated and minimal...", "text": "I particularly liked Mario&#x27;s point about using tmux for long-running commands. I&#x27;ve found models to be very good at reading from &#x2F; writing to tmux, so I&#x27;ll do things like spin up a session with a REPL, use Claude to prototype something, then inspect it more deeply in the REPL.", "author": "SatvikBeri", "timestamp": "2026-02-01T17:01:59+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-01T17:16:18.205336+00:00", "processed": false}
{"id": "hn_story_46844159", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46844159", "title": "Show HN: LocaFlow \u2013 AI app localization in a few minutes instead of days", "text": "Hey HN,<p>I&#x27;m the developer behind LocaFlow. Here&#x27;s the backstory:<p>I&#x27;ve built several iOS apps over the past few years. Every single one stayed English-only because I dreaded the localization process. The typical workflow:<p>1. Open Localizable.strings as a source code\n2. Copy-paste pieces of strings to ChatGPT or Claude manually\n3. Copy-paste translations back\n4. Test everything\n7. Repeat for each language and every app update<p>This would take me 8+ hours per app. I kept putting it off.<p>Last quarter, I finally decided to localize one of my apps. Halfway through the Saturday I spent on it, I thought &quot;I&#x27;m a developer... why am I doing this manually?&quot;<p>So I built LocaFlow.<p>What it does:\n- Select your the app project on your computer\n- AI translates to 50+ languages\n- Takes about a few minutes instead of hours&#x2F;days<p>What&#x27;s different:\n- No &quot;bring your own API key&quot; friction (I handle translation API costs)\n- Preserves formatting (variables, plurals, special characters)\n- Simple pricing: $19&#x2F;month (or generous free plan)\n- Built by a developer who uses it for his own apps<p>Technical details:\n- Uses AI for translation (better context understanding than Google Translate)\n- Validates string formatting before&#x2F;after translation\n- Handles iOS plural forms, Android string arrays, etc.\n- Can process batch translations (entire app at once)<p>Happy to answer questions about implementation, pricing, roadmap, or anything else.<p>Try it out: <a href=\"https:&#x2F;&#x2F;locaflow.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;locaflow.dev</a> (free tier available, no credit card required)", "author": "nikolaitarasov", "timestamp": "2026-02-01T06:54:16+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:28.426288+00:00", "processed": false}
{"id": "hn_story_46844001", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46844001", "title": "Show HN: I lost 3 years of ChatGPT history overnight, so I built a backup tool", "text": "One month ago, OpenAI deactivated my ChatGPT account without warning. 3+ years of conversations\u2014gone.<p>I tried everything. Emailed every OpenAI address I could find. Their response? &quot;Use our data export tool.&quot; The catch? You need an active account to export your data.<p>Classic.<p>So I built a browser extension that lets me save any conversation from ChatGPT, Claude, or Gemini with one click. Markdown format, stored in one place.<p>Turns out it solved another problem I didn&#x27;t expect: I often ask the same question to all three AIs, then forget which one gave me the best answer. Now they&#x27;re all in one place.<p>It&#x27;s been my personal tool for now. Nothing fancy\u2014just scratching my own itch. If there&#x27;s interest, I&#x27;ll consider publishing it. Could easily extend to other AI assistants too.<p>Anyone else paranoid about their AI chat history now?", "author": "benjushi", "timestamp": "2026-02-01T05:58:43+00:00", "score": 4, "num_comments": 1, "products": ["claude", "chatgpt", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:29.743192+00:00", "processed": false}
{"id": "hn_comment_46844270", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46844270", "title": "Re: Show HN: OpenJuris \u2013 AI legal research with citati...", "text": "How do we know it\u2019s not just a crappy wrapper? What\u2019s the difference between just uploading documents into a general purpose LLM and asking it to cite sources?<p>I would also add as feedback that it\u2019s kind of scammy to use the word \u201copen\u201d and \u201c.org\u201d like this when you\u2019re running a for-profit business. It\u2019s not illegal but it feels unethical. Just because OpenAI made fake non-profit status popular doesn\u2019t mean you have to follow that oath.<p>&gt; This free tier will be subsidized by our enterprise functions<p>I assume you are not in any way a non-profit organization.", "author": "dangus", "timestamp": "2026-02-01T07:29:14+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["naming_terminology"], "sentiment": null, "collected_at": "2026-02-01T17:16:33.875331+00:00", "processed": false}
{"id": "hn_story_46842787", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46842787", "title": "Show HN: Hebo Gateway, an embeddable AI gateway with OpenAI-compatible endpoints", "text": "Hey HN, we just shipped v0.1 of Hebo Gateway.<p>There are plenty of gateways already, but we kept running into the same issue: once you need real customization (auth, routing, rate limits, observability, request&#x2F;response transforms), most \u201coff the shelf\u201d gateways get hard to extend.<p>Hebo Gateway is for cases where you want the gateway to be part of your app. You can run it standalone, or embed it into an existing backend. It exposes OpenAI-compatible endpoints (&#x2F;chat&#x2F;completions, &#x2F;embeddings, &#x2F;models), works with any Vercel AI SDK provider, and adds a hook system so you can plug logic into the request lifecycle without forking the core.<p>Quickstart, examples, and \u201cwhat\u2019s next\u201d are in the post:\n<a href=\"https:&#x2F;&#x2F;hebo.ai&#x2F;blog&#x2F;260127-hebo-gateway\" rel=\"nofollow\">https:&#x2F;&#x2F;hebo.ai&#x2F;blog&#x2F;260127-hebo-gateway</a><p>I would love feedback on OpenAI-compat edge cases you have been bitten by (especially streaming and reasoning-related stuff), and what hooks you wish gateways provided out of the box.", "author": "dselvaggio", "timestamp": "2026-02-01T01:35:25+00:00", "score": 2, "num_comments": 0, "products": ["chatgpt"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:37.067642+00:00", "processed": false}
{"id": "hn_story_46842711", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46842711", "title": "Show HN: SymDerive \u2013 A functional, stateless symbolic math library", "text": "Hey HN,<p>I\u2019m a physicist turned quant. Some friends and I &#x27;built&#x27; SymDerive because we wanted a symbolic math library that was &quot;Agent-Native&quot; by design, but still a practical tool for humans.<p>It boils down to two main goals:<p>1. Agent Reliability: I\u2019ve found that AI agents write much more reliable code when they stick to stateless, functional pipelines (Lisp-style). It keeps them from hallucinating state changes or getting lost in long procedural scripts. I wanted a library that enforces that &quot;Input -&gt; Transform -&gt; Output&quot; flow by default.<p>2. Easing the transition to Python: For many physicists, Mathematica is the native tongue. I wanted a way to ease that transition\u2014providing a bridge that keeps the familiar syntax (CamelCase, Sin, Integrate) while strictly using the Python scientific stack under the hood.<p>What I built: It\u2019s a functional wrapper around the standard stack (SymPy, PySR, CVXPY) that works as a standalone engine for anyone\u2014human or agent\u2014who prefers a pipe-based workflow.<p><pre><code>  # The &quot;Pipe&quot; approach (Cleaner for agents, readable for humans)\n  result = (\n      Pipe((x + 1)**3)\n      .then(Expand)\n      .then(Simplify) \n      .value\n  )\n</code></pre>\nThe &quot;Vibes&quot; features:<p>Wolfram Syntax: Integrate, Det, Solve. If you know the math, you know the API.<p>Modular: The heavy stuff (Symbolic Regression, Convex Optimization) are optional installs ([regression], [optimize]). It won\u2019t bloat your venv unless you ask it to.<p>Physics stuff: I added tools I actually use\u2014abstract index notation for GR, Kramers-Kronig for causal models, etc.<p>It\u2019s definitely opinionated, but if you\u2019re building agents to do rigorous math, or just want a familiar functional interface for your own research, this might help.<p>I have found that orchestrators (Claude Code, etc) are fairly good at learning the tools and sending tasks to the right persona, we have been surprised by how well it has worked.<p>Repo here: https:&#x2F;&#x2F;github.com&#x2F;closedform&#x2F;deriver<p>I will cry if roasted too hard", "author": "dinunnob", "timestamp": "2026-02-01T01:20:57+00:00", "score": 2, "num_comments": 0, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:37.407943+00:00", "processed": false}
{"id": "hn_comment_46843411", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46843411", "title": "Re: The Complete Guide to Building Skills for Claude...", "text": "Great guide \u2014 thorough and practical. Two things I&#x27;d add from my experience building and testing skills:<p><pre><code>  1. Baseline comparison across models: The guide suggests comparing with and without a skill (p9), but doesn&#x27;t mention that the same skill can perform very differently across models. A skill that improves outcomes on a larger model might overwhelm a smaller one with too many competing instructions. Testing against the baseline for each model you intend to support matters.                                                                                                                                                       \n  2. Implicit vs explicit constraints as a design tool: The guide focuses on explicit instructions \u2014 tell Claude what to do, what not to do, how to validate. In my testing, implicit constraints (format rules, structural limits) were sometimes more effective at preventing unintended outputs, because they limit the output space structurally rather than relying on the model to interpret conditional logic. Skill authors might benefit from thinking about what their format rules prevent as a side effect, not just what their instructions request.                                                                                                                                                            \n                                                                                                                                                                                   \n  Curious if others building skills are seeing similar patterns.</code></pre>", "author": "shadab_nazar", "timestamp": "2026-02-01T03:42:51+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-01T17:16:37.717707+00:00", "processed": false}
