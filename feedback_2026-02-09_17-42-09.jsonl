{"id": "hn_comment_46948125", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46948125", "title": "Re: I built a customized LLM with RAG for Singapore...", "text": "Hello everyone,<p>I have always loved coding and in the couple I was thinking of making an open source project and it turned out to be awesome I hope you guys like it.<p>I present Explore Singapore which I created as an open-source intelligence engine to execute retrieval-augmented generation (RAG) on Singapore&#x27;s public policy documents and legal statutes and historical archives.<p>The objective required building a domain-specific search engine which enables LLM systems to decrease errors by using government documents as their exclusive information source.<p>What my Project does :- basically it provides legal information faster and reliable(due to RAG) without going through long PDFs of goverment websites and helps travellers get insights faster about Singapore.<p>Target Audience:- Python developers who keep hearing about &quot;RAG&quot; and AI agents but haven&#x27;t build one yet or building one and are stuck somewhere also Singaporean people(obviously!)<p>Comparison:- RAW LLM vs RAG based LLM to test the rag implementation i compared output of my logic code against the standard(gemini&#x2F;Arcee AI&#x2F;groq) and custom system instructions with rag(gemini&#x2F;Arcee AI&#x2F;groq) results were shocking query:- &quot;can I fly in a drone in public park&quot; standard llm response :- &quot;&quot;gave generic advice about &quot;checking local laws&quot;  and safety guidelines&quot;&quot;\nCustomized llm with RAG :- &quot;&quot;cited the air navigation act,specified the 5km no fly zones,and linked to the CAAS permit page&quot;&quot; the difference was clear and it was sure that the ai was not hallucinating.<p>Ingestion:- I have the RAG Architecture about 594 PDFs about Singaporian laws and acts which rougly contains 33000 pages.<p>How did I do it :- I used google Collab to build vector database and metadata which nearly took me 1 hour to do so ie convert PDFs to vectors.<p>How accurate is it:- It&#x27;s still in development phase but still it provides near accurate information as it contains multi query retrieval ie if a user asks (&quot;ease of doing business in Singapore&quot;) the logic would break the keywords &quot;ease&quot;, &quot;business&quot;, &quot;Singapore&quot; and provide the required documents from the PDFs with the page number also it&#x27;s a little hard to explain but you can check it on my webpage.Its not perfect but hey i am still learning.<p>The Tech Stack:  \nIngestion: Python scripts using PyPDF2 to parse various PDF formats.  \nEmbeddings: Hugging Face BGE-M3(1024 dimensions)\nVector Database: FAISS for similarity search.  \nOrchestration: LangChain.  \nBackend: Flask\nFrontend: React and Framer.<p>The RAG Pipeline operates through the following process:  \nChunking: The source text is divided into chunks of 150 with an overlap of 50 tokens to maintain context across boundaries.  \nRetrieval: When a user asks a question (e.g., &quot;What is the policy on HDB grants?&quot;), the system queries the vector database for the top k chunks (k=1).  \nSynthesis: The system adds these chunks to the prompt of LLMs which produces the final response that includes citation information.\nWhy did I say llms :- because I wanted the system to be as non crashable as possible so I am using gemini as my primary llm to provide responses but if it fails to do so due to api requests or any other reasons the backup model(Arcee AI trinity large) can handle the requests.<p>Don&#x27;t worry :- I have implemented different system instructions for different models so that result is a good quality product.<p>Current Challenges:  \nI am working on optimizing the the ranking strategy of the RAG architecture. I would value insights from anyone who has encountered RAG returning unrelevant documents.<p>Feedbacks are the backbone of improving a platform so they are most<p>Repository:- <a href=\"https:&#x2F;&#x2F;github.com&#x2F;adityaprasad-sudo&#x2F;Explore-Singapore\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;adityaprasad-sudo&#x2F;Explore-Singapore</a>", "author": "ambitious_potat", "timestamp": "2026-02-09T17:32:38+00:00", "score": null, "num_comments": null, "products": ["gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-09T17:42:11.879691+00:00", "processed": false}
{"id": "hn_story_46947628", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46947628", "title": "Show HN: Stop tracking time, start reconstructing work (with anker)", "text": "I built anker because I&#x27;m tired of productivity tools that want me to track every minute.<p>The problem: At the end of the day, you need to explain what you did (standups, reports, timesheets). But tracking in real-time is exhausting and interrupts flow.<p>Anker flips this: *work first, summarize later*. It reconstructs your day from sources you already have:\n- Git commits (with full diffs)\n- Markdown notes\n- Obsidian vaults<p>Then generates summaries in multiple formats. The markdown format includes full git diffs, which makes it perfect for piping to AI:<p>```bash\nanker recap yesterday --format markdown | claude -p &quot;Create standup notes&quot;\n```<p>I call this the *#AntiProductivity mindset* - focusing on meaning instead of metrics.<p>Built in Go, works locally (no cloud&#x2F;tracking), MIT licensed. Took it from idea to v0.1.1 in about a week with Claude AI doing most of the heavy lifting.<p>Demo GIF in the README shows it analyzing HTB machine recon notes.<p>Would love feedback on:\n- What data sources would you want to track?\n- Is the &quot;anti-productivity&quot; messaging clear or confusing?\n- How would you use this in your workflow?", "author": "charemma", "timestamp": "2026-02-09T16:57:34+00:00", "score": 3, "num_comments": 0, "products": ["claude"], "categories": ["content_clarity"], "sentiment": null, "collected_at": "2026-02-09T17:42:14.203200+00:00", "processed": false}
{"id": "hn_story_46946750", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46946750", "title": "Show HN: C-CMCP \u2013 Validated AI development workflow with quality gates", "text": "I built a workflow that coordinates Claude.ai (design), Cursor AI (implementation), and API Claude (validation) with human approval gates at critical points.<p>The problem: AI coding tools are great but there&#x27;s no quality control. You get code that &quot;looks right&quot; but fails requirements, has bugs, or doesn&#x27;t match what you actually asked for.<p>C-CMCP solves this with a 4-stage pipeline:\n1. Claude.ai creates detailed task specs (MUST&#x2F;SHOULD&#x2F;COULD requirements)\n2. You approve the task\n3. Cursor AI implements the code\n4. API Claude validates against the original requirements (~$0.01-0.03 per validation)\n5. You review validation + test manually\n6. Accept or request fixes<p>We built this in 2.5 hours and it&#x27;s production-ready. Tested on Windows with PowerShell scripts that automate the workflow.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;stanpressman&#x2F;C-CMCP\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;stanpressman&#x2F;C-CMCP</a><p>Built this for my own startup (password manager) but figured others might find it useful. MIT licensed, completely free.<p>Would love feedback on the approach and what features would make this more useful.", "author": "PhantomKey", "timestamp": "2026-02-09T16:06:26+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-09T17:42:21.019892+00:00", "processed": false}
{"id": "hn_story_46946742", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46946742", "title": "Show HN: Sales Agent Benchmark \u2013 SWE-Bench for sales AI agents (open source)", "text": "Live leaderboard: <a href=\"https:&#x2F;&#x2F;sales-agent-benchmarks.fly.dev&#x2F;benchmark\" rel=\"nofollow\">https:&#x2F;&#x2F;sales-agent-benchmarks.fly.dev&#x2F;benchmark</a>\nGitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;a1j9o94&#x2F;sales-agent-benchmark\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;a1j9o94&#x2F;sales-agent-benchmark</a><p>I built an open-source benchmark for evaluating LLMs as sales agents. The idea came from noticing that every sales AI tool demos well on clean summaries but falls apart on real deal data \u2014 and there was no rigorous way to measure that gap.<p>How it works<p>You register an API endpoint. We send your agent deal context (anonymized real B2B deals), it returns structured recommendations (risks, next steps, stakeholder analysis). A multi-judge panel (Claude, GPT, Gemini via OpenRouter) scores against ground truth \u2014 what actually happened in the deal.<p>Two evaluation modes:<p>Summary Benchmark \u2014 Pre-digested checkpoint summaries. Single-turn. 15 deals, 36 checkpoints, 4 scoring dimensions. Models score 68\u201381%. This is the easy mode.<p>Artifact-Based Benchmark \u2014 Raw call transcripts, email threads, CRM snapshots, Slack messages, documents. Multi-turn (agent can request specific artifacts before answering). 14 deals, 65 checkpoints, 148 evaluation tasks across 8 scoring dimensions. Models score 26\u201338%.<p>Every model we tested drops roughly in half when switching from summaries to real artifacts.<p>The interesting findings<p>Risk Identification collapses. Best model goes from 8.0&#x2F;10 on summaries to 2.3&#x2F;10 on real data. Models confidently identify risks that don&#x27;t exist in the source material.<p>Hallucinated stakeholders. On stakeholder extraction tasks, models invent names (Lisa Sousa, Emma Starr, Mike Lee) that appear in zero artifacts. The actual stakeholders are in the transcripts \u2014 models just don&#x27;t extract them.<p>Structured frameworks survive. MEDDPICC qualification scoring holds up at 7.5&#x2F;10. Turns out models are decent at filling in structured templates even from messy data. It&#x27;s the open-ended analysis that falls apart.<p>Communication quality is fine. Models score 5\u20138&#x2F;10 on drafting follow-up emails and call summaries. The writing is good. The reasoning behind it isn&#x27;t.<p>Technical details<p>Stack: Bun, TypeScript, React, Postgres (Neon), deployed on Fly.io<p>Evaluation: Task-specific judge prompts per artifact type. Three judges run in parallel, scores averaged to reduce single-model bias. Dimensions: risk identification, next step quality, prioritization, outcome alignment, stakeholder mapping, deal qualification, information synthesis, communication quality.<p>Artifact types: TranscriptArtifact (speaker-labeled turns from Granola AI), EmailArtifact (threaded messages with metadata), CrmSnapshotArtifact (HubSpot deal properties + stage history), DocumentArtifact (proposals, decks), SlackThreadArtifact, CalendarEventArtifact<p>Multi-turn protocol: Artifact-based requests include turnNumber&#x2F;maxTurns. Agents can return artifactRequests to ask for more context before submitting their analysis. The benchmark runner handles the conversation loop.<p>API contract: POST your endpoint, receive { version: 2, artifacts: [...], stakeholders: [...], evaluationTask: {...} }, return structured JSON with risks, next steps, and dimension-specific analysis.<p>What I&#x27;m looking for<p>Try it. Register an endpoint and benchmark your agent: <a href=\"https:&#x2F;&#x2F;sales-agent-benchmarks.fly.dev&#x2F;benchmark\" rel=\"nofollow\">https:&#x2F;&#x2F;sales-agent-benchmarks.fly.dev&#x2F;benchmark</a><p>Data partners. The dataset is small (29 deals). If you have anonymized deal artifacts \u2014 call transcripts, email exports, CRM data with outcomes \u2014 I&#x27;d love to process them through the pipeline and credit you as a founding contributor.<p>Feedback on evaluation methodology. The multi-judge approach works but I&#x27;m not confident the prompts are optimal. Happy to discuss the judge prompt design in issues.<p>The gap between summary performance and real-artifact performance seems like a general problem beyond sales. If anyone&#x27;s seen similar benchmark work in other domains (legal document analysis, medical records, etc.), I&#x27;d be interested to compare notes.", "author": "a1j9o94", "timestamp": "2026-02-09T16:05:55+00:00", "score": 1, "num_comments": 0, "products": ["claude", "gemini"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-09T17:42:21.126828+00:00", "processed": false}
{"id": "hn_story_46946621", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46946621", "title": "Show HN: BB \u2013 A persistent message broker for AI agents (MCP, Ed25519, Matrix)", "text": "BB is a message broker that lets AI agents publish information, ask questions, and answer each other&#x27;s requests. Everything is cryptographically signed (Ed25519), semantically searchable, and durable.<p>How it works:<p>- Agents publish INFO events to topics (like `news.ai`, `research.security`, etc.)\n- Agents post REQUESTs \u2014 other agents FULFILL them\n- Semantic search lets any agent find what other agents have published\n- Reputation scores track which agents produce quality responses\n- Bounties let you pay for answers<p>Transport is federated: Built on Matrix, so anyone can run their own homeserver and participate.<p>Integration is 3 lines of config: Works with Claude Code, ChatGPT, Cursor, Windsurf, and any MCP client:<p>To try it out, see the one-liners shared on <a href=\"https:&#x2F;&#x2F;bb.org.ai&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;bb.org.ai&#x2F;</a> Your agent immediately gets tools for: search, publish, request, fulfill, upvote&#x2F;downvote, browse topics, and more.<p>Current state: ~40K events across 20+ topics. Mostly RSS agents right now \u2014 we&#x27;re launching to get real agent-to-agent collaboration going.", "author": "lthms", "timestamp": "2026-02-09T15:56:30+00:00", "score": 2, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["navigation", "response_quality"], "sentiment": null, "collected_at": "2026-02-09T17:42:21.764617+00:00", "processed": false}
{"id": "hn_story_46946215", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46946215", "title": "Show HN: We added AGENTS.md to 120 challenges so AI teaches instead of codes", "text": "Hi HN! I&#x27;m Matt, founder of Frontend Mentor (<a href=\"https:&#x2F;&#x2F;www.frontendmentor.io\" rel=\"nofollow\">https:&#x2F;&#x2F;www.frontendmentor.io</a>). We provide front-end and full-stack coding challenges with professional Figma designs, enabling developers to build real projects and grow their skills.<p>The problem: AI coding tools are great, but they can work against you when you&#x27;re learning. Ask Copilot or Cursor to help with a beginner project, and they&#x27;ll happily write the whole thing for you. You ship the project, but you didn&#x27;t really learn anything.<p>What we did: We added AGENTS.md (and CLAUDE.md) files to every challenge&#x27;s starter code. These files tell AI tools how to help based on the challenge&#x27;s difficulty level, so the AI becomes a learning partner rather than an answer machine.<p>The idea is simple: AI guidance should scale with the learner.<p>- Newbie: AI acts as a patient mentor. Breaks problems into tiny steps, uses analogies, and gives multiple hints before showing an approach. Won&#x27;t hand you a complete solution.<p>- Junior: AI becomes a supportive guide. Introduces debugging, encourages DevTools usage, and explains the &quot;why,&quot; not just the &quot;what.&quot;<p>- Intermediate: AI acts like an experienced colleague. Presents trade-offs, shows multiple approaches, and lets you make decisions.<p>- Advanced: AI acts like a senior dev. Challenges your thinking, plays devil&#x27;s advocate, gives honest feedback.<p>- Guru: AI acts like a peer. Debates approaches, references specs, brings different viewpoints.<p>The core principle across all levels: guide thinking, don&#x27;t replace it.<p>Since tools like Cursor and Copilot already look for AGENTS.md in project directories, this works out of the box with no setup.<p>We don&#x27;t think anyone has fully figured out AI-assisted learning yet, and the landscape is shifting so quickly. This is our first attempt at making AI tools better by default for people who are trying to build foundational coding skills, not just ship projects.<p>Would love to hear your thoughts, especially from anyone considering how AI tools and skill development can work together.", "author": "stooderrr", "timestamp": "2026-02-09T15:22:32+00:00", "score": 1, "num_comments": 0, "products": ["claude", "copilot"], "categories": ["response_quality"], "sentiment": null, "collected_at": "2026-02-09T17:42:25.999768+00:00", "processed": false}
{"id": "hn_story_46945544", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46945544", "title": "Show HN: Claude SaaS Starter \u2013 Next.js Boilerplate for Claude Streaming", "text": "Product: <a href=\"https:&#x2F;&#x2F;bydaewon.gumroad.com&#x2F;l&#x2F;claude-saas-starter\" rel=\"nofollow\">https:&#x2F;&#x2F;bydaewon.gumroad.com&#x2F;l&#x2F;claude-saas-starter</a><p>I built this because every SaaS boilerplate I found was OpenAI-first. Claude&#x27;s SSE streaming differs from OpenAI&#x27;s \u2014 the Anthropic SDK&#x27;s messages.stream() returns a different event structure, and you need a custom ReadableStream transform to pipe it as Server-Sent Events on Edge Runtime.<p>The stack:<p>- Next.js 16 (App Router, TypeScript)\n- Supabase for auth and PostgreSQL (with RLS)\n- Anthropic SDK for Claude streaming\n- Stripe for subscription billing (full webhook lifecycle)\n- Vitest for testing (40 tests)<p>The streaming API route transforms Anthropic SDK events into SSE format and runs on Edge Runtime for low-latency. On the client, a React hook (useClaudeStream) handles SSE parsing, text delta buffering, and error recovery.<p>The non-streaming parts are fairly standard SaaS infrastructure: Supabase Auth with middleware route protection, Stripe webhooks for the subscription lifecycle (checkout, updates, cancellations, failed payments), usage metering that logs token counts non-blocking via the Supabase REST API.<p>I wrote 1,300 lines of documentation across 4 guides \u2014 setup, Stripe configuration, OAuth setup, and a quick-start. The guides assume zero prior knowledge of Supabase, Stripe, or the Anthropic API.<p>$149 on Gumroad (or $119 with code LAUNCH20).<p>Happy to answer technical questions about the Claude streaming implementation or any of the architecture decisions.", "author": "bydaewon", "timestamp": "2026-02-09T14:26:01+00:00", "score": 1, "num_comments": 0, "products": ["claude", "chatgpt"], "categories": ["error_messages", "response_quality"], "sentiment": null, "collected_at": "2026-02-09T17:42:31.968434+00:00", "processed": false}
{"id": "hn_comment_46946180", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46946180", "title": "Re: Jony Ive Designed Ferrari Luce EV Interior...", "text": "I clearly don&#x27;t understand Design. My expectation is that an amazing prolific designer would deliver different designs in different contexts. At Apple maybe it&#x27;s this minimalist industrial design. But what I&#x27;m seeing here - and forgive me if I&#x27;m just an idiot about design, is exactly what you&#x27;d get it you asked ChatGPT &quot;Ferrari but Johnny Ive apple design interior&quot;.<p>It&#x27;s all the same design language and materials you&#x27;d seen on Apple product. It&#x27;s almost like someone went &quot;let&#x27;s make the infotainment a giant Apple watch&quot;.<p>I would expect you call up a good designer and they design something special that works in the language of your product, something uniquely new, but also uniquely Ferrari. But what seems to be happening here is if you phone up Johnny Ive you&#x27;ll get a slapdash re-run of 2010s iPhone design.", "author": "Traster", "timestamp": "2026-02-09T15:20:34+00:00", "score": null, "num_comments": null, "products": ["chatgpt"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-09T17:42:37.927735+00:00", "processed": false}
{"id": "hn_story_46944855", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46944855", "title": "Show HN: Claude-Pipe \u2013 A 1k LOC Bridge from Claude Code to Telegram/Discord", "text": "I recently tried using OpenClaw for my agentic workflows. It is an impressive project, but the complexity was a hurdle for my use case. It contains over 400,000 lines of code and requires significant configuration. I found myself managing the infrastructure more than the tasks.<p>I wanted a minimalist tool that followed the Unix philosophy.<p>I built Claude-Pipe. It is a 1,000-line bridge that connects Anthropic\u2019s Claude Code CLI to Telegram or Discord.<p>Technical details:<p>Inherits existing state. It builds on top of your local Claude Code setup. This means all your previously configured skills, plugins, and sub-agents work out of the box without extra setup.<p>Small codebase. At roughly 1,000 lines, the entire project is auditable in a few minutes. This is important for security when giving an LLM access to a terminal or private APIs.<p>VPS deployment. I run this on a private VPS. It provides a persistent, secure environment for sensitive integrations, such as the Gmail-reading skill I use to scan subjects via mobile.<p>Model flexibility. The CLI allows routing to different models, including third-party options like MiniMax-M2.1, while keeping the same interface.<p>The goal was to stop reinventing the platform and just create a pipe for the tools that already work.<p>Link: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;georgi&#x2F;claude-pipe\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;georgi&#x2F;claude-pipe</a>", "author": "mmgeorgi", "timestamp": "2026-02-09T13:12:17+00:00", "score": 1, "num_comments": 0, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-09T17:42:39.018583+00:00", "processed": false}
{"id": "hn_comment_46944078", "source": "hackernews", "source_url": "https://news.ycombinator.com/item?id=46944078", "title": "Re: A daily word puzzle my 9-year-old daughter designe...", "text": "My daughter drew a word puzzle game on paper \u2014 rules, name, game board, everything. We built it into a real daily puzzle over a weekend.<p>It&#x27;s a word chain: each word starts with the last letter of the previous one, climbing from 3 to 7 letters in a pizza-slice shape. Certain letters are locked in each day (pepperoni) so every puzzle is different. Scrabble-style scoring with row multipliers.<p>Stack: vanilla JS (no framework), Cloudflare Pages + D1 for the leaderboard. Single HTML file, ~90KB. Dictionary is NWL2023 filtered to 3-8 letter words. Puzzles are deterministic via seeded RNG so everyone gets the same puzzle each day.<p>We used AI heavily in the build process \u2014 she had the idea and the rules, I used Claude to translate her design into working code fast. The whole thing went from paper sketch to playable in about an hour, then we&#x27;ve been iterating since.", "author": "tryanythingonce", "timestamp": "2026-02-09T11:28:50+00:00", "score": null, "num_comments": null, "products": ["claude"], "categories": ["general_ux"], "sentiment": null, "collected_at": "2026-02-09T17:42:47.454287+00:00", "processed": false}
